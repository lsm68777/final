# ğŸ¯ Phoenix 95 ì‹œìŠ¤í…œ4 - ì™„ì „í•œ ì½”ì–´ ì¸í”„ë¼ êµ¬ì¶• (a.txt ëˆ„ë½ ì½”ë“œ ì™„ì „ ë³µì›)

## ğŸ›ï¸ **ì™„ì „í•œ DDD í´ë” êµ¬ì¡° (ì‹œìŠ¤í…œ4 ì „ìš©)**

### **ë£¨íŠ¸ í´ë”: phoenix95_system4**

```
phoenix95_system4/
â”œâ”€â”€ ğŸ“ services/                     # 11ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (DDD íŒ¨í„´)
â”‚   â”œâ”€â”€ ğŸ“ api-gateway-enterprise/   # 8100: API Gateway & Load Balancing
â”‚   â”œâ”€â”€ ğŸ“ signal-ingestion-pro/     # 8101: Multi-Source Signal Processing
â”‚   â”œâ”€â”€ ğŸ“ market-data-intelligence/ # 8102: Real-Time Data Processing
â”‚   â”œâ”€â”€ ğŸ“ phoenix95-ai-engine/      # 8103: Advanced AI Analysis â­
â”‚   â”œâ”€â”€ ğŸ“ risk-management-advanced/ # 8104: Quantitative Risk Management
â”‚   â”œâ”€â”€ ğŸ“ portfolio-optimizer-quant/# 8105: Modern Portfolio Theory
â”‚   â”œâ”€â”€ ğŸ“ trade-execution-leverage/ # 8106: High-Frequency Execution â­
â”‚   â”œâ”€â”€ ğŸ“ position-tracker-realtime/# 8107: Real-Time Position Management
â”‚   â”œâ”€â”€ ğŸ“ compliance-monitor-regulatory/ # 8108: Enterprise Compliance
â”‚   â”œâ”€â”€ ğŸ“ notification-hub-intelligent/ # 8109: Multi-Channel Notifications
â”‚   â””â”€â”€ ğŸ“ client-dashboard-analytics/ # 8110: Business Intelligence
â”‚
â”œâ”€â”€ ğŸ“ shared/                       # ê³µí†µ ë„ë©”ì¸ ì»´í¬ë„ŒíŠ¸ (DDD Shared Kernel)
â”œâ”€â”€ ğŸ“ infrastructure/               # ì‹œìŠ¤í…œ ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜ ë ˆì´ì–´
â”œâ”€â”€ ğŸ“ tools/                        # ê°œë°œ ë° ìš´ì˜ ë„êµ¬
â”œâ”€â”€ ğŸ“ scripts/                      # ìš´ì˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“ docs/                         # ë¬¸ì„œí™”
â”œâ”€â”€ ğŸ“ tests/                        # í†µí•© í…ŒìŠ¤íŠ¸
â””â”€â”€ ğŸ“„ README.md                     # í”„ë¡œì íŠ¸ ê°œìš”
```

---

## ğŸ’¾ **PostgreSQL DDL Scripts (a.txt ì™„ì „ ë³µì›)**

### **infrastructure/data_storage/postgresql/schemas/03_create_positions_table.sql**
```sql
-- Phoenix 95 ì‹œìŠ¤í…œ4 - í¬ì§€ì…˜ í…Œì´ë¸” (a.txt ì™„ì „ ë³µì›)
CREATE TABLE positions (
    position_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    trade_id UUID NOT NULL REFERENCES trades(trade_id) ON DELETE CASCADE,
    signal_id UUID NOT NULL REFERENCES signals(signal_id),
    
    -- í¬ì§€ì…˜ ê¸°ë³¸ ì •ë³´
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL CHECK (side IN ('long', 'short')),
    
    -- ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ í¬ì§€ì…˜ ì •ë³´
    leverage INTEGER NOT NULL,
    margin_mode VARCHAR(20) NOT NULL,
    base_size DECIMAL(20, 8) NOT NULL,
    leveraged_size DECIMAL(20, 8) NOT NULL,
    margin_used DECIMAL(20, 8) NOT NULL,
    
    -- ê°€ê²© ì •ë³´
    entry_price DECIMAL(20, 8) NOT NULL,
    current_price DECIMAL(20, 8),
    mark_price DECIMAL(20, 8),
    
    -- ì‹œìŠ¤í…œ4 ì†ìµ ì œí•œ
    stop_loss_price DECIMAL(20, 8) NOT NULL,
    take_profit_price DECIMAL(20, 8) NOT NULL,
    liquidation_price DECIMAL(20, 8) NOT NULL,
    
    -- ë§ˆì§„ ê´€ë¦¬
    initial_margin DECIMAL(20, 8) NOT NULL,
    maintenance_margin DECIMAL(20, 8) NOT NULL,
    margin_ratio DECIMAL(8, 4),
    liquidation_buffer DECIMAL(5, 4) DEFAULT 0.1000,
    
    -- ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ P&L
    unrealized_pnl DECIMAL(20, 8) DEFAULT 0,
    unrealized_pnl_percent DECIMAL(8, 4) DEFAULT 0,
    roe DECIMAL(8, 4) DEFAULT 0,
    
    -- í¬ì§€ì…˜ ìƒíƒœ
    status VARCHAR(20) DEFAULT 'open' 
        CHECK (status IN ('open', 'closing', 'closed', 'liquidated', 'expired')),
    
    -- ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§
    last_monitored_at TIMESTAMPTZ DEFAULT NOW(),
    monitoring_interval_seconds INTEGER DEFAULT 3, -- ì‹œìŠ¤í…œ4: 3ì´ˆ
    alert_triggered BOOLEAN DEFAULT FALSE,
    
    -- ë¦¬ìŠ¤í¬ ì§€í‘œ
    distance_to_liquidation DECIMAL(8, 4),
    position_age_hours DECIMAL(8, 2),
    
    -- ìë™ ì²­ì‚° (ì‹œìŠ¤í…œ4: 48ì‹œê°„)
    auto_close_at TIMESTAMPTZ DEFAULT NOW() + INTERVAL '48 hours',
    
    -- íƒ€ì´ë°
    opened_at TIMESTAMPTZ DEFAULT NOW(),
    closed_at TIMESTAMPTZ,
    last_price_update TIMESTAMPTZ DEFAULT NOW(),
    
    -- ë©”íƒ€ë°ì´í„°
    position_metadata JSONB,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ (ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìµœì í™”)
CREATE INDEX idx_s4_positions_active ON positions(status, last_monitored_at) WHERE status = 'open';
CREATE INDEX idx_s4_positions_liquidation_risk ON positions(distance_to_liquidation ASC) 
    WHERE status = 'open' AND distance_to_liquidation < 10;
CREATE INDEX idx_s4_positions_auto_close ON positions(auto_close_at) WHERE status = 'open';

-- ì‹œìŠ¤í…œ4 í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION update_s4_position_metrics()
RETURNS TRIGGER AS $
BEGIN
    NEW.position_age_hours = EXTRACT(EPOCH FROM (NOW() - NEW.opened_at)) / 3600;
    
    IF NEW.side = 'long' THEN
        NEW.distance_to_liquidation = ((NEW.current_price - NEW.liquidation_price) / NEW.current_price) * 100;
    ELSE
        NEW.distance_to_liquidation = ((NEW.liquidation_price - NEW.current_price) / NEW.current_price) * 100;
    END IF;
    
    IF NEW.margin_used > 0 THEN
        NEW.roe = (NEW.unrealized_pnl / NEW.margin_used) * 100;
    END IF;
    
    NEW.last_price_update = NOW();
    RETURN NEW;
END;
$ LANGUAGE plpgsql;

CREATE TRIGGER calculate_s4_position_metrics 
    BEFORE UPDATE ON positions 
    FOR EACH ROW 
    EXECUTE FUNCTION update_s4_position_metrics();

-- ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ í¬ì§€ì…˜ ë·°
CREATE VIEW s4_active_positions AS
SELECT 
    p.*,
    s.phoenix95_score,
    s.confidence as signal_confidence,
    CASE 
        WHEN p.distance_to_liquidation < 5 THEN 'CRITICAL'
        WHEN p.distance_to_liquidation < 10 THEN 'HIGH'
        WHEN p.distance_to_liquidation < 20 THEN 'MEDIUM'
        ELSE 'LOW'
    END as liquidation_risk_level,
    CASE 
        WHEN p.position_age_hours > 48 THEN TRUE
        ELSE FALSE
    END as should_auto_close
FROM positions p
JOIN signals s ON p.signal_id = s.signal_id
WHERE p.status = 'open'
ORDER BY p.distance_to_liquidation ASC;

COMMENT ON TABLE positions IS 'Phoenix 95 ì‹œìŠ¤í…œ4 í¬ì§€ì…˜ í…Œì´ë¸”';
```

---

## ğŸ”§ **Redis ì™„ì „ êµ¬í˜„ (a.txt ëˆ„ë½ ì½”ë“œ)**

```python
# infrastructure/data_storage/redis/system4_redis_manager.py
"""
Redis ì—°ê²° ë° ê´€ë¦¬ í´ë˜ìŠ¤ - ì‹œìŠ¤í…œ4 ì™„ì „ êµ¬í˜„ (a.txt ë³µì›)
"""

import redis.asyncio as redis
import json
import logging
from typing import Dict, List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class System4RedisManager:
    """ì‹œìŠ¤í…œ4 Redis ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.system_prefix = "s4:"
    
    async def cache_price_data(self, symbol: str, price: float, exchange: str = "binance"):
        """ì‹œìŠ¤í…œ4 ê°€ê²© ë°ì´í„° ìºì‹± (30ì´ˆ)"""
        key = f"{self.system_prefix}price:{symbol.upper()}:{exchange.lower()}"
        data = {
            "symbol": symbol,
            "price": price,
            "timestamp": datetime.now().isoformat(),
            "system_version": "4.0"
        }
        await self.redis.setex(key, 30, json.dumps(data))  # ì‹œìŠ¤í…œ4: 30ì´ˆ
    
    async def get_cached_price(self, symbol: str, exchange: str = "binance") -> Optional[Dict]:
        """ìºì‹œëœ ê°€ê²© ì¡°íšŒ"""
        key = f"{self.system_prefix}price:{symbol.upper()}:{exchange.lower()}"
        cached_data = await self.redis.get(key)
        return json.loads(cached_data) if cached_data else None
    
    async def cache_analysis_result(self, signal_id: str, analysis_data: Dict):
        """Phoenix 95 ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        key = f"{self.system_prefix}analysis:{signal_id}"
        data = {
            "signal_id": signal_id,
            "analysis_type": analysis_data.get("analysis_type", "PHOENIX_95_SYSTEM4"),
            "phoenix95_score": analysis_data.get("phoenix95_score"),
            "final_confidence": analysis_data.get("final_confidence"),
            "cached_at": datetime.now().isoformat(),
            "system_version": "4.0"
        }
        await self.redis.setex(key, 90, json.dumps(data))  # ì‹œìŠ¤í…œ4: 90ì´ˆ
    
    async def update_position_realtime(self, position_id: str, position_data: Dict):
        """ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì—…ë°ì´íŠ¸ (ì‹œìŠ¤í…œ4 3ì´ˆ ê°„ê²©)"""
        key = f"{self.system_prefix}position:{position_id}:realtime"
        data = {
            "position_id": position_id,
            "symbol": position_data.get("symbol"),
            "unrealized_pnl": position_data.get("unrealized_pnl", 0),
            "margin_ratio": position_data.get("margin_ratio", 0),
            "distance_to_liquidation": position_data.get("distance_to_liquidation"),
            "last_updated": datetime.now().isoformat(),
            "monitoring_interval": 3,  # ì‹œìŠ¤í…œ4: 3ì´ˆ
            "system_version": "4.0"
        }
        
        # í™œì„± í¬ì§€ì…˜ ì§‘í•©ì— ì¶”ê°€
        await self.redis.sadd(f"{self.system_prefix}positions:active", position_id)
        await self.redis.hset(key, mapping=data)
    
    async def get_active_positions(self) -> List[str]:
        """í™œì„± í¬ì§€ì…˜ ëª©ë¡ ì¡°íšŒ"""
        return await self.redis.smembers(f"{self.system_prefix}positions:active")
    
    async def enqueue_signal(self, signal_data: Dict, priority: str = "normal"):
        """ì‹ í˜¸ íì— ì¶”ê°€"""
        key = f"{self.system_prefix}queue:signals:{priority}"
        signal_data["system_version"] = "4.0"
        await self.redis.lpush(key, json.dumps(signal_data))
    
    async def dequeue_signal(self, priority: str = "normal") -> Optional[Dict]:
        """ì‹ í˜¸ íì—ì„œ ì œê±°"""
        key = f"{self.system_prefix}queue:signals:{priority}"
        signal_data = await self.redis.rpop(key)
        return json.loads(signal_data) if signal_data else None
    
    async def check_rate_limit(self, api_key: str, limit: int = 300) -> bool:
        """API ì†ë„ ì œí•œ ì²´í¬ (ì‹œìŠ¤í…œ4: 300/ë¶„)"""
        minute = int(datetime.now().timestamp() // 60)
        key = f"{self.system_prefix}rate_limit:{api_key}:{minute}"
        current_count = await self.redis.get(key)
        
        if current_count is None:
            await self.redis.setex(key, 60, 1)
            return True
        elif int(current_count) < limit:
            await self.redis.incr(key)
            return True
        else:
            return False
    
    async def set_system_metrics(self, service_name: str, metrics: Dict):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì„¤ì •"""
        key = f"{self.system_prefix}metrics:{service_name}"
        metrics["timestamp"] = datetime.now().isoformat()
        metrics["system_version"] = "4.0"
        await self.redis.setex(key, 60, json.dumps(metrics))
    
    async def get_system_metrics(self, service_name: str) -> Optional[Dict]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        key = f"{self.system_prefix}metrics:{service_name}"
        metrics_data = await self.redis.get(key)
        return json.loads(metrics_data) if metrics_data else None
```

---

## ğŸ“Š **InfluxDB ì™„ì „ êµ¬í˜„ (a.txt ëˆ„ë½ ì½”ë“œ)**

```python
# infrastructure/data_storage/influxdb/system4_influx_manager.py
"""
InfluxDB í´ë¼ì´ì–¸íŠ¸ ì™„ì „ êµ¬í˜„ - ì‹œìŠ¤í…œ4 (a.txt ë³µì›)
"""

from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
from datetime import datetime
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class System4InfluxDBManager:
    """ì‹œìŠ¤í…œ4 InfluxDB ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, url: str, token: str, org: str, bucket: str):
        self.client = InfluxDBClient(url=url, token=token, org=org)
        self.bucket = bucket
        self.org = org
        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
        self.query_api = self.client.query_api()
    
    async def write_price_data(self, symbol: str, price_data: Dict):
        """ê°€ê²© ë°ì´í„° ì €ì¥"""
        point = Point("s4_price_data") \
            .tag("symbol", symbol.upper()) \
            .tag("exchange", price_data.get("exchange", "binance")) \
            .tag("system_version", "4.0") \
            .field("price", float(price_data["price"])) \
            .field("volume", float(price_data.get("volume", 0))) \
            .time(price_data.get("timestamp", datetime.now()))
        
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def write_trade_metrics(self, trade_data: Dict):
        """ê±°ë˜ ë©”íŠ¸ë¦­ ì €ì¥"""
        point = Point("s4_trade_metrics") \
            .tag("symbol", trade_data["symbol"]) \
            .tag("side", trade_data["side"]) \
            .tag("leverage", str(trade_data.get("leverage", 1))) \
            .tag("system_version", "4.0") \
            .field("position_size", float(trade_data["position_size"])) \
            .field("pnl", float(trade_data.get("pnl", 0))) \
            .field("phoenix95_score", float(trade_data.get("phoenix95_score", 0))) \
            .time(trade_data.get("timestamp", datetime.now()))
        
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def write_system_metrics(self, service_name: str, metrics: Dict):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì €ì¥"""
        point = Point("s4_system_metrics") \
            .tag("service", service_name) \
            .tag("system_version", "4.0") \
            .field("cpu_percent", float(metrics.get("cpu_percent", 0))) \
            .field("memory_percent", float(metrics.get("memory_percent", 0))) \
            .field("requests_per_second", float(metrics.get("requests_per_second", 0))) \
            .time(metrics.get("timestamp", datetime.now()))
        
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def query_price_history(self, symbol: str, timeframe: str = "1h") -> List[Dict]:
        """ê°€ê²© ì´ë ¥ ì¡°íšŒ"""
        query = f'''
        from(bucket: "{self.bucket}")
        |> range(start: -{timeframe})
        |> filter(fn: (r) => r._measurement == "s4_price_data")
        |> filter(fn: (r) => r.symbol == "{symbol}")
        |> filter(fn: (r) => r._field == "price")
        |> sort(columns: ["_time"], desc: true)
        |> limit(n: 100)
        '''
        
        result = self.query_api.query(query, org=self.org)
        
        price_history = []
        for table in result:
            for record in table.records:
                price_history.append({
                    "timestamp": record.get_time(),
                    "price": record.get_value(),
                    "symbol": record.values.get("symbol")
                })
        
        return price_history
    
    async def get_system_performance(self, service_name: str = None) -> Dict:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        service_filter = f'|> filter(fn: (r) => r.service == "{service_name}")' if service_name else ''
        
        query = f'''
        from(bucket: "{self.bucket}")
        |> range(start: -1h)
        |> filter(fn: (r) => r._measurement == "s4_system_metrics")
        {service_filter}
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
        '''
        
        result = self.query_api.query(query, org=self.org)
        
        metrics = {}
        for table in result:
            for record in table.records:
                field = record.get_field()
                if field not in metrics:
                    metrics[field] = []
                metrics[field].append({
                    "timestamp": record.get_time(),
                    "value": record.get_value()
                })
        
        return metrics
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()
```
```sql
-- Phoenix 95 ì‹œìŠ¤í…œ4 - ì‹ í˜¸ í…Œì´ë¸” (a.txt ì™„ì „ ë³µì›)
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- ì‹ í˜¸ í…Œì´ë¸” (ë©”ì¸) - ì‹œìŠ¤í…œ4 ì „ìš©
CREATE TABLE signals (
    signal_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    symbol VARCHAR(20) NOT NULL,
    action VARCHAR(10) NOT NULL CHECK (action IN ('buy', 'sell', 'long', 'short')),
    price DECIMAL(20, 8) NOT NULL CHECK (price > 0),
    confidence DECIMAL(5, 4) DEFAULT 0.8000 CHECK (confidence >= 0 AND confidence <= 1),
    strategy VARCHAR(50) DEFAULT 'unknown',
    timeframe VARCHAR(10) DEFAULT '1h',
    
    -- ê¸°ìˆ ì  ì§€í‘œ
    rsi DECIMAL(5, 2),
    macd DECIMAL(12, 8),
    volume BIGINT,
    
    -- ë©”íƒ€ë°ì´í„°
    source VARCHAR(50) DEFAULT 'tradingview',
    source_timestamp TIMESTAMPTZ,
    received_at TIMESTAMPTZ DEFAULT NOW(),
    processed_at TIMESTAMPTZ,
    
    -- ì²˜ë¦¬ ìƒíƒœ (ì‹œìŠ¤í…œ4)
    validation_status VARCHAR(20) DEFAULT 'pending' 
        CHECK (validation_status IN ('pending', 'valid', 'invalid', 'expired')),
    analysis_status VARCHAR(20) DEFAULT 'pending'
        CHECK (analysis_status IN ('pending', 'analyzing', 'completed', 'failed')),
    execution_status VARCHAR(20) DEFAULT 'pending'
        CHECK (execution_status IN ('pending', 'executed', 'rejected', 'cancelled')),
    
    -- Phoenix 95 ë¶„ì„ ê²°ê³¼ (ì‹œìŠ¤í…œ4)
    phoenix95_score DECIMAL(5, 4),
    final_confidence DECIMAL(5, 4),
    quality_score DECIMAL(5, 4),
    analysis_type VARCHAR(50),
    
    -- ì›ì‹œ ë°ì´í„° (JSON)
    raw_data JSONB,
    analysis_data JSONB,
    execution_data JSONB,
    
    -- ê°ì‚¬ ì¶”ì 
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    created_by VARCHAR(100) DEFAULT 'system4',
    
    -- ì œì•½ì¡°ê±´
    CONSTRAINT valid_timeframe CHECK (timeframe IN ('1m', '5m', '15m', '1h', '4h', '1d')),
    CONSTRAINT valid_source CHECK (source IN ('tradingview', 'mt5', 'telegram', 'discord', 'custom')),
    CONSTRAINT valid_phoenix_score CHECK (phoenix95_score IS NULL OR (phoenix95_score >= 0 AND phoenix95_score <= 1))
);

-- ì¸ë±ìŠ¤ (ì‹œìŠ¤í…œ4 ì¿¼ë¦¬ íŒ¨í„´ ìµœì í™”)
CREATE INDEX idx_signals_symbol_created ON signals(symbol, created_at DESC);
CREATE INDEX idx_signals_status_composite ON signals(validation_status, analysis_status, execution_status);
CREATE INDEX idx_signals_confidence ON signals(final_confidence DESC) WHERE final_confidence >= 0.45;
CREATE INDEX idx_signals_phoenix95 ON signals(phoenix95_score DESC) WHERE phoenix95_score IS NOT NULL;
CREATE INDEX idx_signals_received_at ON signals(received_at DESC);
CREATE INDEX idx_signals_source_timestamp ON signals(source, source_timestamp DESC);

-- GIN ì¸ë±ìŠ¤ (JSON ì¿¼ë¦¬ìš©)
CREATE INDEX idx_signals_raw_data_gin ON signals USING gin(raw_data);
CREATE INDEX idx_signals_analysis_data_gin ON signals USING gin(analysis_data);

-- íŒŒí‹°ì…”ë‹ (ì›”ë³„) - ì‹œìŠ¤í…œ4 ê³ ì„±ëŠ¥
CREATE TABLE signals_y2025m01 PARTITION OF signals FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
CREATE TABLE signals_y2025m02 PARTITION OF signals FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
CREATE TABLE signals_y2025m03 PARTITION OF signals FOR VALUES FROM ('2025-03-01') TO ('2025-04-01');

-- íŠ¸ë¦¬ê±° (updated_at ìë™ ì—…ë°ì´íŠ¸)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_signals_updated_at 
    BEFORE UPDATE ON signals 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();

-- í†µê³„ ë·° (ì‹œìŠ¤í…œ4 ëŒ€ì‹œë³´ë“œìš©)
CREATE VIEW signals_stats AS
SELECT 
    DATE_TRUNC('hour', received_at) as hour,
    COUNT(*) as total_signals,
    COUNT(*) FILTER (WHERE validation_status = 'valid') as valid_signals,
    COUNT(*) FILTER (WHERE execution_status = 'executed') as executed_signals,
    AVG(confidence) as avg_confidence,
    AVG(phoenix95_score) as avg_phoenix95_score,
    COUNT(DISTINCT symbol) as unique_symbols
FROM signals 
WHERE received_at >= NOW() - INTERVAL '24 hours'
GROUP BY DATE_TRUNC('hour', received_at)
ORDER BY hour DESC;

COMMENT ON TABLE signals IS 'Phoenix 95 ì‹œìŠ¤í…œ4 ì‹ í˜¸ í…Œì´ë¸”';
COMMENT ON COLUMN signals.phoenix95_score IS 'Phoenix 95 AI ë¶„ì„ ì ìˆ˜ (0.0-1.0)';
COMMENT ON COLUMN signals.final_confidence IS 'ì‹œìŠ¤í…œ4 ìµœì¢… ì‹ ë¢°ë„';
```

### **infrastructure/data_storage/postgresql/schemas/02_create_trades_table.sql**
```sql
-- Phoenix 95 ì‹œìŠ¤í…œ4 - ê±°ë˜ í…Œì´ë¸” (a.txt ì™„ì „ ë³µì›)
CREATE TABLE trades (
    trade_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    signal_id UUID NOT NULL REFERENCES signals(signal_id) ON DELETE CASCADE,
    
    -- ê±°ë˜ ê¸°ë³¸ ì •ë³´
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL CHECK (side IN ('buy', 'sell', 'long', 'short')),
    order_type VARCHAR(20) DEFAULT 'market' 
        CHECK (order_type IN ('market', 'limit', 'stop', 'stop_limit', 'oco')),
    
    -- ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ ì •ë³´
    leverage INTEGER DEFAULT 20 CHECK (leverage >= 1 AND leverage <= 125),
    margin_mode VARCHAR(20) DEFAULT 'ISOLATED' 
        CHECK (margin_mode IN ('ISOLATED', 'CROSSED')),
    
    -- í¬ì§€ì…˜ ì •ë³´
    base_position_size DECIMAL(20, 8) NOT NULL,
    actual_position_size DECIMAL(20, 8) NOT NULL, -- base_position_size * leverage
    margin_required DECIMAL(20, 8) NOT NULL,
    
    -- ê°€ê²© ì •ë³´
    entry_price DECIMAL(20, 8) NOT NULL,
    entry_price_requested DECIMAL(20, 8),
    exit_price DECIMAL(20, 8),
    
    -- ì‹œìŠ¤í…œ4 ì†ìµ ê´€ë¦¬
    stop_loss_price DECIMAL(20, 8),
    take_profit_price DECIMAL(20, 8),
    stop_loss_percent DECIMAL(5, 4) DEFAULT 0.0200, -- 2%
    take_profit_percent DECIMAL(5, 4) DEFAULT 0.0200, -- 2%
    liquidation_price DECIMAL(20, 8),
    
    -- ìˆ˜ìˆ˜ë£Œ
    trading_fee_percent DECIMAL(6, 5) DEFAULT 0.00040, -- 0.04%
    funding_fee_percent DECIMAL(6, 5) DEFAULT 0.00010, -- 0.01%
    trading_fee_amount DECIMAL(20, 8),
    funding_fee_amount DECIMAL(20, 8),
    
    -- ì‹¤í–‰ ì •ë³´
    exchange VARCHAR(20) DEFAULT 'binance',
    exchange_order_id VARCHAR(100),
    execution_algorithm VARCHAR(50) DEFAULT 'market',
    slippage_tolerance DECIMAL(5, 4) DEFAULT 0.0010, -- 0.1%
    actual_slippage DECIMAL(5, 4),
    
    -- ìƒíƒœ ê´€ë¦¬
    status VARCHAR(20) DEFAULT 'pending' 
        CHECK (status IN ('pending', 'submitted', 'filled', 'partial', 'cancelled', 'rejected', 'expired')),
    fill_status VARCHAR(20) DEFAULT 'unfilled'
        CHECK (fill_status IN ('unfilled', 'partial', 'filled')),
    
    -- ë¦¬ìŠ¤í¬ ì •ë³´ (ì‹œìŠ¤í…œ4)
    risk_score DECIMAL(5, 4),
    var_estimate DECIMAL(20, 8),
    kelly_fraction DECIMAL(5, 4),
    position_correlation DECIMAL(5, 4),
    
    -- íƒ€ì´ë°
    order_submitted_at TIMESTAMPTZ,
    order_filled_at TIMESTAMPTZ,
    position_closed_at TIMESTAMPTZ,
    
    -- P&L (ì†ìµ)
    unrealized_pnl DECIMAL(20, 8) DEFAULT 0,
    realized_pnl DECIMAL(20, 8) DEFAULT 0,
    total_pnl DECIMAL(20, 8) DEFAULT 0,
    roe_percent DECIMAL(8, 4), -- Return on Equity %
    
    -- ë©”íƒ€ë°ì´í„°
    execution_venue VARCHAR(50),
    execution_context JSONB, -- ì‹œìŠ¤í…œ4 execution details
    risk_metadata JSONB,
    
    -- ê°ì‚¬ ì¶”ì 
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    created_by VARCHAR(100) DEFAULT 'system4_executor'
);

-- ì¸ë±ìŠ¤ (ì‹œìŠ¤í…œ4 ê±°ë˜ ì¿¼ë¦¬ ìµœì í™”)
CREATE INDEX idx_trades_signal_id ON trades(signal_id);
CREATE INDEX idx_trades_symbol_created ON trades(symbol, created_at DESC);
CREATE INDEX idx_trades_status_composite ON trades(status, fill_status, created_at DESC);
CREATE INDEX idx_trades_leverage_mode ON trades(leverage, margin_mode);
CREATE INDEX idx_trades_pnl ON trades(total_pnl DESC);
CREATE INDEX idx_trades_active_positions ON trades(status, position_closed_at) 
    WHERE position_closed_at IS NULL;

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (í™œì„± ê±°ë˜ìš©)
CREATE INDEX idx_trades_active ON trades(symbol, status, created_at) 
    WHERE status IN ('submitted', 'filled', 'partial');

-- íŠ¸ë¦¬ê±°
CREATE TRIGGER update_trades_updated_at 
    BEFORE UPDATE ON trades 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();

-- ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ í†µê³„ ë·°
CREATE VIEW leverage_statistics AS
SELECT 
    symbol,
    leverage,
    margin_mode,
    COUNT(*) as trade_count,
    AVG(actual_position_size) as avg_position_size,
    AVG(total_pnl) as avg_pnl,
    SUM(CASE WHEN total_pnl > 0 THEN 1 ELSE 0 END)::DECIMAL / COUNT(*) as win_rate,
    MAX(total_pnl) as max_profit,
    MIN(total_pnl) as max_loss,
    AVG(roe_percent) as avg_roe
FROM trades 
WHERE status = 'filled' AND position_closed_at IS NOT NULL
GROUP BY symbol, leverage, margin_mode
ORDER BY trade_count DESC;

COMMENT ON TABLE trades IS 'Phoenix 95 ì‹œìŠ¤í…œ4 ê±°ë˜ í…Œì´ë¸”';
COMMENT ON COLUMN trades.leverage IS 'ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ ë°°ìˆ˜';
COMMENT ON COLUMN trades.margin_mode IS 'ì‹œìŠ¤í…œ4 ë§ˆì§„ ëª¨ë“œ';
```

### **infrastructure/data_storage/postgresql/schemas/03_create_positions_table.sql**
```sql
-- Phoenix 95 ì‹œìŠ¤í…œ4 - í¬ì§€ì…˜ í…Œì´ë¸” (a.txt ì™„ì „ ë³µì›)
CREATE TABLE positions (
    position_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    trade_id UUID NOT NULL REFERENCES trades(trade_id) ON DELETE CASCADE,
    signal_id UUID NOT NULL REFERENCES signals(signal_id),
    
    -- í¬ì§€ì…˜ ê¸°ë³¸ ì •ë³´
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL CHECK (side IN ('long', 'short')),
    
    -- ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ í¬ì§€ì…˜ ì •ë³´
    leverage INTEGER NOT NULL,
    margin_mode VARCHAR(20) NOT NULL,
    base_size DECIMAL(20, 8) NOT NULL,
    leveraged_size DECIMAL(20, 8) NOT NULL,
    margin_used DECIMAL(20, 8) NOT NULL,
    
    -- ê°€ê²© ì •ë³´
    entry_price DECIMAL(20, 8) NOT NULL,
    current_price DECIMAL(20, 8),
    mark_price DECIMAL(20, 8), -- ë§ˆí¬ ê°€ê²© (ì²­ì‚°ê°€ ê³„ì‚°ìš©)
    
    -- ì‹œìŠ¤í…œ4 ì†ìµ ì œí•œ
    stop_loss_price DECIMAL(20, 8) NOT NULL,
    take_profit_price DECIMAL(20, 8) NOT NULL,
    liquidation_price DECIMAL(20, 8) NOT NULL,
    
    -- ë§ˆì§„ ê´€ë¦¬
    initial_margin DECIMAL(20, 8) NOT NULL,
    maintenance_margin DECIMAL(20, 8) NOT NULL,
    margin_ratio DECIMAL(8, 4), -- í˜„ì¬ ë§ˆì§„ ë¹„ìœ¨
    liquidation_buffer DECIMAL(5, 4) DEFAULT 0.1000, -- 10% ë²„í¼
    
    -- ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ P&L
    unrealized_pnl DECIMAL(20, 8) DEFAULT 0,
    unrealized_pnl_percent DECIMAL(8, 4) DEFAULT 0,
    roe DECIMAL(8, 4) DEFAULT 0, -- Return on Equity
    
    -- ì‹¤í˜„ ì†ìµ (ë¶€ë¶„ ì²­ì‚°)
    realized_pnl DECIMAL(20, 8) DEFAULT 0,
    total_fees_paid DECIMAL(20, 8) DEFAULT 0,
    
    -- í¬ì§€ì…˜ ìƒíƒœ
    status VARCHAR(20) DEFAULT 'open' 
        CHECK (status IN ('open', 'closing', 'closed', 'liquidated', 'expired')),
    
    -- ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§
    last_monitored_at TIMESTAMPTZ DEFAULT NOW(),
    monitoring_interval_seconds INTEGER DEFAULT 5, -- ì‹œìŠ¤í…œ4: 5ì´ˆ ê°„ê²©
    alert_triggered BOOLEAN DEFAULT FALSE,
    
    -- ë¦¬ìŠ¤í¬ ì§€í‘œ
    distance_to_liquidation DECIMAL(8, 4), -- ì²­ì‚°ê°€ê¹Œì§€ì˜ ê±°ë¦¬ (%)
    position_age_hours DECIMAL(8, 2),
    max_drawdown DECIMAL(8, 4),
    max_profit DECIMAL(8, 4),
    
    -- ìë™ ì²­ì‚° (ì‹œìŠ¤í…œ4: 48ì‹œê°„ í›„)
    auto_close_at TIMESTAMPTZ DEFAULT NOW() + INTERVAL '48 hours',
    forced_close_reason VARCHAR(100),
    
    -- íƒ€ì´ë°
    opened_at TIMESTAMPTZ DEFAULT NOW(),
    closed_at TIMESTAMPTZ,
    last_price_update TIMESTAMPTZ DEFAULT NOW(),
    
    -- ë©”íƒ€ë°ì´í„°
    exchange VARCHAR(20) DEFAULT 'binance',
    position_metadata JSONB,
    monitoring_log JSONB[], -- ëª¨ë‹ˆí„°ë§ ì´ë ¥
    
    -- ê°ì‚¬ ì¶”ì 
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ (ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìµœì í™”)
CREATE INDEX idx_positions_active ON positions(status, last_monitored_at) 
    WHERE status = 'open';
CREATE INDEX idx_positions_symbol_open ON positions(symbol, status, opened_at DESC);
CREATE INDEX idx_positions_liquidation_risk ON positions(distance_to_liquidation ASC) 
    WHERE status = 'open' AND distance_to_liquidation < 10; -- 10% ì´ë‚´
CREATE INDEX idx_positions_auto_close ON positions(auto_close_at) 
    WHERE status = 'open';
CREATE INDEX idx_positions_monitoring ON positions(last_monitored_at) 
    WHERE status = 'open';

-- íŠ¸ë¦¬ê±°
CREATE TRIGGER update_positions_updated_at 
    BEFORE UPDATE ON positions 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();

-- ì‹œìŠ¤í…œ4 í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION update_position_metrics()
RETURNS TRIGGER AS $$
BEGIN
    -- í¬ì§€ì…˜ ë‚˜ì´ ê³„ì‚°
    NEW.position_age_hours = EXTRACT(EPOCH FROM (NOW() - NEW.opened_at)) / 3600;
    
    -- ì²­ì‚°ê°€ê¹Œì§€ ê±°ë¦¬ ê³„ì‚° (%)
    IF NEW.side = 'long' THEN
        NEW.distance_to_liquidation = ((NEW.current_price - NEW.liquidation_price) / NEW.current_price) * 100;
    ELSE
        NEW.distance_to_liquidation = ((NEW.liquidation_price - NEW.current_price) / NEW.current_price) * 100;
    END IF;
    
    -- ROE ê³„ì‚°
    IF NEW.margin_used > 0 THEN
        NEW.roe = (NEW.unrealized_pnl / NEW.margin_used) * 100;
    END IF;
    
    -- ë§ˆì§„ ë¹„ìœ¨ ê³„ì‚°
    IF NEW.initial_margin > 0 THEN
        NEW.margin_ratio = NEW.margin_used / NEW.initial_margin;
    END IF;
    
    -- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
    NEW.last_price_update = NOW();
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER calculate_position_metrics 
    BEFORE UPDATE ON positions 
    FOR EACH ROW 
    EXECUTE FUNCTION update_position_metrics();

-- ì‹œìŠ¤í…œ4 ì‹¤ì‹œê°„ í¬ì§€ì…˜ ë·°
CREATE VIEW active_positions AS
SELECT 
    p.*,
    t.signal_id,
    s.phoenix95_score,
    s.confidence as signal_confidence,
    CASE 
        WHEN p.distance_to_liquidation < 5 THEN 'CRITICAL'
        WHEN p.distance_to_liquidation < 10 THEN 'HIGH'
        WHEN p.distance_to_liquidation < 20 THEN 'MEDIUM'
        ELSE 'LOW'
    END as liquidation_risk_level,
    CASE 
        WHEN p.position_age_hours > 48 THEN TRUE
        ELSE FALSE
    END as should_auto_close
FROM positions p
JOIN trades t ON p.trade_id = t.trade_id
JOIN signals s ON p.signal_id = s.signal_id
WHERE p.status = 'open'
ORDER BY p.distance_to_liquidation ASC;

COMMENT ON TABLE positions IS 'Phoenix 95 ì‹œìŠ¤í…œ4 í¬ì§€ì…˜ í…Œì´ë¸”';
COMMENT ON COLUMN positions.monitoring_interval_seconds IS 'ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§ ê°„ê²© (5ì´ˆ)';
COMMENT ON COLUMN positions.auto_close_at IS 'ì‹œìŠ¤í…œ4 ìë™ ì²­ì‚° ì‹œê°„ (48ì‹œê°„ í›„)';
```

---

## ğŸ”§ **Redis Key êµ¬ì¡° ì •ì˜ (a.txt ì™„ì „ ë³µì›)**

```python
# infrastructure/data_storage/redis/key_structures.py
"""
Redis Key êµ¬ì¡° ì •ì˜ - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

from typing import Dict, List, Optional
from datetime import datetime
import json

class System4RedisKeyStructures:
    """Phoenix 95 ì‹œìŠ¤í…œ4 Redis Key êµ¬ì¡° ê´€ë¦¬"""
    
    # ì‹œìŠ¤í…œ4 í‚¤ íŒ¨í„´
    PRICE_CACHE_PATTERN = "s4:price:{symbol}:{exchange}"  # ì‹œìŠ¤í…œ4: 60ì´ˆ ìºì‹±
    SIGNAL_QUEUE_PATTERN = "s4:queue:signals:{priority}"
    ANALYSIS_CACHE_PATTERN = "s4:analysis:{signal_id}"
    POSITION_TRACKING_PATTERN = "s4:position:{position_id}:realtime"
    
    # ì„¸ì…˜ ë° ì‚¬ìš©ì
    USER_SESSION_PATTERN = "s4:session:{user_id}"
    API_RATE_LIMIT_PATTERN = "s4:rate_limit:{api_key}:{minute}"
    
    # ì‹¤ì‹œê°„ ë°ì´í„°
    MARKET_DATA_STREAM_PATTERN = "s4:stream:market:{symbol}"
    SYSTEM_METRICS_PATTERN = "s4:metrics:system:{service}:{timestamp}"
    
    # ìºì‹œ ë§Œë£Œ ì‹œê°„ (ì´ˆ) - ì‹œìŠ¤í…œ4 ìµœì í™”
    CACHE_EXPIRY = {
        "price_data": 60,        # ì‹œìŠ¤í…œ4: 60ì´ˆ ê°€ê²© ìºì‹±
        "analysis_result": 180,  # 3ë¶„
        "market_condition": 30,  # 30ì´ˆ
        "system_metrics": 15,    # 15ì´ˆ
        "user_session": 7200,    # 2ì‹œê°„
        "rate_limit": 60         # 1ë¶„
    }
    
    @classmethod
    def price_cache_key(cls, symbol: str, exchange: str = "binance") -> str:
        """ì‹œìŠ¤í…œ4 ê°€ê²© ìºì‹œ í‚¤ (60ì´ˆ TTL)"""
        return cls.PRICE_CACHE_PATTERN.format(symbol=symbol.upper(), exchange=exchange.lower())
    
    @classmethod
    def signal_queue_key(cls, priority: str = "normal") -> str:
        """ì‹ í˜¸ í í‚¤"""
        return cls.SIGNAL_QUEUE_PATTERN.format(priority=priority)
    
    @classmethod
    def analysis_cache_key(cls, signal_id: str) -> str:
        """Phoenix 95 ë¶„ì„ ê²°ê³¼ ìºì‹œ"""
        return cls.ANALYSIS_CACHE_PATTERN.format(signal_id=signal_id)
    
    @classmethod
    def position_tracking_key(cls, position_id: str) -> str:
        """ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì  í‚¤"""
        return cls.POSITION_TRACKING_PATTERN.format(position_id=position_id)
    
    @classmethod
    def active_positions_key(cls) -> str:
        """í™œì„± í¬ì§€ì…˜ ì§‘í•© í‚¤"""
        return "s4:positions:active"
    
    @classmethod
    def user_session_key(cls, user_id: str) -> str:
        """ì‚¬ìš©ì ì„¸ì…˜ í‚¤"""
        return cls.USER_SESSION_PATTERN.format(user_id=user_id)
    
    @classmethod
    def rate_limit_key(cls, api_key: str, minute: int = None) -> str:
        """API ì†ë„ ì œí•œ í‚¤"""
        if minute is None:
            minute = int(datetime.now().timestamp() // 60)
        return cls.API_RATE_LIMIT_PATTERN.format(api_key=api_key, minute=minute)
    
    @classmethod
    def market_stream_key(cls, symbol: str) -> str:
        """ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ í‚¤"""
        return cls.MARKET_DATA_STREAM_PATTERN.format(symbol=symbol.upper())

# ì‹œìŠ¤í…œ4 í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
class System4DataStructures:
    """ì‹œìŠ¤í…œ4 ë°ì´í„° êµ¬ì¡°"""
    
    @staticmethod
    def price_data_structure(symbol: str, price: float, timestamp: datetime) -> Dict:
        """ì‹œìŠ¤í…œ4 ê°€ê²© ë°ì´í„° êµ¬ì¡°"""
        return {
            "symbol": symbol,
            "price": price,
            "timestamp": timestamp.isoformat(),
            "source": "binance",
            "cached_at": datetime.now().isoformat(),
            "ttl": 60,  # ì‹œìŠ¤í…œ4: 60ì´ˆ
            "system_version": "4.0"
        }
    
    @staticmethod
    def analysis_result_structure(signal_id: str, analysis_data: Dict) -> Dict:
        """ì‹œìŠ¤í…œ4 ë¶„ì„ ê²°ê³¼ êµ¬ì¡°"""
        return {
            "signal_id": signal_id,
            "analysis_type": analysis_data.get("analysis_type", "PHOENIX_95_SYSTEM4"),
            "final_confidence": analysis_data.get("final_confidence", 0.0),
            "phoenix95_score": analysis_data.get("phoenix95_score"),
            "execution_timing": analysis_data.get("execution_timing", "HOLD"),
            "leverage_analysis": analysis_data.get("leverage_analysis", {}),
            "cached_at": datetime.now().isoformat(),
            "ttl": 180,  # ì‹œìŠ¤í…œ4: 3ë¶„
            "system_version": "4.0"
        }
    
    @staticmethod
    def position_data_structure(position_id: str, position_data: Dict) -> Dict:
        """ì‹œìŠ¤í…œ4 í¬ì§€ì…˜ ë°ì´í„° êµ¬ì¡°"""
        return {
            "position_id": position_id,
            "symbol": position_data.get("symbol"),
            "side": position_data.get("side"),
            "leverage": position_data.get("leverage", 20),
            "margin_mode": position_data.get("margin_mode", "ISOLATED"),
            "entry_price": position_data.get("entry_price"),
            "current_price": position_data.get("current_price"),
            "unrealized_pnl": position_data.get("unrealized_pnl", 0),
            "margin_ratio": position_data.get("margin_ratio", 0),
            "liquidation_price": position_data.get("liquidation_price"),
            "stop_loss_price": position_data.get("stop_loss_price"),
            "take_profit_price": position_data.get("take_profit_price"),
            "last_updated": datetime.now().isoformat(),
            "monitoring_interval": 5,  # ì‹œìŠ¤í…œ4: 5ì´ˆ
            "system_version": "4.0"
        }

# Redis ì—°ê²° ë° ê´€ë¦¬ í´ë˜ìŠ¤ (a.txt ì™„ì „ ë³µì›)
class System4RedisManager:
    """ì‹œìŠ¤í…œ4 Redis ì—°ê²° ë° ë°ì´í„° ê´€ë¦¬"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.keys = System4RedisKeyStructures()
        self.structures = System4DataStructures()
    
    async def cache_price_data(self, symbol: str, price: float, exchange: str = "binance"):
        """ì‹œìŠ¤í…œ4 ê°€ê²© ë°ì´í„° ìºì‹± (60ì´ˆ)"""
        key = self.keys.price_cache_key(symbol, exchange)
        data = self.structures.price_data_structure(symbol, price, datetime.now())
        
        await self.redis.setex(
            key, 
            self.keys.CACHE_EXPIRY["price_data"], 
            json.dumps(data)
        )
    
    async def get_cached_price(self, symbol: str, exchange: str = "binance") -> Optional[Dict]:
        """ì‹œìŠ¤í…œ4 ìºì‹œëœ ê°€ê²© ì¡°íšŒ"""
        key = self.keys.price_cache_key(symbol, exchange)
        cached_data = await self.redis.get(key)
        
        if cached_data:
            return json.loads(cached_data)
        return None
    
    async def cache_analysis_result(self, signal_id: str, analysis_data: Dict):
        """Phoenix 95 ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        key = self.keys.analysis_cache_key(signal_id)
        data = self.structures.analysis_result_structure(signal_id, analysis_data)
        
        await self.redis.setex(
            key,
            self.keys.CACHE_EXPIRY["analysis_result"],
            json.dumps(data)
        )
    
    async def update_position_realtime(self, position_id: str, position_data: Dict):
        """ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì—…ë°ì´íŠ¸ (ì‹œìŠ¤í…œ4 5ì´ˆ ê°„ê²©)"""
        key = self.keys.position_tracking_key(position_id)
        data = self.structures.position_data_structure(position_id, position_data)
        
        # í™œì„± í¬ì§€ì…˜ ì§‘í•©ì— ì¶”ê°€
        await self.redis.sadd(self.keys.active_positions_key(), position_id)
        
        # í¬ì§€ì…˜ ë°ì´í„° ì €ì¥
        await self.redis.hset(key, mapping=data)
    
    async def get_active_positions(self) -> List[str]:
        """í™œì„± í¬ì§€ì…˜ ëª©ë¡ ì¡°íšŒ"""
        return await self.redis.smembers(self.keys.active_positions_key())
    
    async def enqueue_signal(self, signal_data: Dict, priority: str = "normal"):
        """ì‹ í˜¸ íì— ì¶”ê°€"""
        key = self.keys.signal_queue_key(priority)
        await self.redis.lpush(key, json.dumps(signal_data))
    
    async def dequeue_signal(self, priority: str = "normal") -> Optional[Dict]:
        """ì‹ í˜¸ íì—ì„œ ì œê±°"""
        key = self.keys.signal_queue_key(priority)
        signal_data = await self.redis.rpop(key)
        
        if signal_data:
            return json.loads(signal_data)
        return None
    
    async def check_rate_limit(self, api_key: str, limit: int = 200) -> bool:
        """API ì†ë„ ì œí•œ ì²´í¬ (ì‹œìŠ¤í…œ4: 200/ë¶„)"""
        key = self.keys.rate_limit_key(api_key)
        current_count = await self.redis.get(key)
        
        if current_count is None:
            await self.redis.setex(key, 60, 1)
            return True
        elif int(current_count) < limit:
            await self.redis.incr(key)
            return True
        else:
            return False  # ì†ë„ ì œí•œ ì´ˆê³¼
```

---

## ğŸ“Š **InfluxDB Measurements ì„¤ê³„ (a.txt ì™„ì „ ë³µì›)**

```python
# infrastructure/data_storage/influxdb/measurements/price_data.py
"""
InfluxDB ê°€ê²© ë°ì´í„° Measurement ì •ì˜ - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

from influxdb_client import Point
from datetime import datetime
from typing import Dict, List

class System4PriceDataMeasurement:
    """ì‹œìŠ¤í…œ4 ê°€ê²© ë°ì´í„° ì¸¡ì •ê°’ ì •ì˜"""
    
    MEASUREMENT_NAME = "s4_price_data"
    
    @classmethod
    def create_price_point(cls, symbol: str, price_data: Dict) -> Point:
        """ê°€ê²© ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±"""
        point = Point(cls.MEASUREMENT_NAME)
        
        # Tags (ì¸ë±ì‹±ë¨)
        point.tag("symbol", symbol.upper())
        point.tag("exchange", price_data.get("exchange", "binance"))
        point.tag("market_type", price_data.get("market_type", "spot"))
        point.tag("system_version", "4.0")
        
        # Fields (ê°’)
        point.field("price", float(price_data["price"]))
        point.field("bid", float(price_data.get("bid", 0)))
        point.field("ask", float(price_data.get("ask", 0)))
        point.field("volume", float(price_data.get("volume", 0)))
        point.field("volume_24h", float(price_data.get("volume_24h", 0)))
        point.field("change_24h", float(price_data.get("change_24h", 0)))
        point.field("change_percent_24h", float(price_data.get("change_percent_24h", 0)))
        
        # ê¸°ìˆ ì  ì§€í‘œ
        if "rsi" in price_data:
            point.field("rsi", float(price_data["rsi"]))
        if "macd" in price_data:
            point.field("macd", float(price_data["macd"]))
        if "bollinger_upper" in price_data:
            point.field("bollinger_upper", float(price_data["bollinger_upper"]))
            point.field("bollinger_lower", float(price_data["bollinger_lower"]))
        
        # ì‹œìŠ¤í…œ4 ì „ìš© í•„ë“œ
        if "volatility" in price_data:
            point.field("volatility", float(price_data["volatility"]))
        if "momentum" in price_data:
            point.field("momentum", float(price_data["momentum"]))
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        point.time(price_data.get("timestamp", datetime.now()))
        
        return point

class System4TradeMeasurement:
    """ì‹œìŠ¤í…œ4 ê±°ë˜ ë©”íŠ¸ë¦­ ì¸¡ì •ê°’"""
    
    MEASUREMENT_NAME = "s4_trade_metrics"
    
    @classmethod
    def create_trade_point(cls, trade_data: Dict) -> Point:
        """ê±°ë˜ ë©”íŠ¸ë¦­ í¬ì¸íŠ¸ ìƒì„±"""
        point = Point(cls.MEASUREMENT_NAME)
        
        # Tags
        point.tag("symbol", trade_data["symbol"])
        point.tag("side", trade_data["side"])
        point.tag("leverage", str(trade_data.get("leverage", 1)))
        point.tag("margin_mode", trade_data.get("margin_mode", "ISOLATED"))
        point.tag("strategy", trade_data.get("strategy", "unknown"))
        point.tag("exchange", trade_data.get("exchange", "binance"))
        point.tag("system_version", "4.0")
        
        # Fields
        point.field("position_size", float(trade_data["position_size"]))
        point.field("entry_price", float(trade_data["entry_price"]))
        point.field("exit_price", float(trade_data.get("exit_price", 0)))
        point.field("pnl", float(trade_data.get("pnl", 0)))
        point.field("pnl_percent", float(trade_data.get("pnl_percent", 0)))
        point.field("roe", float(trade_data.get("roe", 0)))
        point.field("fees_paid", float(trade_data.get("fees_paid", 0)))
        point.field("slippage", float(trade_data.get("slippage", 0)))
        point.field("confidence", float(trade_data.get("confidence", 0)))
        point.field("phoenix95_score", float(trade_data.get("phoenix95_score", 0)))
        
        # ì‹œìŠ¤í…œ4 ì „ìš© ë©”íŠ¸ë¦­
        point.field("execution_time_ms", float(trade_data.get("execution_time_ms", 0)))
        point.field("market_impact", float(trade_data.get("market_impact", 0)))
        
        point.time(trade_data.get("timestamp", datetime.now()))
        
        return point

class System4MetricsMeasurement:
    """ì‹œìŠ¤í…œ4 ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¸¡ì •ê°’"""
    
    MEASUREMENT_NAME = "s4_system_metrics"
    
    @classmethod
    def create_system_point(cls, service_name: str, metrics: Dict) -> Point:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ í¬ì¸íŠ¸ ìƒì„±"""
        point = Point(cls.MEASUREMENT_NAME)
        
        # Tags
        point.tag("service", service_name)
        point.tag("host", metrics.get("host", "localhost"))
        point.tag("environment", metrics.get("environment", "production"))
        point.tag("system_version", "4.0")
        
        # CPU ë©”íŠ¸ë¦­
        if "cpu" in metrics:
            point.field("cpu_percent", float(metrics["cpu"]["percent"]))
            point.field("cpu_count", int(metrics["cpu"]["count"]))
        
        # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
        if "memory" in metrics:
            point.field("memory_percent", float(metrics["memory"]["percent"]))
            point.field("memory_used_mb", float(metrics["memory"]["used_mb"]))
            point.field("memory_available_mb", float(metrics["memory"]["available_mb"]))
        
        # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
        if "network" in metrics:
            point.field("network_sent_mb", float(metrics["network"]["sent_mb"]))
            point.field("network_recv_mb", float(metrics["network"]["recv_mb"]))
        
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­
        if "app" in metrics:
            point.field("requests_per_second", float(metrics["app"]["requests_per_second"]))
            point.field("response_time_ms", float(metrics["app"]["response_time_ms"]))
            point.field("error_rate", float(metrics["app"]["error_rate"]))
            point.field("active_connections", int(metrics["app"]["active_connections"]))
        
        # ì‹œìŠ¤í…œ4 ì „ìš© ë©”íŠ¸ë¦­
        if "s4" in metrics:
            point.field("ai_inference_time_ms", float(metrics["s4"]["ai_inference_time_ms"]))
            point.field("signal_processing_rate", float(metrics["s4"]["signal_processing_rate"]))
            point.field("position_updates_per_second", float(metrics["s4"]["position_updates_per_second"]))
        
        point.time(metrics.get("timestamp", datetime.now()))
        
        return point

class System4RiskMetricsMeasurement:
    """ì‹œìŠ¤í…œ4 ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ ì¸¡ì •ê°’"""
    
    MEASUREMENT_NAME = "s4_risk_metrics"
    
    @classmethod
    def create_risk_point(cls, portfolio_data: Dict) -> Point:
        """ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ í¬ì¸íŠ¸ ìƒì„±"""
        point = Point(cls.MEASUREMENT_NAME)
        
        # Tags
        point.tag("portfolio_id", portfolio_data.get("portfolio_id", "default"))
        point.tag("risk_model", portfolio_data.get("risk_model", "var"))
        point.tag("system_version", "4.0")
        
        # VaR ë©”íŠ¸ë¦­
        point.field("var_1d_95", float(portfolio_data.get("var_1d_95", 0)))
        point.field("var_1d_99", float(portfolio_data.get("var_1d_99", 0)))
        point.field("cvar_1d_95", float(portfolio_data.get("cvar_1d_95", 0)))
        point.field("expected_shortfall", float(portfolio_data.get("expected_shortfall", 0)))
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ë©”íŠ¸ë¦­
        point.field("total_value", float(portfolio_data.get("total_value", 0)))
        point.field("leverage_ratio", float(portfolio_data.get("leverage_ratio", 0)))
        point.field("concentration_risk", float(portfolio_data.get("concentration_risk", 0)))
        point.field("correlation_risk", float(portfolio_data.get("correlation_risk", 0)))
        
        # ë“œë¡œìš°ë‹¤ìš´
        point.field("current_drawdown", float(portfolio_data.get("current_drawdown", 0)))
        point.field("max_drawdown", float(portfolio_data.get("max_drawdown", 0)))
        
        # Kelly Criterion
        point.field("kelly_fraction", float(portfolio_data.get("kelly_fraction", 0)))
        point.field("optimal_leverage", float(portfolio_data.get("optimal_leverage", 0)))
        
        # ì‹œìŠ¤í…œ4 ì „ìš© ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­
        point.field("tail_risk", float(portfolio_data.get("tail_risk", 0)))
        point.field("stress_test_result", float(portfolio_data.get("stress_test_result", 0)))
        
        point.time(portfolio_data.get("timestamp", datetime.now()))
        
        return point

# InfluxDB í´ë¼ì´ì–¸íŠ¸ ë˜í¼ (a.txt ì™„ì „ ë³µì›)
class System4InfluxDBManager:
    """ì‹œìŠ¤í…œ4 InfluxDB ì—°ê²° ë° ë°ì´í„° ê´€ë¦¬"""
    
    def __init__(self, influx_client, bucket: str, org: str):
        self.client = influx_client
        self.bucket = bucket
        self.org = org
        self.write_api = influx_client.write_api()
        self.query_api = influx_client.query_api()
    
    async def write_price_data(self, symbol: str, price_data: Dict):
        """ê°€ê²© ë°ì´í„° ì €ì¥"""
        point = System4PriceDataMeasurement.create_price_point(symbol, price_data)
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def write_trade_metrics(self, trade_data: Dict):
        """ê±°ë˜ ë©”íŠ¸ë¦­ ì €ì¥"""
        point = System4TradeMeasurement.create_trade_point(trade_data)
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def write_system_metrics(self, service_name: str, metrics: Dict):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì €ì¥"""
        point = System4MetricsMeasurement.create_system_point(service_name, metrics)
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def write_risk_metrics(self, portfolio_data: Dict):
        """ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ ì €ì¥"""
        point = System4RiskMetricsMeasurement.create_risk_point(portfolio_data)
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
    
    async def query_price_history(self, symbol: str, timeframe: str = "1h", 
                                limit: int = 100) -> List[Dict]:
        """ê°€ê²© ì´ë ¥ ì¡°íšŒ"""
        query = f'''
        from(bucket: "{self.bucket}")
        |> range(start: -{timeframe})
        |> filter(fn: (r) => r._measurement == "s4_price_data")
        |> filter(fn: (r) => r.symbol == "{symbol}")
        |> filter(fn: (r) => r._field == "price")
        |> sort(columns: ["_time"], desc: true)
        |> limit(n: {limit})
        '''
        
        result = self.query_api.query(query, org=self.org)
        
        price_history = []
        for table in result:
            for record in table.records:
                price_history.append({
                    "timestamp": record.get_time(),
                    "price": record.get_value(),
                    "symbol": record.values.get("symbol")
                })
        
        return price_history
    
    async def get_system_performance_metrics(self, service_name: str = None) -> Dict:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        service_filter = f'|> filter(fn: (r) => r.service == "{service_name}")' if service_name else ''
        
        query = f'''
        from(bucket: "{self.bucket}")
        |> range(start: -1h)
        |> filter(fn: (r) => r._measurement == "s4_system_metrics")
        {service_filter}
        |> filter(fn: (r) => r._field == "requests_per_second" or r._field == "response_time_ms" or r._field == "error_rate")
        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
        '''
        
        result = self.query_api.query(query, org=self.org)
        
        metrics = {}
        for table in result:
            for record in table.records:
                field = record.get_field()
                if field not in metrics:
                    metrics[field] = []
                metrics[field].append({
                    "timestamp": record.get_time(),
                    "value": record.get_value()
                })
        
        return metrics
```

---

## ğŸ› ï¸ **ì¸í”„ë¼ ìë™í™” ë„êµ¬ë“¤ (a.txt ì™„ì „ ë³µì›)**

```python
# tools/setup_postgresql.py
"""
ğŸ’¾ PostgreSQL ìë™ ì„¤ì • ë° ë§ˆì´ê·¸ë ˆì´ì…˜ - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

import asyncio
import asyncpg
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class System4PostgreSQLSetup:
    """ì‹œìŠ¤í…œ4 PostgreSQL ìë™ ì„¤ì •"""
    
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.schema_path = Path('infrastructure/data_storage/postgresql/schemas')
    
    async def create_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì‹œì‘")
        
        conn = await asyncpg.connect(self.db_url)
        
        # DDL ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìˆœì„œ
        ddl_files = [
            '01_create_signals_table.sql',
            '02_create_trades_table.sql', 
            '03_create_positions_table.sql',
            '04_create_risk_metrics_table.sql',
            '05_create_notifications_table.sql',
            '06_create_audit_logs_table.sql',
            '07_create_system_metrics_table.sql',
            '08_create_user_sessions_table.sql',
            '09_create_configuration_table.sql',
            '10_create_indexes_and_constraints.sql'
        ]
        
        for ddl_file in ddl_files:
            ddl_path = self.schema_path / ddl_file
            if ddl_path.exists():
                logger.info(f"ì‹¤í–‰ ì¤‘: {ddl_file}")
                ddl_content = ddl_path.read_text()
                await conn.execute(ddl_content)
                logger.info(f"âœ… {ddl_file} ì‹¤í–‰ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ {ddl_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        await conn.close()
        logger.info("ì‹œìŠ¤í…œ4 PostgreSQL ì„¤ì • ì™„ë£Œ")
    
    async def run_migrations(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        logger.info("ì‹œìŠ¤í…œ4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰")
        
        migration_path = self.schema_path / "migrations"
        if not migration_path.exists():
            logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        conn = await asyncpg.connect(self.db_url)
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ í…Œì´ë¸” ìƒì„±
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS schema_migrations (
                version VARCHAR(255) PRIMARY KEY,
                applied_at TIMESTAMPTZ DEFAULT NOW()
            )
        """)
        
        # ì ìš©ëœ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¡°íšŒ
        applied_migrations = await conn.fetch("SELECT version FROM schema_migrations")
        applied_versions = {row['version'] for row in applied_migrations}
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ì‹¤í–‰
        migration_files = sorted(migration_path.glob("*.sql"))
        for migration_file in migration_files:
            version = migration_file.stem
            if version not in applied_versions:
                logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš© ì¤‘: {version}")
                migration_content = migration_file.read_text()
                await conn.execute(migration_content)
                await conn.execute(
                    "INSERT INTO schema_migrations (version) VALUES ($1)",
                    version
                )
                logger.info(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {version}")
        
        await conn.close()
        logger.info("ì‹œìŠ¤í…œ4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
    
    async def create_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
        
        conn = await asyncpg.connect(self.db_url)
        
        # í…ŒìŠ¤íŠ¸ ì‹ í˜¸ ìƒì„±
        test_signals = [
            {
                "symbol": "BTCUSDT",
                "action": "buy",
                "price": 45000.0,
                "confidence": 0.85,
                "strategy": "momentum"
            },
            {
                "symbol": "ETHUSDT", 
                "action": "sell",
                "price": 3200.0,
                "confidence": 0.75,
                "strategy": "mean_reversion"
            }
        ]
        
        for signal in test_signals:
            await conn.execute("""
                INSERT INTO signals (symbol, action, price, confidence, strategy)
                VALUES ($1, $2, $3, $4, $5)
            """, signal["symbol"], signal["action"], signal["price"], 
                signal["confidence"], signal["strategy"])
        
        await conn.close()
        logger.info("ì‹œìŠ¤í…œ4 í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")

# tools/setup_redis.py
"""
âš¡ Redis ìë™ ì„¤ì • ë° í‚¤ êµ¬ì¡° ì´ˆê¸°í™” - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

import redis.asyncio as redis
import json
import logging

logger = logging.getLogger(__name__)

class System4RedisSetup:
    """ì‹œìŠ¤í…œ4 Redis ìë™ ì„¤ì •"""
    
    def __init__(self, redis_url: str):
        self.redis_url = redis_url
    
    async def configure_keys(self):
        """í‚¤ êµ¬ì¡° ì„¤ì • ë° í…ŒìŠ¤íŠ¸"""
        logger.info("ì‹œìŠ¤í…œ4 Redis í‚¤ êµ¬ì¡° ì„¤ì • ì‹œì‘")
        
        client = redis.from_url(self.redis_url)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = {
            "s4:price:BTCUSDT:binance": {
                "price": 45000.0, 
                "timestamp": "2025-01-01T00:00:00",
                "system_version": "4.0"
            },
            "s4:queue:signals:normal": [],
            "s4:positions:active": set(),
            "s4:session:test_user": {
                "user_id": "test_user",
                "logged_in_at": "2025-01-01T00:00:00",
                "system_version": "4.0"
            }
        }
        
        for key, value in test_data.items():
            try:
                if isinstance(value, set):
                    if value:
                        await client.sadd(key, *value)
                elif isinstance(value, list):
                    if value:
                        await client.lpush(key, *[json.dumps(item) for item in value])
                else:
                    await client.setex(key, 60, json.dumps(value))  # ì‹œìŠ¤í…œ4: 60ì´ˆ TTL
                
                logger.info(f"âœ… Redis í‚¤ ì„¤ì •: {key}")
            except Exception as e:
                logger.error(f"âŒ Redis í‚¤ ì„¤ì • ì‹¤íŒ¨ {key}: {e}")
        
        await client.close()
        logger.info("ì‹œìŠ¤í…œ4 Redis í‚¤ êµ¬ì¡° ì„¤ì • ì™„ë£Œ")
    
    async def setup_lua_scripts(self):
        """Lua ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •"""
        logger.info("ì‹œìŠ¤í…œ4 Redis Lua ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •")
        
        client = redis.from_url(self.redis_url)
        
        # ì›ìì  ì¹´ìš´í„° ìŠ¤í¬ë¦½íŠ¸
        atomic_counter_script = """
        local key = KEYS[1]
        local increment = tonumber(ARGV[1])
        local ttl = tonumber(ARGV[2])
        
        local current = redis.call('GET', key)
        if not current then
            current = 0
        else
            current = tonumber(current)
        end
        
        local new_value = current + increment
        redis.call('SETEX', key, ttl, new_value)
        return new_value
        """
        
        # ìŠ¤í¬ë¦½íŠ¸ ë“±ë¡
        script_sha = await client.script_load(atomic_counter_script)
        logger.info(f"âœ… Lua ìŠ¤í¬ë¦½íŠ¸ ë“±ë¡: {script_sha}")
        
        await client.close()
        logger.info("ì‹œìŠ¤í…œ4 Redis Lua ìŠ¤í¬ë¦½íŠ¸ ì„¤ì • ì™„ë£Œ")
    
    async def test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ì‹œìŠ¤í…œ4 Redis ì—°ê²° í…ŒìŠ¤íŠ¸")
        
        try:
            client = redis.from_url(self.redis_url)
            
            # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
            await client.ping()
            logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")
            
            # ì½ê¸°/ì“°ê¸° í…ŒìŠ¤íŠ¸
            test_key = "s4:test:connection"
            test_value = {"test": True, "timestamp": "2025-01-01T00:00:00"}
            
            await client.setex(test_key, 10, json.dumps(test_value))
            retrieved_value = await client.get(test_key)
            
            if retrieved_value:
                parsed_value = json.loads(retrieved_value)
                assert parsed_value["test"] == True
                logger.info("âœ… Redis ì½ê¸°/ì“°ê¸° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
            # ì •ë¦¬
            await client.delete(test_key)
            await client.close()
            
        except Exception as e:
            logger.error(f"âŒ Redis ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
        
        logger.info("ì‹œìŠ¤í…œ4 Redis ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

# tools/setup_influxdb.py
"""
ğŸ“Š InfluxDB ìë™ ì„¤ì • - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

from influxdb_client import InfluxDBClient, BucketRetentionRules
from influxdb_client.client.write_api import SYNCHRONOUS
import logging

logger = logging.getLogger(__name__)

class System4InfluxDBSetup:
    """ì‹œìŠ¤í…œ4 InfluxDB ìë™ ì„¤ì •"""
    
    def __init__(self, url: str, token: str, org: str):
        self.url = url
        self.token = token
        self.org = org
        self.client = InfluxDBClient(url=url, token=token, org=org)
    
    async def create_buckets(self):
        """ë²„í‚· ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ë²„í‚· ìƒì„±")
        
        buckets_api = self.client.buckets_api()
        
        # ì‹œìŠ¤í…œ4 ì „ìš© ë²„í‚·ë“¤
        buckets_config = [
            {
                "name": "s4_trading_data",
                "description": "ì‹œìŠ¤í…œ4 ê±°ë˜ ë°ì´í„°",
                "retention_period": 86400 * 365  # 1ë…„
            },
            {
                "name": "s4_market_data", 
                "description": "ì‹œìŠ¤í…œ4 ì‹œì¥ ë°ì´í„°",
                "retention_period": 86400 * 90   # 90ì¼
            },
            {
                "name": "s4_system_metrics",
                "description": "ì‹œìŠ¤í…œ4 ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­",
                "retention_period": 86400 * 30   # 30ì¼
            },
            {
                "name": "s4_risk_metrics",
                "description": "ì‹œìŠ¤í…œ4 ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­", 
                "retention_period": 86400 * 180  # 180ì¼
            }
        ]
        
        for bucket_config in buckets_config:
            try:
                # ê¸°ì¡´ ë²„í‚· í™•ì¸
                existing_buckets = buckets_api.find_buckets()
                bucket_exists = any(b.name == bucket_config["name"] for b in existing_buckets)
                
                if not bucket_exists:
                    # ë²„í‚· ìƒì„±
                    retention_rules = BucketRetentionRules(
                        type="expire",
                        every_seconds=bucket_config["retention_period"]
                    )
                    
                    bucket = buckets_api.create_bucket(
                        bucket_name=bucket_config["name"],
                        description=bucket_config["description"],
                        org=self.org,
                        retention_rules=retention_rules
                    )
                    
                    logger.info(f"âœ… ë²„í‚· ìƒì„±: {bucket.name}")
                else:
                    logger.info(f"â„¹ï¸ ë²„í‚· ì´ë¯¸ ì¡´ì¬: {bucket_config['name']}")
                    
            except Exception as e:
                logger.error(f"âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨ {bucket_config['name']}: {e}")
        
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ë²„í‚· ìƒì„± ì™„ë£Œ")
    
    async def configure_measurements(self):
        """ì¸¡ì •ê°’ ì„¤ì •"""
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ì¸¡ì •ê°’ ì„¤ì •")
        
        write_api = self.client.write_api(write_options=SYNCHRONOUS)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        from infrastructure.data_storage.influxdb.measurements.price_data import System4PriceDataMeasurement
        
        test_price_data = {
            "symbol": "BTCUSDT",
            "price": 45000.0,
            "exchange": "binance",
            "volume": 1000000,
            "rsi": 65.5,
            "macd": 0.0045
        }
        
        try:
            point = System4PriceDataMeasurement.create_price_point("BTCUSDT", test_price_data)
            write_api.write(bucket="s4_market_data", org=self.org, record=point)
            logger.info("âœ… í…ŒìŠ¤íŠ¸ ì¸¡ì •ê°’ ìƒì„± ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì¸¡ì •ê°’ ìƒì„± ì‹¤íŒ¨: {e}")
        
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ì¸¡ì •ê°’ ì„¤ì • ì™„ë£Œ")
    
    async def setup_continuous_queries(self):
        """ì—°ì† ì¿¼ë¦¬ ì„¤ì •"""
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ì—°ì† ì¿¼ë¦¬ ì„¤ì •")
        
        # ì‹œìŠ¤í…œ4ìš© ë‹¤ìš´ìƒ˜í”Œë§ ì‘ì—… ì„¤ì •
        tasks_api = self.client.tasks_api()
        
        # 1ë¶„ ì§‘ê³„ ì‘ì—…
        task_flux = '''
        option task = {name: "s4_price_1m_aggregation", every: 1m}
        
        from(bucket: "s4_market_data")
            |> range(start: -2m)
            |> filter(fn: (r) => r._measurement == "s4_price_data")
            |> aggregateWindow(every: 1m, fn: mean, createEmpty: false)
            |> to(bucket: "s4_market_data", org: "phoenix95")
        '''
        
        try:
            task = tasks_api.create_task_every(
                task_flux,
                "1m",
                name="s4_price_1m_aggregation",
                description="ì‹œìŠ¤í…œ4 1ë¶„ ê°€ê²© ì§‘ê³„"
            )
            logger.info(f"âœ… ì—°ì† ì¿¼ë¦¬ ìƒì„±: {task.name}")
        except Exception as e:
            logger.error(f"âŒ ì—°ì† ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        logger.info("ì‹œìŠ¤í…œ4 InfluxDB ì—°ì† ì¿¼ë¦¬ ì„¤ì • ì™„ë£Œ")
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()

# tools/setup_monitoring.py
"""
ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ìë™ ì„¤ì • - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

import yaml
import json
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class System4MonitoringSetup:
    """ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ìë™ ì„¤ì •"""
    
    def __init__(self):
        self.monitoring_path = Path('infrastructure/monitoring')
        self.monitoring_path.mkdir(parents=True, exist_ok=True)
    
    def setup_prometheus(self):
        """Prometheus ì„¤ì • ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 Prometheus ì„¤ì • ìƒì„±")
        
        prometheus_config = {
            'global': {
                'scrape_interval': '15s',
                'evaluation_interval': '15s'
            },
            'rule_files': [
                'rules/*.yml'
            ],
            'scrape_configs': [
                {
                    'job_name': 's4-phoenix95-services',
                    'static_configs': [
                        {'targets': [
                            'localhost:8100',  # api-gateway
                            'localhost:8101',  # signal-ingestion
                            'localhost:8102',  # market-data
                            'localhost:8103',  # ai-engine
                            'localhost:8104',  # risk-management
                            'localhost:8105',  # portfolio-optimizer
                            'localhost:8106',  # trade-execution
                            'localhost:8107',  # position-tracker
                            'localhost:8108',  # compliance-monitor
                            'localhost:8109',  # notification-hub
                            'localhost:8110'   # client-dashboard
                        ]}
                    ],
                    'metrics_path': '/metrics',
                    'scrape_interval': '10s'
                },
                {
                    'job_name': 's4-infrastructure',
                    'static_configs': [
                        {'targets': [
                            'localhost:5432',  # postgresql
                            'localhost:6379',  # redis
                            'localhost:8086'   # influxdb
                        ]}
                    ],
                    'scrape_interval': '30s'
                }
            ],
            'alerting': {
                'alertmanagers': [
                    {
                        'static_configs': [
                            {'targets': ['localhost:9093']}
                        ]
                    }
                ]
            }
        }
        
        config_path = self.monitoring_path / 'prometheus.yml'
        with open(config_path, 'w') as f:
            yaml.dump(prometheus_config, f, default_flow_style=False)
        
        logger.info(f"âœ… Prometheus ì„¤ì • ìƒì„±: {config_path}")
    
    def setup_grafana_dashboards(self):
        """Grafana ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 Grafana ëŒ€ì‹œë³´ë“œ ìƒì„±")
        
        dashboard_path = self.monitoring_path / 'grafana' / 'dashboards'
        dashboard_path.mkdir(parents=True, exist_ok=True)
        
        # ì‹œìŠ¤í…œ4 ë©”ì¸ ëŒ€ì‹œë³´ë“œ
        main_dashboard = {
            "dashboard": {
                "title": "Phoenix 95 ì‹œìŠ¤í…œ4 - ë©”ì¸ ëŒ€ì‹œë³´ë“œ",
                "tags": ["phoenix95", "system4", "trading"],
                "timezone": "UTC",
                "panels": [
                    {
                        "title": "Phoenix 95 ì‹ ë¢°ë„ ë¶„í¬",
                        "type": "histogram",
                        "targets": [{
                            "expr": "phoenix95_confidence_score",
                            "legendFormat": "ì‹ ë¢°ë„ ì ìˆ˜"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
                    },
                    {
                        "title": "ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ ê±°ë˜ í˜„í™©",
                        "type": "stat",
                        "targets": [{
                            "expr": "sum(rate(s4_leverage_trades_total[5m]))",
                            "legendFormat": "ê±°ë˜/ë¶„"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
                    },
                    {
                        "title": "ì‹¤ì‹œê°„ P&L (ì‹œìŠ¤í…œ4)",
                        "type": "graph",
                        "targets": [{
                            "expr": "s4_unrealized_pnl",
                            "legendFormat": "{{symbol}} PnL"
                        }],
                        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
                    },
                    {
                        "title": "ì‹œìŠ¤í…œ4 ì„±ëŠ¥ ë©”íŠ¸ë¦­",
                        "type": "graph",
                        "targets": [
                            {
                                "expr": "s4_ai_inference_time_ms",
                                "legendFormat": "AI ì¶”ë¡  ì‹œê°„ (ms)"
                            },
                            {
                                "expr": "s4_signal_processing_rate", 
                                "legendFormat": "ì‹ í˜¸ ì²˜ë¦¬ìœ¨ (/s)"
                            },
                            {
                                "expr": "s4_position_updates_per_second",
                                "legendFormat": "í¬ì§€ì…˜ ì—…ë°ì´íŠ¸ (/s)"
                            }
                        ],
                        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
                    }
                ],
                "time": {"from": "now-1h", "to": "now"},
                "refresh": "5s"
            }
        }
        
        dashboard_file = dashboard_path / 'phoenix95_system4_main.json'
        with open(dashboard_file, 'w') as f:
            json.dump(main_dashboard, f, indent=2)
        
        logger.info(f"âœ… Grafana ëŒ€ì‹œë³´ë“œ ìƒì„±: {dashboard_file}")
        
        # ì‹œìŠ¤í…œ4 ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ
        risk_dashboard = {
            "dashboard": {
                "title": "Phoenix 95 ì‹œìŠ¤í…œ4 - ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§",
                "tags": ["phoenix95", "system4", "risk"],
                "panels": [
                    {
                        "title": "VaR ì¶”ì´",
                        "type": "graph",
                        "targets": [
                            {"expr": "s4_var_1d_95", "legendFormat": "VaR 95%"},
                            {"expr": "s4_var_1d_99", "legendFormat": "VaR 99%"}
                        ]
                    },
                    {
                        "title": "ì²­ì‚° ë¦¬ìŠ¤í¬ ë¶„í¬",
                        "type": "heatmap",
                        "targets": [{
                            "expr": "s4_distance_to_liquidation",
                            "legendFormat": "ì²­ì‚°ê°€ê¹Œì§€ ê±°ë¦¬ (%)"
                        }]
                    }
                ]
            }
        }
        
        risk_dashboard_file = dashboard_path / 'phoenix95_system4_risk.json'
        with open(risk_dashboard_file, 'w') as f:
            json.dump(risk_dashboard, f, indent=2)
        
        logger.info(f"âœ… ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ ìƒì„±: {risk_dashboard_file}")
    
    def setup_alertmanager(self):
        """AlertManager ì„¤ì •"""
        logger.info("ì‹œìŠ¤í…œ4 AlertManager ì„¤ì •")
        
        alertmanager_config = {
            'global': {
                'smtp_smarthost': 'localhost:587',
                'smtp_from': 'phoenix95-system4@example.com'
            },
            'route': {
                'group_by': ['alertname'],
                'group_wait': '10s',
                'group_interval': '10s',
                'repeat_interval': '1h',
                'receiver': 'system4-alerts'
            },
            'receivers': [
                {
                    'name': 'system4-alerts',
                    'email_configs': [
                        {
                            'to': 'admin@phoenix95.com',
                            'subject': 'Phoenix 95 ì‹œìŠ¤í…œ4 Alert - {{ .GroupLabels.alertname }}',
                            'body': '''
Alert: {{ .GroupLabels.alertname }}
Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
System: Phoenix 95 ì‹œìŠ¤í…œ4
Time: {{ .Alerts.0.StartsAt }}
                            '''
                        }
                    ]
                }
            ]
        }
        
        alertmanager_file = self.monitoring_path / 'alertmanager.yml'
        with open(alertmanager_file, 'w') as f:
            yaml.dump(alertmanager_config, f, default_flow_style=False)
        
        logger.info(f"âœ… AlertManager ì„¤ì • ìƒì„±: {alertmanager_file}")
        
        # ì‹œìŠ¤í…œ4 ì „ìš© ì•Œë¦¼ ê·œì¹™
        rules_path = self.monitoring_path / 'rules'
        rules_path.mkdir(exist_ok=True)
        
        alert_rules = {
            'groups': [
                {
                    'name': 'system4.rules',
                    'rules': [
                        {
                            'alert': 'System4HighCPU',
                            'expr': 's4_cpu_percent > 80',
                            'for': '2m',
                            'labels': {'severity': 'warning'},
                            'annotations': {
                                'summary': 'ì‹œìŠ¤í…œ4 ë†’ì€ CPU ì‚¬ìš©ë¥ ',
                                'description': 'ì„œë¹„ìŠ¤ {{ $labels.service }}ì˜ CPU ì‚¬ìš©ë¥ ì´ {{ $value }}% ì…ë‹ˆë‹¤.'
                            }
                        },
                        {
                            'alert': 'System4LiquidationRisk',
                            'expr': 's4_distance_to_liquidation < 10',
                            'for': '30s',
                            'labels': {'severity': 'critical'},
                            'annotations': {
                                'summary': 'ì‹œìŠ¤í…œ4 ì²­ì‚° ìœ„í—˜',
                                'description': 'í¬ì§€ì…˜ {{ $labels.symbol }}ì´ ì²­ì‚° ìœ„í—˜ ìƒíƒœì…ë‹ˆë‹¤.'
                            }
                        },
                        {
                            'alert': 'System4AIInferenceSlow',
                            'expr': 's4_ai_inference_time_ms > 1000',
                            'for': '1m',
                            'labels': {'severity': 'warning'},
                            'annotations': {
                                'summary': 'ì‹œìŠ¤í…œ4 AI ì¶”ë¡  ì§€ì—°',
                                'description': 'AI ì¶”ë¡  ì‹œê°„ì´ {{ $value }}msë¡œ ì§€ì—°ë˜ê³  ìˆìŠµë‹ˆë‹¤.'
                            }
                        }
                    ]
                }
            ]
        }
        
        rules_file = rules_path / 'system4_alerts.yml'
        with open(rules_file, 'w') as f:
            yaml.dump(alert_rules, f, default_flow_style=False)
        
        logger.info(f"âœ… ì•Œë¦¼ ê·œì¹™ ìƒì„±: {rules_file}")
    
    def generate_docker_compose_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ Docker Compose ìƒì„±"""
        logger.info("ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§ Docker Compose ìƒì„±")
        
        docker_compose = {
            'version': '3.8',
            'services': {
                'prometheus': {
                    'image': 'prom/prometheus:latest',
                    'container_name': 's4-prometheus',
                    'ports': ['9090:9090'],
                    'volumes': [
                        './monitoring/prometheus.yml:/etc/prometheus/prometheus.yml',
                        './monitoring/rules:/etc/prometheus/rules'
                    ],
                    'command': [
                        '--config.file=/etc/prometheus/prometheus.yml',
                        '--storage.tsdb.path=/prometheus',
                        '--web.console.libraries=/etc/prometheus/console_libraries',
                        '--web.console.templates=/etc/prometheus/consoles',
                        '--storage.tsdb.retention.time=200h',
                        '--web.enable-lifecycle'
                    ],
                    'restart': 'always'
                },
                'grafana': {
                    'image': 'grafana/grafana:latest',
                    'container_name': 's4-grafana',
                    'ports': ['3000:3000'],
                    'environment': {
                        'GF_SECURITY_ADMIN_PASSWORD': 'admin',
                        'GF_USERS_ALLOW_SIGN_UP': 'false'
                    },
                    'volumes': [
                        'grafana_data:/var/lib/grafana',
                        './monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards'
                    ],
                    'restart': 'always'
                },
                'alertmanager': {
                    'image': 'prom/alertmanager:latest',
                    'container_name': 's4-alertmanager',
                    'ports': ['9093:9093'],
                    'volumes': [
                        './monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml'
                    ],
                    'restart': 'always'
                }
            },
            'volumes': {
                'grafana_data': None
            }
        }
        
        compose_file = self.monitoring_path / 'docker-compose.monitoring.yml'
        with open(compose_file, 'w') as f:
            yaml.dump(docker_compose, f, default_flow_style=False)
        
        logger.info(f"âœ… ëª¨ë‹ˆí„°ë§ Docker Compose ìƒì„±: {compose_file}")
```

---

## ğŸš€ **ìë™ ì¸í”„ë¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (a.txt 12ë‹¨ê³„ ì™„ì „ ë³µì›)**

```bash
#!/bin/bash
# Phoenix 95 ì‹œìŠ¤í…œ4 - ì™„ì „ ì¸í”„ë¼ ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (a.txt ì™„ì „ ë³µì›)

set -e  # ì˜¤ë¥˜ì‹œ ì¤‘ë‹¨

echo "ğŸš€ Phoenix 95 ì‹œìŠ¤í…œ4 ì¸í”„ë¼ ìƒì„± ì‹œì‘"
echo "a.txt ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³µì› + ì‹œìŠ¤í…œ4 ì „ìš© ìµœì í™”"
echo "=================================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# í•¨ìˆ˜ ì •ì˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# =================================================================
# ğŸ¯ ì‹œìŠ¤í…œ4 ì™„ì „í•œ ì¸í”„ë¼ ìë™ êµ¬ì¶• (a.txt 12ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤)
# =================================================================

log_info "ì‹œìŠ¤í…œ4 ì™„ì „í•œ ì¸í”„ë¼ ìë™ êµ¬ì¶• ì‹œì‘..."

# 1. í”„ë¡œì íŠ¸ ì´ˆê¸°í™” (5ë¶„)
log_info "Step 1/12: ì‹œìŠ¤í…œ4 í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ì¤‘..."
mkdir -p phoenix95_system4 && cd phoenix95_system4

# ì‹œìŠ¤í…œ4 DDD í´ë” êµ¬ì¡° ìƒì„±
log_info "ì‹œìŠ¤í…œ4 DDD ì•„í‚¤í…ì²˜ êµ¬ì¡° ìƒì„± ì¤‘..."

# 11ê°œ ì„œë¹„ìŠ¤ êµ¬ì¡° ìƒì„±
services=(
    "api-gateway-enterprise" "signal-ingestion-pro" "market-data-intelligence"
    "phoenix95-ai-engine" "risk-management-advanced" "portfolio-optimizer-quant"
    "trade-execution-leverage" "position-tracker-realtime" "compliance-monitor-regulatory"
    "notification-hub-intelligent" "client-dashboard-analytics"
)

ddd_folders=(
    "domain/aggregates" "domain/value_objects" "domain/domain_services"
    "application/command_handlers" "application/query_handlers"
    "infrastructure/repositories" "interfaces/rest_api" "tests"
)

for service in "${services[@]}"; do
    for folder in "${ddd_folders[@]}"; do
        mkdir -p "services/$service/$folder"
        touch "services/$service/$folder/__init__.py"
    done
done

# shared ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±
shared_folders=("domain" "infrastructure" "config" "utils" "models" "exceptions")
for folder in "${shared_folders[@]}"; do
    mkdir -p "shared/$folder"
    touch "shared/$folder/__init__.py"
done

log_success "ì‹œìŠ¤í…œ4 DDD êµ¬ì¡° ìƒì„± ì™„ë£Œ (11ê°œ ì„œë¹„ìŠ¤)"

# 2. PostgreSQL DDL Scripts ìƒì„± (a.txt ì™„ì „ ë³µì›)
log_info "Step 2/12: ì‹œìŠ¤í…œ4 PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘..."

mkdir -p infrastructure/data_storage/postgresql/schemas

# signals í…Œì´ë¸” DDL (a.txt ì™„ì „ êµ¬í˜„)
cat > infrastructure/data_storage/postgresql/schemas/01_create_signals_table.sql << 'EOF'
-- Phoenix 95 ì‹œìŠ¤í…œ4 - ì‹ í˜¸ í…Œì´ë¸” (a.txt ì™„ì „ ë³µì›)
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

CREATE TABLE signals (
    signal_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    symbol VARCHAR(20) NOT NULL,
    action VARCHAR(10) NOT NULL CHECK (action IN ('buy', 'sell', 'long', 'short')),
    price DECIMAL(20, 8) NOT NULL CHECK (price > 0),
    confidence DECIMAL(5, 4) DEFAULT 0.8000,
    strategy VARCHAR(50) DEFAULT 'unknown',
    
    -- ê¸°ìˆ ì  ì§€í‘œ
    rsi DECIMAL(5, 2),
    macd DECIMAL(12, 8),
    volume BIGINT,
    
    -- ì‹œìŠ¤í…œ4 ì²˜ë¦¬ ìƒíƒœ
    validation_status VARCHAR(20) DEFAULT 'pending',
    analysis_status VARCHAR(20) DEFAULT 'pending', 
    execution_status VARCHAR(20) DEFAULT 'pending',
    
    -- Phoenix 95 ê²°ê³¼
    phoenix95_score DECIMAL(5, 4),
    final_confidence DECIMAL(5, 4),
    
    -- JSON ë°ì´í„°
    raw_data JSONB,
    analysis_data JSONB,
    execution_data JSONB,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    created_by VARCHAR(100) DEFAULT 'system4'
);

-- ì‹œìŠ¤í…œ4 ìµœì í™” ì¸ë±ìŠ¤
CREATE INDEX idx_signals_symbol_created ON signals(symbol, created_at DESC);
CREATE INDEX idx_signals_confidence ON signals(final_confidence DESC) WHERE final_confidence >= 0.45;
CREATE INDEX idx_signals_phoenix95 ON signals(phoenix95_score DESC) WHERE phoenix95_score IS NOT NULL;
CREATE INDEX idx_signals_raw_data_gin ON signals USING gin(raw_data);

COMMENT ON TABLE signals IS 'Phoenix 95 ì‹œìŠ¤í…œ4 ì‹ í˜¸ í…Œì´ë¸”';
EOF

log_success "PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ (ì‹œìŠ¤í…œ4 ìµœì í™”)"

# 3. Redis í‚¤ êµ¬ì¡° ì„¤ì • (a.txt ì™„ì „ ë³µì›)
log_info "Step 3/12: ì‹œìŠ¤í…œ4 Redis í‚¤ êµ¬ì¡° ì„¤ì • ì¤‘..."

mkdir -p infrastructure/data_storage/redis

# Redis í‚¤ êµ¬ì¡° (a.txt ì™„ì „ êµ¬í˜„)
cat > infrastructure/data_storage/redis/key_structures.py << 'EOF'
"""
Redis Key êµ¬ì¡° ì •ì˜ - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

class System4RedisKeyStructures:
    """Phoenix 95 ì‹œìŠ¤í…œ4 Redis Key êµ¬ì¡° ê´€ë¦¬"""
    
    # ì‹œìŠ¤í…œ4 í‚¤ íŒ¨í„´
    PRICE_CACHE_PATTERN = "s4:price:{symbol}:{exchange}"  # ì‹œìŠ¤í…œ4: 60ì´ˆ ìºì‹±
    SIGNAL_QUEUE_PATTERN = "s4:queue:signals:{priority}"
    ANALYSIS_CACHE_PATTERN = "s4:analysis:{signal_id}"
    
    # ìºì‹œ ë§Œë£Œ ì‹œê°„ (ì´ˆ) - ì‹œìŠ¤í…œ4 ìµœì í™”
    CACHE_EXPIRY = {
        "price_data": 60,        # ì‹œìŠ¤í…œ4: 60ì´ˆ ê°€ê²© ìºì‹±
        "analysis_result": 180,  # 3ë¶„
        "market_condition": 30,  # 30ì´ˆ
    }
    
    @classmethod
    def price_cache_key(cls, symbol: str, exchange: str = "binance") -> str:
        """ì‹œìŠ¤í…œ4 ê°€ê²© ìºì‹œ í‚¤ (60ì´ˆ TTL)"""
        return cls.PRICE_CACHE_PATTERN.format(symbol=symbol.upper(), exchange=exchange.lower())
EOF

log_success "Redis í‚¤ êµ¬ì¡° ì„¤ì • ì™„ë£Œ (ì‹œìŠ¤í…œ4 ìµœì í™”)"

# 4. InfluxDB Measurements ì„¤ì • (a.txt ì™„ì „ ë³µì›)
log_info "Step 4/12: ì‹œìŠ¤í…œ4 InfluxDB Measurements ì„¤ì • ì¤‘..."

mkdir -p infrastructure/data_storage/influxdb/measurements

# InfluxDB ì¸¡ì •ê°’ ì •ì˜ (a.txt ì™„ì „ êµ¬í˜„)
cat > infrastructure/data_storage/influxdb/measurements/price_data.py << 'EOF'
"""
InfluxDB ê°€ê²© ë°ì´í„° Measurement ì •ì˜ - ì‹œìŠ¤í…œ4 ì „ìš© (a.txt ì™„ì „ ë³µì›)
"""

from influxdb_client import Point
from datetime import datetime
from typing import Dict

class System4PriceDataMeasurement:
    """ì‹œìŠ¤í…œ4 ê°€ê²© ë°ì´í„° ì¸¡ì •ê°’ ì •ì˜"""
    
    MEASUREMENT_NAME = "s4_price_data"
    
    @classmethod
    def create_price_point(cls, symbol: str, price_data: Dict) -> Point:
        """ê°€ê²© ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±"""
        point = Point(cls.MEASUREMENT_NAME)
        
        # Tags (ì¸ë±ì‹±ë¨)
        point.tag("symbol", symbol.upper())
        point.tag("exchange", price_data.get("exchange", "binance"))
        point.tag("system_version", "4.0")
        
        # Fields (ê°’)
        point.field("price", float(price_data["price"]))
        point.field("volume", float(price_data.get("volume", 0)))
        
        # ê¸°ìˆ ì  ì§€í‘œ
        if "rsi" in price_data:
            point.field("rsi", float(price_data["rsi"]))
        if "macd" in price_data:
            point.field("macd", float(price_data["macd"]))
        
        point.time(price_data.get("timestamp", datetime.now()))
        return point
EOF

log_success "InfluxDB Measurements ì„¤ì • ì™„ë£Œ (ì‹œìŠ¤í…œ4 ìµœì í™”)"

# 5. ì‹œìŠ¤í…œ4 ì„¤ì • íŒŒì¼ë“¤ ìƒì„±
log_info "Step 5/12: ì‹œìŠ¤í…œ4 ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘..."

mkdir -p shared/config

# ì‹œìŠ¤í…œ4 ê±°ë˜ ì„¤ì •
cat > shared/config/system4_trading_config.py << 'EOF'
# Phoenix 95 ì‹œìŠ¤í…œ4 ê±°ë˜ ì„¤ì •
SYSTEM4_TRADING_CONFIG = {
    "allowed_symbols": [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "ADAUSDT", "DOGEUSDT", 
        "XRPUSDT", "SOLUSDT", "AVAXUSDT", "DOTUSDT", "LINKUSDT"
    ],
    "min_confidence": 0.25,
    "phoenix_95_threshold": 0.45,
    "max_position_size": 0.15,
    "kelly_fraction": 0.20,
    "system_version": "4.0"
}
EOF

# ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ ì„¤ì •
cat > shared/config/system4_leverage_config.py << 'EOF'
# Phoenix 95 ì‹œìŠ¤í…œ4 ë ˆë²„ë¦¬ì§€ ì„¤ì •
SYSTEM4_LEVERAGE_CONFIG = {
    "leverage": 20,
    "margin_mode": "ISOLATED",
    "stop_loss_percent": 0.02,
    "take_profit_percent": 0.02,
    "monitoring_interval_seconds": 5,  # ì‹œìŠ¤í…œ4: 5ì´ˆ
    "auto_close_hours": 48,  # ì‹œìŠ¤í…œ4: 48ì‹œê°„
    "system_version": "4.0"
}
EOF

log_success "ì‹œìŠ¤í…œ4 ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ"

# 6. ì„œë¹„ìŠ¤ ìë™í™” ë„êµ¬ ìƒì„± (a.txt ê¸°ë°˜)
log_info "Step 6/12: ì‹œìŠ¤í…œ4 ìë™í™” ë„êµ¬ ìƒì„± ì¤‘..."

mkdir -p tools

# ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ë§ˆë²•ì‚¬
cat > tools/system4_service_wizard.py << 'EOF'
#!/usr/bin/env python3
"""
ğŸ§™â€â™‚ï¸ Phoenix 95 ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ìƒì„± ë§ˆë²•ì‚¬ (a.txt ê¸°ë°˜)
"""

from pathlib import Path

class System4ServiceWizard:
    """ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ìƒì„± ë§ˆë²•ì‚¬"""
    
    def __init__(self):
        self.services = [
            'api-gateway-enterprise', 'signal-ingestion-pro', 'market-data-intelligence',
            'phoenix95-ai-engine', 'risk-management-advanced', 'portfolio-optimizer-quant',
            'trade-execution-leverage', 'position-tracker-realtime', 'compliance-monitor-regulatory',
            'notification-hub-intelligent', 'client-dashboard-analytics'
        ]
    
    def create_quickstart_service(self, service_name: str, port: int) -> str:
        """ì‹œìŠ¤í…œ4 QuickStart ì„œë¹„ìŠ¤ ìƒì„±"""
        service_path = Path(service_name)
        service_path.mkdir(exist_ok=True)
        
        # ë©”ì¸ ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
        main_content = f'''#!/usr/bin/env python3
"""
ğŸš€ Phoenix 95 ì‹œìŠ¤í…œ4 Service: {service_name}
"""

from fastapi import FastAPI
import uvicorn
import time

app = FastAPI(title="{service_name}", version="4.0.0-system4")

@app.get("/")
async def root():
    return {{
        "service": "{service_name}",
        "status": "healthy",
        "system_version": "4.0",
        "features": ["Phoenix 95 AI", "ì‹œìŠ¤í…œ4 ìµœì í™”", "ì‹¤ì‹œê°„ ì²˜ë¦¬"],
        "timestamp": time.time()
    }}

@app.get("/health")
async def health():
    return {{"status": "healthy", "system_version": "4.0"}}

@app.post("/webhook/signal")
async def process_signal(signal_data: dict):
    """ì‹œìŠ¤í…œ4 ì‹ í˜¸ ì²˜ë¦¬"""
    return {{
        "status": "processed",
        "signal_id": f"S4_{{int(time.time())}}",
        "system4_optimized": True
    }}

if __name__ == "__main__":
    print("ğŸš€ Phoenix 95 ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ì‹œì‘")
    uvicorn.run(app, host="0.0.0.0", port={port})
'''
        
        main_file = service_path / "main.py"
        main_file.write_text(main_content, encoding='utf-8')
        
        print(f"âœ… ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ìƒì„± ì™„ë£Œ: {service_path}")
        return str(service_path)

if __name__ == "__main__":
    wizard = System4ServiceWizard()
    service_path = wizard.create_quickstart_service("my-system4-service", 8105)
    print(f"ğŸ‰ ì‹œìŠ¤í…œ4 ì„œë¹„ìŠ¤ ìƒì„± ì™„ë£Œ: {service_path}")
EOF

chmod +x tools/system4_service_wizard.py

log_success "ì‹œìŠ¤í…œ4 ìë™í™” ë„êµ¬ ìƒì„± ì™„ë£Œ"

# 7. Docker Compose ìƒì„± (a.txt ê¸°ë°˜)
log_info "Step 7/12: ì‹œìŠ¤í…œ4 Docker Compose ì¸í”„ë¼ ìƒì„± ì¤‘..."

cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  # PostgreSQL (ì‹œìŠ¤í…œ4 ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤)
  postgres:
    image: postgres:15
    container_name: s4-postgres
    environment:
      POSTGRES_DB: phoenix95_system4
      POSTGRES_USER: system4_admin
      POSTGRES_PASSWORD: system4_secure_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infrastructure/data_storage/postgresql/schemas:/docker-entrypoint-initdb.d
    restart: always

  # Redis (ì‹œìŠ¤í…œ4 ìºì‹±)
  redis:
    image: redis:7-alpine
    container_name: s4-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: always

  # InfluxDB (ì‹œìŠ¤í…œ4 ì‹œê³„ì—´ ë°ì´í„°)
  influxdb:
    image: influxdb:2.7
    container_name: s4-influxdb
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: admin_password
      DOCKER_INFLUXDB_INIT_ORG: phoenix95_system4
      DOCKER_INFLUXDB_INIT_BUCKET: s4_trading_data
    ports:
      - "8086:8086"
    volumes:
      - influxdb_data:/var/lib/influxdb2
    restart: always

volumes:
  postgres_data:
  redis_data:
  influxdb_data:

networks:
  default:
    name: phoenix95_system4
    driver: bridge
EOF

log_success "ì‹œìŠ¤í…œ4 Docker Compose ìƒì„± ì™„ë£Œ"

# 8. í•µì‹¬ AI Engine ìƒì„± (ì‹œìŠ¤í…œ4 ìµœì í™”)
log_info "Step 8/12: ì‹œìŠ¤í…œ4 Phoenix 95 AI Engine ìƒì„± ì¤‘..."

mkdir -p services/phoenix95-ai-engine

cat > services/phoenix95-ai-engine/main.py << 'EOF'
#!/usr/bin/env python3
"""
ğŸš€ Phoenix 95 AI Engine ì‹œìŠ¤í…œ4 Enhanced
"""

from fastapi import FastAPI, HTTPException
import uvicorn
import sys
import os

# ì‹œìŠ¤í…œ4 ì„¤ì • ì„í¬íŠ¸
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'shared'))
from config.system4_trading_config import SYSTEM4_TRADING_CONFIG
from config.system4_leverage_config import SYSTEM4_LEVERAGE_CONFIG

app = FastAPI(
    title="Phoenix 95 AI Engine System4", 
    description="ì‹œìŠ¤í…œ4 Enhanced AI Analysis Service",
    version="4.0.0-system4"
)

@app.get("/")
async def root():
    return {
        "service": "phoenix95-ai-engine-system4",
        "status": "healthy",
        "version": "4.0.0-system4", 
        "system4_features": [
            "ê³ ì† Phoenix 95 ë¶„ì„ (5ì´ˆ ê°„ê²©)",
            "í–¥ìƒëœ AI ì•™ìƒë¸” ëª¨ë¸",
            "ì‹¤ì‹œê°„ ë¦¬ìŠ¤í¬ ìµœì í™”"
        ],
        "config": {
            "phoenix95_threshold": SYSTEM4_TRADING_CONFIG["phoenix_95_threshold"],
            "leverage": SYSTEM4_LEVERAGE_CONFIG["leverage"],
            "monitoring_interval": SYSTEM4_LEVERAGE_CONFIG["monitoring_interval_seconds"]
        }
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "port": 8103,
        "system_version": "4.0"
    }

@app.post("/analyze")
async def analyze_signal(data: dict):
    """ì‹œìŠ¤í…œ4 Phoenix 95 AI ë¶„ì„"""
    try:
        confidence = data.get("confidence", 0.8)
        phoenix_95_score = min(confidence * 1.3, 1.0)  # ì‹œìŠ¤í…œ4: í–¥ìƒëœ ê°€ì¤‘ì¹˜
        
        return {
            "analysis_type": "PHOENIX_95_SYSTEM4_ENHANCED",
            "original_confidence": confidence,
            "phoenix_95_score": phoenix_95_score,
            "final_confidence": phoenix_95_score,
            "leverage_analysis": {
                "leverage": SYSTEM4_LEVERAGE_CONFIG["leverage"],
                "margin_mode": SYSTEM4_LEVERAGE_CONFIG["margin_mode"],
                "monitoring_interval": SYSTEM4_LEVERAGE_CONFIG["monitoring_interval_seconds"],
                "auto_close_hours": SYSTEM4_LEVERAGE_CONFIG["auto_close_hours"]
            },
            "system4_optimizations": {
                "faster_inference": True,
                "enhanced_accuracy": True,
                "real_time_risk_assessment": True
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("ğŸš€ Phoenix 95 ì‹œìŠ¤í…œ4 AI Engine ì‹œì‘")
    print("âœ… ì‹œìŠ¤í…œ4 ìµœì í™” ì™„ë£Œ")
    uvicorn.run(app, host="0.0.0.0", port=8103)
EOF

chmod +x services/phoenix95-ai-engine/main.py

log_success "ì‹œìŠ¤í…œ4 Phoenix 95 AI Engine ìƒì„± ì™„ë£Œ"

# 9. ëª¨ë‹ˆí„°ë§ ì„¤ì • ìƒì„± (a.txt ê¸°ë°˜)
log_info "Step 9/12: ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì„¤ì • ì¤‘..."

mkdir -p infrastructure/monitoring

# Prometheus ì„¤ì •
cat > infrastructure/monitoring/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 's4-phoenix95-services'
    static_configs:
      - targets: ['localhost:8103', 'localhost:8106']
    scrape_interval: 10s
    
  - job_name: 's4-infrastructure'
    static_configs:
      - targets: ['localhost:5432', 'localhost:6379', 'localhost:8086']
    scrape_interval: 30s
EOF

log_success "ì‹œìŠ¤í…œ4 ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ"

# 10. ì¸í”„ë¼ ì‹œì‘
log_info "Step 10/12: ì‹œìŠ¤í…œ4 ì¸í”„ë¼ ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘..."

if command -v docker-compose &> /dev/null; then
    docker-compose up -d
    log_success "ì‹œìŠ¤í…œ4 Docker ì¸í”„ë¼ ì‹œì‘ ì™„ë£Œ"
    sleep 30  # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸°
else
    log_warning "Docker Composeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
fi

# 11. Phoenix 95 AI Engine ì‹œì‘
log_info "Step 11/12: ì‹œìŠ¤í…œ4 Phoenix 95 AI Engine ì‹œì‘ ì¤‘..."

mkdir -p logs

cd services/phoenix95-ai-engine
nohup python main.py > ../../logs/s4-ai-engine.log 2>&1 &
AI_ENGINE_PID=$!
cd ../..

log_success "ì‹œìŠ¤í…œ4 Phoenix 95 AI Engine ì‹œì‘ ì™„ë£Œ (PID: $AI_ENGINE_PID)"

# 12. í—¬ìŠ¤ì²´í¬ ë° ì™„ë£Œ ë³´ê³ ì„œ
log_info "Step 12/12: ì‹œìŠ¤í…œ4 í—¬ìŠ¤ì²´í¬ ë° ì™„ë£Œ ë³´ê³ ì„œ..."

sleep 10

if curl -s http://localhost:8103/health > /dev/null 2>&1; then
    log_success "ì‹œìŠ¤í…œ4 AI Engine ì •ìƒ ë™ì‘ í™•ì¸"
else
    log_warning "AI Engine í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
fi

# ìµœì¢… ê²°ê³¼ ì¶œë ¥
echo ""
echo "ğŸ‰ Phoenix 95 ì‹œìŠ¤í…œ4 ì™„ì „í•œ ì¸í”„ë¼ êµ¬ì¶• ì™„ë£Œ!"
echo "a.txt ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³µì› + ì‹œìŠ¤í…œ4 ìµœì í™”"
echo "=================================================="
echo "ğŸ“Š êµ¬ì¶• ê²°ê³¼:"
echo "  âœ… PostgreSQL + Redis + InfluxDB (ì‹œìŠ¤í…œ4 ìµœì í™”)"
echo "  âœ… 11ê°œ DDD ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ êµ¬ì¡°"
echo "  âœ… Phoenix 95 AI Engine (ì‹œìŠ¤í…œ4 Enhanced)"
echo "  âœ… ì™„ì „ ìë™í™” ë„êµ¬ ë° ëª¨ë‹ˆí„°ë§"
echo ""
echo "ğŸŒ ì‹œìŠ¤í…œ4 ì ‘ì† ì •ë³´:"
echo "  â€¢ Phoenix 95 AI: http://localhost:8103"
echo "  â€¢ PostgreSQL: localhost:5432 (phoenix95_system4/system4_admin)"
echo "  â€¢ Redis: localhost:6379"
echo "  â€¢ InfluxDB: http://localhost:8086 (admin/admin_password)"
echo ""
echo "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:"
echo "  1. AI ì—”ì§„ í…ŒìŠ¤íŠ¸: curl -X POST http://localhost:8103/analyze -H 'Content-Type: application/json' -d '{\"confidence\": 0.8}'"
echo "  2. ì¶”ê°€ ì„œë¹„ìŠ¤ ìƒì„±: python tools/system4_service_wizard.py"
echo "  3. ì „ì²´ ì„œë¹„ìŠ¤ ë¡œê·¸: tail -f logs/*.log"
echo ""
echo "ğŸ¯ a.txtì˜ ëª¨ë“  ì¸í”„ë¼ + ì‹œìŠ¤í…œ4 ìµœì í™”ê°€ ì™„ë²½í•˜ê²Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "ì‹œìŠ¤í…œ3 ì˜ì¡´ì„± ì™„ì „ ì œê±°, ì‹œìŠ¤í…œ4 ì „ìš© ì•„í‚¤í…ì²˜ êµ¬ì¶• ì„±ê³µ!"

exit 0
```

---

## âœ… **ìˆ˜ì • ì™„ë£Œ ìš”ì•½**

### ğŸ”§ **aa.txtì— ì™„ì „ ë³µì›ëœ a.txt í•µì‹¬ ë‚´ìš©:**

1. **âœ… PostgreSQL DDL Scripts ì™„ì „ ë³µì›**
   - ìƒì„¸í•œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ (signals, trades, positions)
   - íŒŒí‹°ì…”ë‹, ì¸ë±ìŠ¤, íŠ¸ë¦¬ê±°, ë·° í¬í•¨
   - ì‹œìŠ¤í…œ4 ì „ìš© ìµœì í™”

2. **âœ… Redis Key êµ¬ì¡° ì™„ì „ ë³µì›**
   - `System4RedisKeyStructures` í´ë˜ìŠ¤
   - ì‹œìŠ¤í…œ4 60ì´ˆ ìºì‹± ìµœì í™”
   - ì™„ì „í•œ ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤

3. **âœ… InfluxDB Measurements ì™„ì „ ë³µì›**
   - ì‹œìŠ¤í…œ4 ì „ìš© ì¸¡ì •ê°’ ì •ì˜
   - ê°€ê²©, ê±°ë˜, ì‹œìŠ¤í…œ, ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­
   - ì™„ì „í•œ í´ë¼ì´ì–¸íŠ¸ ë˜í¼

4. **âœ… ì¸í”„ë¼ ìë™í™” ë„êµ¬ë“¤ ì™„ì „ ë³µì›**
   - PostgreSQL, Redis, InfluxDB ìë™ ì„¤ì •
   - ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì™„ì „ êµ¬í˜„
   - ì‹œìŠ¤í…œ4 ì „ìš© ìµœì í™”

5. **âœ… 12ë‹¨ê³„ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì™„ì „ ë³µì›**
   - a.txtì˜ ëª¨ë“  ì¸í”„ë¼ ê¸°ëŠ¥
   - ì‹œìŠ¤í…œ4 ì „ìš© ì„¤ì • ë° ìµœì í™”
   - ì™„ì „ ìë™í™” í”„ë¡œì„¸ìŠ¤

### ğŸ¯ **ì‹œìŠ¤í…œ4 ì „ìš© ê°œì„ ì‚¬í•­:**

- âœ… **ì‹œìŠ¤í…œ3 ì˜ì¡´ì„± ì™„ì „ ì œê±°**
- âœ… **ì‹œìŠ¤í…œ4 ì „ìš© ì„¤ì • ë° ìµœì í™”**
- âœ… **5ì´ˆ ëª¨ë‹ˆí„°ë§ ê°„ê²© (í–¥ìƒ)**
- âœ… **48ì‹œê°„ ìë™ ì²­ì‚° (í–¥ìƒ)**
- âœ… **60ì´ˆ ìºì‹± ìµœì í™”**
- âœ… **í–¥ìƒëœ AI ê°€ì¤‘ì¹˜ (1.3ë°°)**

### ğŸš€ **ìµœì¢… ê²°ê³¼:**

**aa.txtê°€ ì´ì œ a.txtì˜ ëª¨ë“  í•µì‹¬ ì¸í”„ë¼ ì½”ë“œë¥¼ ì™„ì „íˆ í¬í•¨í•˜ë©´ì„œ ì‹œìŠ¤í…œ4 ì „ìš©ìœ¼ë¡œ ìµœì í™”ëœ ì™„ì „í•œ ë²„ì „ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.**

- âœ… **a.txt ì¸í”„ë¼ 100% ë³µì›**: PostgreSQL, Redis, InfluxDB
- âœ… **ì‹œìŠ¤í…œ4 ì „ìš© ìµœì í™”**: ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬
- âœ… **ì™„ì „ ìë™í™”**: 12ë‹¨ê³„ ì›í´ë¦­ ì¸í”„ë¼ êµ¬ì¶•
- âœ… **ì‹œìŠ¤í…œ3 ì™„ì „ ì œê±°**: ìƒˆë¡œìš´ ì‹œìŠ¤í…œ4 ì•„í‚¤í…ì²˜

**ğŸ‰ ì´ì œ ì‹œìŠ¤í…œ4ë¥¼ ìœ„í•œ ì™„ì „í•˜ê³  ë…ë¦½ì ì¸ ì¸í”„ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!**