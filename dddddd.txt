#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Phoenix 95 Ultimate Trading System - ENHANCED VERSION
    V3 ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜ + V4 ê³ ê¸‰ ê¸°ëŠ¥ + í—¤ì§€í€ë“œê¸‰ ì•ˆì „ì„± + Lambda ì•„í‚¤í…ì²˜
    
    ğŸ¯ ì™„ì „ í†µí•© ìµœì¢… ë²„ì „ - ëª¨ë“  ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ íŒŒì¼ì— í†µí•©
    ğŸ’° Wall Streetê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ
    âš¡ Lambda ì•„í‚¤í…ì²˜, ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬, ML Pipeline ìë™í™” ì¶”ê°€
    
    ğŸ”§ ìƒˆë¡œìš´ ì¶”ê°€ ê¸°ëŠ¥ë“¤:
    - Lambda ë°ì´í„° ì•„í‚¤í…ì²˜ (ë°°ì¹˜/ìŠ¤í”¼ë“œ/ì„œë¹™ ë ˆì´ì–´)
    - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - ML Pipeline ìë™í™” (MLflow í†µí•©)
    - í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ëª¨ë‹ˆí„°ë§
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import asyncio
import aiohttp
import aioredis
import asyncpg
import json
import time
import logging
import os
import sys
import uuid
import hmac
import hashlib
import secrets
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from typing import Dict, List, Optional, Tuple, Any, Union
from decimal import Decimal
from collections import deque
import numpy as np
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Security, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, validator
import uvicorn
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ”§ í™˜ê²½ë³€ìˆ˜ ê²€ì¦ (ìµœìš°ì„  ì‹¤í–‰)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_environment() -> bool:
    """í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ê²€ì¦ - ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì „ ìµœìš°ì„  ì‹¤í–‰"""
    # .env íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë“œ
    load_dotenv()
    
    required_vars = {
        "TELEGRAM_BOT_TOKEN": "í…”ë ˆê·¸ë¨ ë´‡ í† í°",
        "TELEGRAM_CHAT_ID": "í…”ë ˆê·¸ë¨ ì±„íŒ… ID", 
        "WEBHOOK_SECRET": "ì›¹í›… ì‹œí¬ë¦¿",
        "API_KEY": "API í‚¤",
        "JWT_SECRET": "JWT ì‹œí¬ë¦¿"
    }
    
    missing = []
    invalid = []
    
    for var, desc in required_vars.items():
        value = os.getenv(var)
        if not value or value.strip() == "":
            missing.append(f"  - {var}: {desc}")
        else:
            # ê°’ í˜•ì‹ ê²€ì¦
            if var == "TELEGRAM_BOT_TOKEN" and not value.split(':')[0].isdigit():
                invalid.append(f"  - {var}: ì˜¬ë°”ë¥¸ í…”ë ˆê·¸ë¨ ë´‡ í† í° í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤")
            elif var == "TELEGRAM_CHAT_ID" and not value.lstrip('-').isdigit():
                invalid.append(f"  - {var}: ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    if missing or invalid:
        error_parts = []
        if missing:
            error_parts.append("í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:\n" + "\n".join(missing))
        if invalid:
            error_parts.append("ì˜ëª»ëœ í™˜ê²½ë³€ìˆ˜ í˜•ì‹:\n" + "\n".join(invalid))
        
        error_msg = "\n\n".join(error_parts)
        error_msg += "\n\n.env íŒŒì¼ì„ ìƒì„±í•˜ê³  ì˜¬ë°”ë¥¸ ê°’ì„ ì„¤ì •í•˜ì„¸ìš”."
        raise EnvironmentError(error_msg)
    
    logging.info("í™˜ê²½ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
    return True

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì „ í™˜ê²½ë³€ìˆ˜ ê²€ì¦ ì‹¤í–‰
validate_environment()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SystemConfig:
    """Phoenix 95 Ultimate í†µí•© ì„¤ì • - ë³´ì•ˆ ê°•í™”"""
    
    # ğŸ¯ í•µì‹¬ í…”ë ˆê·¸ë¨ ì„¤ì • (ë³´ì•ˆ ê°•í™”: ê¸°ë³¸ê°’ None ì²˜ë¦¬)
    TELEGRAM = {
        "bot_token": os.getenv("TELEGRAM_BOT_TOKEN"),
        "chat_id": os.getenv("TELEGRAM_CHAT_ID"),
        "alerts": {
            "trade_execution": True,
            "position_updates": True, 
            "system_errors": True,
            "performance_reports": True,
            "liquidation_warnings": True,
            "daily_summary": True
        }
    }
    
    # ğŸ’° í—¤ì§€í€ë“œê¸‰ íŠ¸ë ˆì´ë”© ì„¤ì • (ë³´ìˆ˜ì  ì ‘ê·¼)
    TRADING = {
        "max_leverage": 10,
        "margin_mode": "ISOLATED",
        "position_size_pct": 0.02,
        "stop_loss_pct": 0.015,
        "take_profit_pct": 0.03,
        "max_daily_loss": 1000,
        "max_positions": 3,
        "confidence_threshold": 0.85,
        "kelly_max": 0.25,
        "max_retries": 3,
        "retry_delay": 5,
        "allowed_symbols": [
            "BTCUSDT", "ETHUSDT", "ADAUSDT", "SOLUSDT", "AVAXUSDT",
            "DOTUSDT", "LINKUSDT", "MATICUSDT", "ATOMUSDT", "NEARUSDT"
        ]
    }
    
    # ğŸ§  Phoenix 95 AI ì—”ì§„ ì„¤ì • (V3 ê²€ì¦ëœ íŒŒë¼ë¯¸í„°)
    PHOENIX95 = {
        "confidence_multiplier": 1.15,
        "market_condition_weight": 0.3,
        "ensemble_weights": {
            "phoenix95": 0.6,
            "lstm": 0.25, 
            "transformer": 0.15
        },
        "analysis_timeout": 2.0,
        "min_confidence": 0.7,
        "max_confidence": 0.99,
        "cache_ttl": 300,
        "max_cache_size": 1000
    }
    
    # ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    DATABASE = {
        "redis_url": os.getenv("REDIS_URL", "redis://localhost:6379"),
        "postgres_url": os.getenv("POSTGRES_URL", "postgresql://postgres:password@localhost:5432/phoenix95"),
        "connection_pool_size": 20,
        "max_connections": 100,
        "connection_timeout": 30,
        "query_timeout": 10
    }
    
    # ğŸ”’ ë³´ì•ˆ ì„¤ì •
    SECURITY = {
        "webhook_secret": os.getenv("WEBHOOK_SECRET"),
        "api_key": os.getenv("API_KEY"),
        "jwt_secret": os.getenv("JWT_SECRET"),
        "allowed_ips": ["127.0.0.1", "localhost"],
        "rate_limit": 60
    }
    
    # ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •
    MONITORING = {
        "metrics_interval": 30,
        "health_check_interval": 10,
        "alert_cooldown": 300,
        "performance_threshold": {
            "response_time_ms": 2000,
            "error_rate_pct": 5,
            "memory_usage_pct": 85,
            "cpu_usage_pct": 80
        },
        "max_metrics_history": 1440
    }
    
    # ğŸŒŠ Lambda ì•„í‚¤í…ì²˜ ì„¤ì • (NEW)
    LAMBDA_ARCHITECTURE = {
        "batch_layer": {
            "processing_interval": 3600,  # 1ì‹œê°„ë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬
            "batch_size": 1000,
            "retention_days": 90
        },
        "speed_layer": {
            "window_size": 300,  # 5ë¶„ ìœˆë„ìš°
            "buffer_size": 10000,
            "flush_interval": 60
        },
        "serving_layer": {
            "cache_size": 50000,
            "ttl": 300,
            "materialization_interval": 600
        }
    }
    
    # ğŸš€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì„¤ì • (NEW)
    STREAM_PROCESSING = {
        "kafka": {
            "bootstrap_servers": os.getenv("KAFKA_SERVERS", "localhost:9092"),
            "topics": {
                "market_data": "phoenix95-market-data",
                "trading_signals": "phoenix95-signals",
                "positions": "phoenix95-positions"
            },
            "consumer_group": "phoenix95-group",
            "batch_size": 100
        },
        "redis_streams": {
            "streams": {
                "market": "phoenix95:stream:market",
                "signals": "phoenix95:stream:signals",
                "positions": "phoenix95:stream:positions"
            },
            "consumer_group": "phoenix95-consumers",
            "max_len": 10000
        }
    }
    
    # ğŸ¤– ML Pipeline ì„¤ì • (NEW)
    ML_PIPELINE = {
        "mlflow": {
            "tracking_uri": os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000"),
            "experiment_name": "phoenix95-trading",
            "model_registry": "phoenix95-models"
        },
        "training": {
            "retrain_interval": 86400,  # 24ì‹œê°„ë§ˆë‹¤ ì¬í›ˆë ¨
            "validation_split": 0.2,
            "early_stopping_patience": 10,
            "max_epochs": 100
        },
        "inference": {
            "model_cache_size": 5,
            "prediction_batch_size": 32,
            "fallback_model": "phoenix95_v3"
        }
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ“Š ë°ì´í„° ëª¨ë¸ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class TradingSignal:
    """íŠ¸ë ˆì´ë”© ì‹ í˜¸ - V3/V4 í†µí•© ëª¨ë¸ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)"""
    signal_id: str
    symbol: str
    action: str  # buy/sell
    price: float
    confidence: float
    timestamp: datetime
    
    # Phoenix 95 ë¶„ì„ ê²°ê³¼ (Optional í•„ë“œ ëª…ì‹œì  íƒ€ì… ì²´í¬)
    phoenix95_score: Optional[float] = None
    kelly_ratio: Optional[float] = None
    recommendation: Optional[str] = None
    
    # ì‹œì¥ ë°ì´í„°
    market_conditions: Optional[Dict[str, Any]] = None
    technical_indicators: Optional[Dict[str, Any]] = None
    
    # ì²˜ë¦¬ ìƒíƒœ
    processed: bool = False
    error_message: Optional[str] = None
    
    def is_valid(self) -> bool:
        """ì‹ í˜¸ ìœ íš¨ì„± ê²€ì¦ - None ì²´í¬ ê°•í™”"""
        if not self.symbol or not self.action:
            return False
        
        return (
            self.symbol in SystemConfig.TRADING["allowed_symbols"] and
            self.action.lower() in ["buy", "sell"] and
            self.price > 0 and
            0.0 <= self.confidence <= 1.0 and
            self.confidence >= SystemConfig.TRADING["confidence_threshold"]
        )
    
    def has_phoenix95_analysis(self) -> bool:
        """Phoenix95 ë¶„ì„ ê²°ê³¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return (
            self.phoenix95_score is not None and
            self.kelly_ratio is not None and
            self.recommendation is not None
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜ - íƒ€ì… ì•ˆì •ì„± ë³´ì¥"""
        return {
            "signal_id": self.signal_id,
            "symbol": self.symbol,
            "action": self.action,
            "price": self.price,
            "confidence": self.confidence,
            "phoenix95_score": self.phoenix95_score,
            "kelly_ratio": self.kelly_ratio,
            "recommendation": self.recommendation,
            "timestamp": self.timestamp.isoformat(),
            "processed": self.processed
        }

@dataclass  
class Position:
    """í¬ì§€ì…˜ - ì™„ì „ í†µí•© ëª¨ë¸ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)"""
    position_id: str
    signal_id: str
    symbol: str
    side: str  # BUY/SELL
    
    # í¬ì§€ì…˜ ì •ë³´
    entry_price: float
    quantity: float
    leverage: int
    margin_mode: str
    margin_required: float
    
    # ë¦¬ìŠ¤í¬ ê´€ë¦¬
    liquidation_price: float
    stop_loss_price: float
    take_profit_price: float
    
    # ì‹¤ì‹œê°„ ë°ì´í„°
    current_price: float = 0.0
    unrealized_pnl: float = 0.0
    pnl_percentage: float = 0.0
    liquidation_risk: float = 0.0
    
    # ìƒíƒœ
    status: str = "OPEN"  # OPEN/CLOSED/LIQUIDATED
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    
    # ì²­ì‚° ì •ë³´
    exit_price: Optional[float] = None
    exit_time: Optional[datetime] = None
    exit_reason: Optional[str] = None
    realized_pnl: Optional[float] = None
    
    def calculate_pnl(self, current_price: float) -> Tuple[float, float]:
        """P&L ê³„ì‚° - ì•ˆì „í•œ ê³„ì‚°"""
        try:
            if self.side == "BUY":
                pnl = (current_price - self.entry_price) * self.quantity
            else:
                pnl = (self.entry_price - current_price) * self.quantity
            
            pnl_pct = (pnl / self.margin_required) * 100 if self.margin_required > 0 else 0.0
            return pnl, pnl_pct
        except (ZeroDivisionError, TypeError) as e:
            logging.error(f"P&L ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0, 0.0
    
    def calculate_liquidation_risk(self, current_price: float) -> float:
        """ì²­ì‚° ìœ„í—˜ë„ ê³„ì‚° (0-1) - ì•ˆì „í•œ ê³„ì‚°"""
        try:
            if self.side == "BUY":
                distance = current_price - self.liquidation_price
                max_distance = self.entry_price - self.liquidation_price
            else:
                distance = self.liquidation_price - current_price  
                max_distance = self.liquidation_price - self.entry_price
            
            if max_distance <= 0:
                return 1.0
            
            risk = 1 - (distance / max_distance)
            return max(0.0, min(1.0, risk))
        except (ZeroDivisionError, TypeError) as e:
            logging.error(f"ì²­ì‚° ìœ„í—˜ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ - íƒ€ì… ì•ˆì •ì„± ê°•í™”"""
    timestamp: datetime
    
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
    cpu_usage: float
    memory_usage: float  
    response_time_ms: float
    active_connections: int
    
    # íŠ¸ë ˆì´ë”© ë©”íŠ¸ë¦­
    active_positions: int
    total_pnl: float
    win_rate: float
    avg_trade_duration: float
    phoenix95_avg_score: float
    
    # ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­
    max_drawdown: float
    var_95: float  # Value at Risk
    sharpe_ratio: float

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸŒŠ Lambda ë°ì´í„° ì•„í‚¤í…ì²˜ (NEW)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BatchDataProcessor:
    """ë°°ì¹˜ ë ˆì´ì–´ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
    
    def __init__(self, postgres_pool, redis_client):
        self.postgres = postgres_pool
        self.redis = redis_client
        self.config = SystemConfig.LAMBDA_ARCHITECTURE["batch_layer"]
        
    async def process_historical_data(self):
        """ê³¼ê±° ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            logging.info("ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
            
            # ê³¼ê±° ê±°ë˜ ë°ì´í„° ë¶„ì„
            batch_size = self.config["batch_size"]
            
            async with self.postgres.acquire() as conn:
                # ì§€ë‚œ 24ì‹œê°„ ê±°ë˜ ë°ì´í„° ì¡°íšŒ
                trades = await conn.fetch("""
                    SELECT * FROM trade_history 
                    WHERE entry_time >= NOW() - INTERVAL '24 hours'
                    ORDER BY entry_time DESC
                    LIMIT $1
                """, batch_size)
                
                # ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰
                analytics = await self._analyze_batch_data(trades)
                
                # ê²°ê³¼ ì €ì¥
                await self._store_batch_results(analytics)
                
            logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(trades)}ê°œ ê±°ë˜ ë¶„ì„")
            
        except Exception as e:
            logging.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _analyze_batch_data(self, trades) -> Dict[str, Any]:
        """ë°°ì¹˜ ë°ì´í„° ë¶„ì„"""
        if not trades:
            return {}
        
        # í†µê³„ ê³„ì‚°
        total_pnl = sum(float(trade['pnl'] or 0) for trade in trades)
        win_trades = [t for t in trades if (t['pnl'] or 0) > 0]
        win_rate = len(win_trades) / len(trades) if trades else 0
        
        # ì‹¬ë³¼ë³„ ì„±ê³¼
        symbol_performance = {}
        for trade in trades:
            symbol = trade['symbol']
            if symbol not in symbol_performance:
                symbol_performance[symbol] = {'trades': 0, 'pnl': 0}
            symbol_performance[symbol]['trades'] += 1
            symbol_performance[symbol]['pnl'] += float(trade['pnl'] or 0)
        
        return {
            'timestamp': datetime.utcnow(),
            'total_trades': len(trades),
            'total_pnl': total_pnl,
            'win_rate': win_rate,
            'symbol_performance': symbol_performance,
            'avg_duration': np.mean([t['duration_minutes'] or 0 for t in trades]),
            'max_drawdown': min(float(t['pnl'] or 0) for t in trades) if trades else 0
        }
    
    async def _store_batch_results(self, analytics: Dict[str, Any]):
        """ë°°ì¹˜ ê²°ê³¼ ì €ì¥"""
        try:
            # Redisì— ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥
            await self.redis.hset("batch:analytics", mapping={
                'timestamp': analytics['timestamp'].isoformat(),
                'total_trades': str(analytics['total_trades']),
                'total_pnl': str(analytics['total_pnl']),
                'win_rate': str(analytics['win_rate']),
                'avg_duration': str(analytics['avg_duration']),
                'max_drawdown': str(analytics['max_drawdown'])
            })
            
            # PostgreSQLì— ë°°ì¹˜ ê²°ê³¼ ì €ì¥
            async with self.postgres.acquire() as conn:
                await conn.execute("""
                    INSERT INTO batch_analytics (
                        timestamp, total_trades, total_pnl, win_rate, 
                        avg_duration, max_drawdown, symbol_performance
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                """, analytics['timestamp'], analytics['total_trades'],
                analytics['total_pnl'], analytics['win_rate'],
                analytics['avg_duration'], analytics['max_drawdown'],
                json.dumps(analytics['symbol_performance']))
                
        except Exception as e:
            logging.error(f"ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


class StreamDataProcessor:
    """ìŠ¤í”¼ë“œ ë ˆì´ì–´ - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.config = SystemConfig.LAMBDA_ARCHITECTURE["speed_layer"]
        self.window_buffer = deque(maxlen=self.config["buffer_size"])
        
    async def process_real_time_signal(self, signal: TradingSignal):
        """ì‹¤ì‹œê°„ ì‹ í˜¸ ì²˜ë¦¬"""
        try:
            # ìœˆë„ìš° ë²„í¼ì— ì¶”ê°€
            signal_data = {
                'timestamp': signal.timestamp,
                'symbol': signal.symbol,
                'price': signal.price,
                'confidence': signal.confidence,
                'phoenix95_score': signal.phoenix95_score
            }
            
            self.window_buffer.append(signal_data)
            
            # ì‹¤ì‹œê°„ ë¶„ì„
            real_time_metrics = await self._calculate_real_time_metrics()
            
            # Redis Streamì— ë°œí–‰
            await self._publish_to_stream(signal_data, real_time_metrics)
            
        except Exception as e:
            logging.error(f"ì‹¤ì‹œê°„ ì‹ í˜¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _calculate_real_time_metrics(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self.window_buffer:
            return {}
        
        recent_data = list(self.window_buffer)[-100:]  # ìµœê·¼ 100ê°œ
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ 5ë¶„ ìœˆë„ìš°
        window_start = datetime.utcnow() - timedelta(seconds=self.config["window_size"])
        window_data = [d for d in recent_data if d['timestamp'] >= window_start]
        
        if not window_data:
            return {}
        
        return {
            'window_size': len(window_data),
            'avg_confidence': np.mean([d['confidence'] for d in window_data]),
            'avg_phoenix95_score': np.mean([d['phoenix95_score'] for d in window_data if d['phoenix95_score']]),
            'symbols_count': len(set(d['symbol'] for d in window_data)),
            'price_volatility': np.std([d['price'] for d in window_data]),
            'timestamp': datetime.utcnow()
        }
    
    async def _publish_to_stream(self, signal_data: Dict, metrics: Dict):
        """Redis Streamì— ë°ì´í„° ë°œí–‰"""
        try:
            stream_config = SystemConfig.STREAM_PROCESSING["redis_streams"]
            
            # ì‹ í˜¸ ìŠ¤íŠ¸ë¦¼ì— ë°œí–‰
            await self.redis.xadd(
                stream_config["streams"]["signals"],
                signal_data,
                maxlen=stream_config["max_len"]
            )
            
            # ë©”íŠ¸ë¦­ ìŠ¤íŠ¸ë¦¼ì— ë°œí–‰
            if metrics:
                await self.redis.xadd(
                    stream_config["streams"]["market"],
                    metrics,
                    maxlen=stream_config["max_len"]
                )
                
        except Exception as e:
            logging.error(f"ìŠ¤íŠ¸ë¦¼ ë°œí–‰ ì‹¤íŒ¨: {e}")


class ViewMaterializer:
    """ì„œë¹™ ë ˆì´ì–´ - ë·° êµ¬ì²´í™” ë° ìºì‹±"""
    
    def __init__(self, redis_client, postgres_pool):
        self.redis = redis_client
        self.postgres = postgres_pool
        self.config = SystemConfig.LAMBDA_ARCHITECTURE["serving_layer"]
        
    async def materialize_views(self):
        """ë·° êµ¬ì²´í™”"""
        try:
            # ë°°ì¹˜ + ì‹¤ì‹œê°„ ë°ì´í„° í†µí•©
            batch_data = await self._get_batch_data()
            stream_data = await self._get_stream_data()
            
            # í†µí•© ë·° ìƒì„±
            materialized_view = await self._merge_batch_and_stream(batch_data, stream_data)
            
            # ìºì‹œì— ì €ì¥
            await self._cache_materialized_view(materialized_view)
            
            logging.info("ë·° êµ¬ì²´í™” ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"ë·° êµ¬ì²´í™” ì‹¤íŒ¨: {e}")
    
    async def _get_batch_data(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ"""
        try:
            batch_data = await self.redis.hgetall("batch:analytics")
            return {k: v for k, v in batch_data.items()} if batch_data else {}
        except Exception as e:
            logging.error(f"ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _get_stream_data(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì¡°íšŒ"""
        try:
            stream_config = SystemConfig.STREAM_PROCESSING["redis_streams"]
            
            # ìµœê·¼ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì¡°íšŒ
            stream_data = await self.redis.xrevrange(
                stream_config["streams"]["market"],
                count=100
            )
            
            if not stream_data:
                return {}
            
            # ìµœì‹  ë©”íŠ¸ë¦­ ì¶”ì¶œ
            latest_metrics = stream_data[0][1] if stream_data else {}
            return latest_metrics
            
        except Exception as e:
            logging.error(f"ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _merge_batch_and_stream(self, batch_data: Dict, stream_data: Dict) -> Dict[str, Any]:
        """ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° í†µí•©"""
        merged_view = {
            'timestamp': datetime.utcnow().isoformat(),
            'batch_metrics': batch_data,
            'real_time_metrics': stream_data,
            'combined_stats': {}
        }
        
        # í†µí•© í†µê³„ ê³„ì‚°
        try:
            if batch_data and stream_data:
                merged_view['combined_stats'] = {
                    'total_confidence': float(stream_data.get('avg_confidence', 0)),
                    'historical_performance': float(batch_data.get('win_rate', 0)),
                    'current_volatility': float(stream_data.get('price_volatility', 0)),
                    'active_symbols': int(stream_data.get('symbols_count', 0))
                }
        except (ValueError, TypeError) as e:
            logging.warning(f"í†µí•© í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return merged_view
    
    async def _cache_materialized_view(self, view: Dict[str, Any]):
        """êµ¬ì²´í™”ëœ ë·° ìºì‹±"""
        try:
            cache_key = "serving:materialized_view"
            await self.redis.set(
                cache_key,
                json.dumps(view, default=str),
                ex=self.config["ttl"]
            )
        except Exception as e:
            logging.error(f"ë·° ìºì‹± ì‹¤íŒ¨: {e}")


class LambdaDataArchitecture:
    """Lambda ë°ì´í„° ì•„í‚¤í…ì²˜ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, redis_client, postgres_pool):
        self.batch_layer = BatchDataProcessor(postgres_pool, redis_client)
        self.speed_layer = StreamDataProcessor(redis_client)
        self.serving_layer = ViewMaterializer(redis_client, postgres_pool)
        
    async def start_lambda_architecture(self):
        """Lambda ì•„í‚¤í…ì²˜ ì‹œì‘"""
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
            asyncio.create_task(self._batch_scheduler())
            
            # ë·° êµ¬ì²´í™” ìŠ¤ì¼€ì¤„ëŸ¬
            asyncio.create_task(self._serving_scheduler())
            
            logging.info("Lambda ì•„í‚¤í…ì²˜ ì‹œì‘ë¨")
            
        except Exception as e:
            logging.error(f"Lambda ì•„í‚¤í…ì²˜ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def _batch_scheduler(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬"""
        while True:
            try:
                await self.batch_layer.process_historical_data()
                interval = SystemConfig.LAMBDA_ARCHITECTURE["batch_layer"]["processing_interval"]
                await asyncio.sleep(interval)
            except Exception as e:
                logging.error(f"ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    async def _serving_scheduler(self):
        """ì„œë¹™ ë ˆì´ì–´ ìŠ¤ì¼€ì¤„ëŸ¬"""
        while True:
            try:
                await self.serving_layer.materialize_views()
                interval = SystemConfig.LAMBDA_ARCHITECTURE["serving_layer"]["materialization_interval"]
                await asyncio.sleep(interval)
            except Exception as e:
                logging.error(f"ì„œë¹™ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸš€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (NEW)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StreamProcessor:
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.config = SystemConfig.STREAM_PROCESSING
        self.consumers = {}
        
    async def start_stream_processing(self):
        """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘"""
        try:
            # Redis Streams ì»¨ìŠˆë¨¸ ì‹œì‘
            await self._start_redis_consumers()
            
            logging.info("ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ë¨")
            
        except Exception as e:
            logging.error(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def _start_redis_consumers(self):
        """Redis Streams ì»¨ìŠˆë¨¸ ì‹œì‘"""
        redis_config = self.config["redis_streams"]
        
        for stream_name, stream_key in redis_config["streams"].items():
            consumer_task = asyncio.create_task(
                self._consume_redis_stream(stream_name, stream_key)
            )
            self.consumers[stream_name] = consumer_task
    
    async def _consume_redis_stream(self, stream_name: str, stream_key: str):
        """Redis Stream ì†Œë¹„"""
        consumer_group = self.config["redis_streams"]["consumer_group"]
        consumer_name = f"{consumer_group}-{stream_name}-{uuid.uuid4().hex[:8]}"
        
        try:
            # ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
            try:
                await self.redis.xgroup_create(
                    stream_key, consumer_group, id='0', mkstream=True
                )
            except Exception:
                pass  # ê·¸ë£¹ì´ ì´ë¯¸ ì¡´ì¬
            
            logging.info(f"{stream_name} ìŠ¤íŠ¸ë¦¼ ì»¨ìŠˆë¨¸ ì‹œì‘: {consumer_name}")
            
            while True:
                try:
                    # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë©”ì‹œì§€ ì½ê¸°
                    messages = await self.redis.xreadgroup(
                        consumer_group,
                        consumer_name,
                        {stream_key: '>'},
                        count=10,
                        block=1000
                    )
                    
                    for stream, msgs in messages:
                        for msg_id, fields in msgs:
                            await self._process_stream_message(
                                stream_name, msg_id, fields
                            )
                            
                            # ë©”ì‹œì§€ ACK
                            await self.redis.xack(stream_key, consumer_group, msg_id)
                            
                except Exception as e:
                    logging.error(f"{stream_name} ìŠ¤íŠ¸ë¦¼ ì†Œë¹„ ì˜¤ë¥˜: {e}")
                    await asyncio.sleep(5)
                    
        except Exception as e:
            logging.error(f"{stream_name} ì»¨ìŠˆë¨¸ ì‹¤íŒ¨: {e}")
    
    async def _process_stream_message(self, stream_name: str, msg_id: str, fields: Dict):
        """ìŠ¤íŠ¸ë¦¼ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            if stream_name == "market":
                await self._process_market_message(fields)
            elif stream_name == "signals":
                await self._process_signal_message(fields)
            elif stream_name == "positions":
                await self._process_position_message(fields)
                
        except Exception as e:
            logging.error(f"ìŠ¤íŠ¸ë¦¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({stream_name}): {e}")
    
    async def _process_market_message(self, fields: Dict):
        """ì‹œì¥ ë°ì´í„° ë©”ì‹œì§€ ì²˜ë¦¬"""
        # ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ë¶„ì„ ë° ì•Œë¦¼
        try:
            volatility = float(fields.get('price_volatility', 0))
            if volatility > 0.05:  # 5% ì´ìƒ ë³€ë™ì„±
                logging.warning(f"ë†’ì€ ë³€ë™ì„± ê°ì§€: {volatility:.1%}")
        except (ValueError, TypeError):
            pass
    
    async def _process_signal_message(self, fields: Dict):
        """ì‹ í˜¸ ë©”ì‹œì§€ ì²˜ë¦¬"""
        # ì‹ í˜¸ ì§‘ê³„ ë° íŒ¨í„´ ë¶„ì„
        try:
            confidence = float(fields.get('confidence', 0))
            if confidence > 0.95:  # ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„
                logging.info(f"ê³ ì‹ ë¢°ë„ ì‹ í˜¸ ê°ì§€: {confidence:.1%}")
        except (ValueError, TypeError):
            pass
    
    async def _process_position_message(self, fields: Dict):
        """í¬ì§€ì…˜ ë©”ì‹œì§€ ì²˜ë¦¬"""
        # í¬ì§€ì…˜ ì§‘ê³„ ë° ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§
        pass
    
    async def process_market_stream(self, market_data: Dict):
        """ì‹œì¥ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
        try:
            stream_key = self.config["redis_streams"]["streams"]["market"]
            await self.redis.xadd(stream_key, market_data, maxlen=10000)
        except Exception as e:
            logging.error(f"ì‹œì¥ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ¤– ML Pipeline ìë™í™” (NEW)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MLPipelineManager:
    """ML ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    
    def __init__(self, postgres_pool):
        self.postgres = postgres_pool
        self.config = SystemConfig.ML_PIPELINE
        self.model_cache = {}
        
        # MLflow í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì‹¤ì œë¡œëŠ” MLflow ì„¤ì¹˜ í•„ìš”)
        self.mlflow_client = None
        self.model_registry = None
        
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬
        self.current_model_version = "v3.0"
        self.model_performance_history = deque(maxlen=1000)
        
    async def start_ml_pipeline(self):
        """ML íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        try:
            # ëª¨ë¸ ë¡œë”©
            await self._load_models()
            
            # ì£¼ê¸°ì  ì¬í›ˆë ¨ ìŠ¤ì¼€ì¤„ëŸ¬
            asyncio.create_task(self._training_scheduler())
            
            # ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            asyncio.create_task(self._performance_monitor())
            
            logging.info("ML íŒŒì´í”„ë¼ì¸ ì‹œì‘ë¨")
            
        except Exception as e:
            logging.error(f"ML íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def _load_models(self):
        """ëª¨ë¸ ë¡œë”©"""
        try:
            # ì‹¤ì œë¡œëŠ” MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            self.model_cache["phoenix95_current"] = {
                "version": self.current_model_version,
                "accuracy": 0.85,
                "loaded_at": datetime.utcnow()
            }
            
            self.model_cache["phoenix95_fallback"] = {
                "version": "v2.5",
                "accuracy": 0.82,
                "loaded_at": datetime.utcnow()
            }
            
            logging.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {len(self.model_cache)}ê°œ ëª¨ë¸")
            
        except Exception as e:
            logging.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    async def predict_with_ensemble(self, signal: TradingSignal) -> Tuple[float, Dict[str, Any]]:
        """ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡"""
        try:
            predictions = {}
            
            # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
            for model_name, model_info in self.model_cache.items():
                prediction = await self._predict_single_model(model_name, signal)
                predictions[model_name] = prediction
            
            # ì•™ìƒë¸” ê²°í•©
            ensemble_score = await self._combine_predictions(predictions)
            
            # ì˜ˆì¸¡ ë©”íƒ€ë°ì´í„°
            metadata = {
                "model_versions": {name: info["version"] for name, info in self.model_cache.items()},
                "individual_predictions": predictions,
                "ensemble_method": "weighted_average",
                "prediction_timestamp": datetime.utcnow().isoformat()
            }
            
            return ensemble_score, metadata
            
        except Exception as e:
            logging.error(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # í´ë°± ì˜ˆì¸¡
            return signal.confidence * 0.9, {"error": str(e)}
    
    async def _predict_single_model(self, model_name: str, signal: TradingSignal) -> float:
        """ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡"""
        try:
            model_info = self.model_cache.get(model_name, {})
            base_accuracy = model_info.get("accuracy", 0.8)
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ì˜ˆì¸¡ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ inference)
            import random
            random.seed(hash(signal.signal_id) % 1000)
            noise = random.uniform(-0.05, 0.05)
            
            prediction = min(1.0, max(0.0, signal.confidence * base_accuracy + noise))
            return prediction
            
        except Exception as e:
            logging.error(f"ëª¨ë¸ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return 0.0
    
    async def _combine_predictions(self, predictions: Dict[str, float]) -> float:
        """ì˜ˆì¸¡ ê²°í•©"""
        if not predictions:
            return 0.0
        
        # ê°€ì¤‘ í‰ê·  (í˜„ì¬ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = {
            "phoenix95_current": 0.7,
            "phoenix95_fallback": 0.3
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for model_name, prediction in predictions.items():
            weight = weights.get(model_name, 0.5)
            weighted_sum += prediction * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    async def record_prediction_result(self, signal_id: str, prediction: float, 
                                     actual_result: Optional[float]):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡"""
        try:
            if actual_result is not None:
                performance_record = {
                    "signal_id": signal_id,
                    "prediction": prediction,
                    "actual": actual_result,
                    "error": abs(prediction - actual_result),
                    "timestamp": datetime.utcnow()
                }
                
                self.model_performance_history.append(performance_record)
                
                # ì„±ëŠ¥ ì €í•˜ ê°ì§€
                if len(self.model_performance_history) >= 100:
                    recent_errors = [r["error"] for r in list(self.model_performance_history)[-100:]]
                    avg_error = np.mean(recent_errors)
                    
                    if avg_error > 0.1:  # 10% ì´ìƒ ì˜¤ì°¨
                        logging.warning(f"ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€: í‰ê·  ì˜¤ì°¨ {avg_error:.1%}")
                        
        except Exception as e:
            logging.error(f"ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def _training_scheduler(self):
        """ì¬í›ˆë ¨ ìŠ¤ì¼€ì¤„ëŸ¬"""
        while True:
            try:
                # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
                should_retrain = await self._should_retrain()
                
                if should_retrain:
                    await self._retrain_models()
                
                # 24ì‹œê°„ë§ˆë‹¤ ì²´í¬
                interval = self.config["training"]["retrain_interval"]
                await asyncio.sleep(interval)
                
            except Exception as e:
                logging.error(f"ì¬í›ˆë ¨ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)  # 1ì‹œê°„ í›„ ì¬ì‹œë„
    
    async def _should_retrain(self) -> bool:
        """ì¬í›ˆë ¨ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        try:
            if len(self.model_performance_history) < 100:
                return False
            
            # ìµœê·¼ ì„±ëŠ¥ í‰ê°€
            recent_performance = list(self.model_performance_history)[-100:]
            avg_error = np.mean([r["error"] for r in recent_performance])
            
            # ì„±ëŠ¥ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì¬í›ˆë ¨
            return avg_error > 0.15  # 15% ì´ìƒ ì˜¤ì°¨
            
        except Exception as e:
            logging.error(f"ì¬í›ˆë ¨ íŒë‹¨ ì‹¤íŒ¨: {e}")
            return False
    
    async def _retrain_models(self):
        """ëª¨ë¸ ì¬í›ˆë ¨"""
        try:
            logging.info("ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘")
            
            # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            training_data = await self._prepare_training_data()
            
            if not training_data:
                logging.warning("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ì–´ ì¬í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                return
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ì¬í›ˆë ¨ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ í•™ìŠµ)
            await asyncio.sleep(5)  # í›ˆë ¨ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            
            # ìƒˆ ëª¨ë¸ ë²„ì „ ìƒì„±
            old_version = self.current_model_version
            version_parts = old_version.split('.')
            new_minor = int(version_parts[1]) + 1
            self.current_model_version = f"v{version_parts[0][1:]}.{new_minor}"
            
            # ëª¨ë¸ ìºì‹œ ì—…ë°ì´íŠ¸
            self.model_cache["phoenix95_current"] = {
                "version": self.current_model_version,
                "accuracy": 0.87,  # ê°œì„ ëœ ì •í™•ë„
                "loaded_at": datetime.utcnow()
            }
            
            logging.info(f"ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ: {old_version} -> {self.current_model_version}")
            
        except Exception as e:
            logging.error(f"ëª¨ë¸ ì¬í›ˆë ¨ ì‹¤íŒ¨: {e}")
    
    async def _prepare_training_data(self) -> List[Dict]:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        try:
            async with self.postgres.acquire() as conn:
                # ìµœê·¼ ê±°ë˜ ë°ì´í„° ì¡°íšŒ
                trades = await conn.fetch("""
                    SELECT t.*, s.confidence, s.phoenix95_score 
                    FROM trade_history t
                    JOIN signals s ON t.position_id = s.signal_id
                    WHERE t.entry_time >= NOW() - INTERVAL '30 days'
                    ORDER BY t.entry_time DESC
                    LIMIT 1000
                """)
                
                return [dict(trade) for trade in trades]
                
        except Exception as e:
            logging.error(f"í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return []
    
    async def _performance_monitor(self):
        """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                if len(self.model_performance_history) >= 50:
                    recent_performance = list(self.model_performance_history)[-50:]
                    
                    avg_error = np.mean([r["error"] for r in recent_performance])
                    accuracy = 1 - avg_error
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
                    logging.info(f"ML ëª¨ë¸ ì„±ëŠ¥: ì •í™•ë„ {accuracy:.1%}, í‰ê·  ì˜¤ì°¨ {avg_error:.1%}")
                
                await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                logging.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = {
                "current_version": self.current_model_version,
                "loaded_models": len(self.model_cache),
                "performance_records": len(self.model_performance_history),
                "models": {}
            }
            
            for name, info in self.model_cache.items():
                status["models"][name] = {
                    "version": info["version"],
                    "accuracy": info["accuracy"],
                    "loaded_at": info["loaded_at"].isoformat()
                }
            
            # ìµœê·¼ ì„±ëŠ¥
            if self.model_performance_history:
                recent = list(self.model_performance_history)[-10:]
                avg_error = np.mean([r["error"] for r in recent])
                status["recent_performance"] = {
                    "accuracy": 1 - avg_error,
                    "avg_error": avg_error,
                    "sample_size": len(recent)
                }
            
            return status
            
        except Exception as e:
            logging.error(f"ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ§  Phoenix 95 AI ì—”ì§„ (Enhanced)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Phoenix95AIEngine:
    """Phoenix 95 Ultimate AI ì—”ì§„ - Enhanced with ML Pipeline"""
    
    def __init__(self, ml_pipeline_manager: Optional[MLPipelineManager] = None):
        self.config = SystemConfig.PHOENIX95
        self.analysis_cache: Dict[str, Dict[str, Any]] = {}
        self.model_performance = deque(maxlen=1000)
        self.ml_pipeline = ml_pipeline_manager
        
        # V3ì—ì„œ ê²€ì¦ëœ íŒŒë¼ë¯¸í„°
        self.confidence_multiplier = self.config["confidence_multiplier"]
        self.market_weight = self.config["market_condition_weight"]
        
        # ìºì‹œ ê´€ë¦¬
        self.max_cache_size = self.config["max_cache_size"]
        self.cache_ttl = self.config["cache_ttl"]
        
    async def analyze_signal_complete(self, signal: TradingSignal) -> Tuple[float, float, str]:
        """
        ì™„ì „ ì‹ í˜¸ ë¶„ì„ - Enhanced with ML Pipeline
        Returns: (phoenix95_score, kelly_ratio, recommendation)
        """
        start_time = time.time()
        
        try:
            # 1. ìºì‹œ í™•ì¸ (TTL ì²´í¬ í¬í•¨)
            cache_key = f"{signal.symbol}_{signal.price}_{signal.confidence}"
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                return cached_result
            
            # 2. ML Pipeline ì˜ˆì¸¡ (if available)
            ml_boost = 1.0
            ml_metadata = {}
            if self.ml_pipeline:
                try:
                    ml_score, ml_metadata = await self.ml_pipeline.predict_with_ensemble(signal)
                    ml_boost = ml_score / signal.confidence if signal.confidence > 0 else 1.0
                    ml_boost = max(0.8, min(1.2, ml_boost))  # 20% ë²”ìœ„ ë‚´ì—ì„œ ë¶€ìŠ¤íŠ¸
                except Exception as e:
                    logging.warning(f"ML ì˜ˆì¸¡ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
            
            # 3. ê¸°ì¡´ Phoenix 95 ë¶„ì„
            phoenix95_score = await self._phoenix95_core_analysis(signal)
            
            # 4. ML ë¶€ìŠ¤íŠ¸ ì ìš©
            phoenix95_score *= ml_boost
            
            # 5. V4 ì•™ìƒë¸” ëª¨ë¸ ë¶„ì„
            ensemble_boost = await self._ensemble_analysis(signal)
            phoenix95_score = min(phoenix95_score * ensemble_boost, self.config["max_confidence"])
            
            # 6. ì‹œì¥ ì¡°ê±´ íŒ©í„°
            market_factor = await self._analyze_market_conditions(signal.symbol)
            phoenix95_score *= market_factor
            
            # 7. Kelly Criterion ê³„ì‚°
            kelly_ratio = await self._calculate_kelly_criterion(phoenix95_score)
            
            # 8. ìµœì¢… ì¶”ì²œ ìƒì„±
            recommendation = await self._generate_recommendation(phoenix95_score, kelly_ratio)
            
            # 9. ê²°ê³¼ ìºì‹±
            result = (phoenix95_score, kelly_ratio, recommendation)
            self._store_in_cache(cache_key, result)
            
            # 10. ì„±ëŠ¥ ì¶”ì 
            analysis_time = time.time() - start_time
            self.model_performance.append({
                'score': phoenix95_score,
                'analysis_time': analysis_time,
                'ml_boost': ml_boost,
                'ml_metadata': ml_metadata,
                'timestamp': datetime.utcnow()
            })
            
            return result
            
        except Exception as e:
            logging.error(f"Phoenix95 ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.0, 0.0, "REJECT"
    
    def _get_from_cache(self, cache_key: str) -> Optional[Tuple[float, float, str]]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ (TTL ì²´í¬ í¬í•¨)"""
        if cache_key in self.analysis_cache:
            cached = self.analysis_cache[cache_key]
            # TTL ì²´í¬
            if (datetime.utcnow() - cached['timestamp']).seconds < self.cache_ttl:
                return cached['result']
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.analysis_cache[cache_key]
        return None
    
    def _store_in_cache(self, cache_key: str, result: Tuple[float, float, str]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬ í¬í•¨)"""
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.analysis_cache) >= self.max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±° (LRU ë°©ì‹)
            oldest_keys = sorted(
                self.analysis_cache.keys(),
                key=lambda k: self.analysis_cache[k]['timestamp']
            )[:100]  # 100ê°œì”© ì œê±°
            
            for key in oldest_keys:
                del self.analysis_cache[key]
        
        self.analysis_cache[cache_key] = {
            'result': result,
            'timestamp': datetime.utcnow()
        }
    
    async def _phoenix95_core_analysis(self, signal: TradingSignal) -> float:
        """Phoenix 95 í•µì‹¬ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ - V3 ê²€ì¦ëœ ë¡œì§"""
        base_confidence = signal.confidence
        
        # V3ì—ì„œ ê²€ì¦ëœ ì‹ ë¢°ë„ ë¶€ìŠ¤íŒ…
        boosted_confidence = base_confidence * self.confidence_multiplier
        
        # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ (V3 íŒ¨í„´)
        hour = datetime.utcnow().hour
        time_weight = 1.0
        if 8 <= hour <= 16:  # ìœ ëŸ½ ì‹œê°„
            time_weight = 1.1
        elif hour >= 21 or hour <= 5:  # ë¯¸êµ­ ì‹œê°„
            time_weight = 1.05
        
        # ì‹¬ë³¼ë³„ ê°€ì¤‘ì¹˜
        symbol_weights = {
            "BTCUSDT": 1.0,
            "ETHUSDT": 0.95,
            "ADAUSDT": 0.9,
            "SOLUSDT": 0.9
        }
        symbol_weight = symbol_weights.get(signal.symbol, 0.85)
        
        # ìµœì¢… Phoenix 95 ì ìˆ˜
        phoenix95_score = boosted_confidence * time_weight * symbol_weight
        return min(phoenix95_score, self.config["max_confidence"])
    
    async def _ensemble_analysis(self, signal: TradingSignal) -> float:
        """ì•™ìƒë¸” ëª¨ë¸ ë¶„ì„ - V4 ê³ ë„í™”"""
        weights = self.config["ensemble_weights"]
        
        # LSTM ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
        lstm_score = min(signal.confidence * 1.05, 1.0)
        
        # Transformer ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜  
        transformer_score = min(signal.confidence * 1.08, 1.0)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë¶€ìŠ¤íŠ¸ íŒ©í„° ê³„ì‚°
        ensemble_score = (
            weights["phoenix95"] * signal.confidence +
            weights["lstm"] * lstm_score +
            weights["transformer"] * transformer_score
        )
        
        # ë¶€ìŠ¤íŠ¸ íŒ©í„° ë°˜í™˜ (1.0 = ë³€í™”ì—†ìŒ, 1.1 = 10% ì¦ê°€)
        boost_factor = ensemble_score / signal.confidence if signal.confidence > 0 else 1.0
        return min(boost_factor, 1.15)  # ìµœëŒ€ 15% ë¶€ìŠ¤íŠ¸
    
    async def _analyze_market_conditions(self, symbol: str) -> float:
        """ì‹œì¥ ì¡°ê±´ ë¶„ì„"""
        try:
            # ì‹¤ì œë¡œëŠ” Binance APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            # ì—¬ê¸°ì„œëŠ” ì‹œê°„ ê¸°ë°˜ ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
            hour = datetime.utcnow().hour
            
            # ê±°ë˜ëŸ‰ í™œë°œí•œ ì‹œê°„ëŒ€
            if 8 <= hour <= 16 or 21 <= hour <= 23:
                return 1.1
            elif 2 <= hour <= 6:  # ì €ì¡°í•œ ì‹œê°„ëŒ€
                return 0.9
            else:
                return 1.0
                
        except Exception as e:
            logging.error(f"ì‹œì¥ ì¡°ê±´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 1.0
    
    async def _calculate_kelly_criterion(self, phoenix95_score: float) -> float:
        """Kelly Criterion ê³„ì‚° - V3 ê²€ì¦ëœ ê³µì‹"""
        try:
            # ìŠ¹ë¥  ì¶”ì • (Phoenix 95 ì ìˆ˜ ê¸°ë°˜)
            win_probability = phoenix95_score
            
            # ì†ìµ ë¹„ìœ¨ (ì†ì ˆ 1.5% vs ìµì ˆ 3% = 1:2)
            win_loss_ratio = (
                SystemConfig.TRADING["take_profit_pct"] / 
                SystemConfig.TRADING["stop_loss_pct"]
            )
            
            # Kelly Formula: (bp - q) / b
            # b = ì†ìµë¹„ìœ¨, p = ìŠ¹ë¥ , q = íŒ¨ë°°ìœ¨
            kelly_ratio = (
                (win_probability * win_loss_ratio - (1 - win_probability)) / 
                win_loss_ratio
            )
            
            # ë³´ìˆ˜ì  ì œí•œ (ìµœëŒ€ 25%)
            kelly_ratio = max(0.01, min(kelly_ratio, SystemConfig.TRADING["kelly_max"]))
            
            return kelly_ratio
        except (ZeroDivisionError, TypeError) as e:
            logging.error(f"Kelly Criterion ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.01
    
    async def _generate_recommendation(self, phoenix95_score: float, kelly_ratio: float) -> str:
        """ìµœì¢… ì¶”ì²œ ìƒì„±"""
        if phoenix95_score >= 0.95 and kelly_ratio >= 0.2:
            return "STRONG_BUY"
        elif phoenix95_score >= 0.85 and kelly_ratio >= 0.15:
            return "BUY"
        elif phoenix95_score >= 0.75 and kelly_ratio >= 0.1:
            return "WEAK_BUY"
        elif phoenix95_score >= 0.6:
            return "HOLD"
        else:
            return "REJECT"
    
    def get_performance_stats(self) -> Dict[str, Union[float, int]]:
        """ëª¨ë¸ ì„±ëŠ¥ í†µê³„ - Enhanced"""
        if not self.model_performance:
            return {}
        
        performances = list(self.model_performance)
        scores = [p['score'] for p in performances]
        times = [p['analysis_time'] for p in performances]
        ml_boosts = [p.get('ml_boost', 1.0) for p in performances]
        
        return {
            "avg_score": np.mean(scores),
            "score_std": np.std(scores),
            "avg_analysis_time": np.mean(times),
            "max_analysis_time": np.max(times),
            "total_analyses": len(performances),
            "cache_size": len(self.analysis_cache),
            "avg_ml_boost": np.mean(ml_boosts),
            "ml_pipeline_active": self.ml_pipeline is not None,
            "cache_hit_rate": 0.0  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìºì‹œ íˆíŠ¸ìœ¨ ì¶”ì 
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              âš¡ ë ˆë²„ë¦¬ì§€ íŠ¸ë ˆì´ë”© ì—”ì§„ (Enhanced)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class UltimateLeverageTrader:
    """Ultimate ë ˆë²„ë¦¬ì§€ íŠ¸ë ˆì´ë”© ì—”ì§„ - Enhanced with Lambda Architecture"""
    
    def __init__(self, redis_client, postgres_pool, telegram_notifier, 
                 lambda_architecture: Optional[LambdaDataArchitecture] = None):
        self.redis = redis_client
        self.postgres = postgres_pool
        self.telegram = telegram_notifier
        self.lambda_arch = lambda_architecture
        self.active_positions: Dict[str, Position] = {}
        self.monitoring_tasks: Dict[str, asyncio.Task] = {}
        self.risk_metrics = deque(maxlen=1000)
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = SystemConfig.TRADING["max_retries"]
        self.retry_delay = SystemConfig.TRADING["retry_delay"]
        
    async def execute_trade_complete(self, signal: TradingSignal, 
                                   phoenix95_score: float, 
                                   kelly_ratio: float) -> Optional[Position]:
        """ì™„ì „í•œ ê±°ë˜ ì‹¤í–‰ - Enhanced with Lambda Architecture"""
        execution_start = time.time()
        
        for attempt in range(self.max_retries + 1):
            try:
                # 1. ì¢…í•© ë¦¬ìŠ¤í¬ ì²´í¬
                risk_check = await self._comprehensive_risk_check(signal)
                if not risk_check["approved"]:
                    logging.warning(f"ë¦¬ìŠ¤í¬ ì²´í¬ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {risk_check['reason']}")
                    if attempt == self.max_retries:
                        await self.telegram.send_message(f"âš ï¸ ê±°ë˜ ê±°ë¶€: {risk_check['reason']}")
                        return None
                    await asyncio.sleep(self.retry_delay)
                    continue
                
                # 2. í¬ì§€ì…˜ í¬ê¸° ê³„ì‚° (Kelly + ë¦¬ìŠ¤í¬ ì¡°ì •)
                position_size_usd = await self._calculate_optimal_position_size(kelly_ratio, signal)
                
                # 3. ë ˆë²„ë¦¬ì§€ ê³„ì‚°
                leverage = await self._calculate_optimal_leverage(phoenix95_score)
                
                # 4. ìˆ˜ëŸ‰ ë° ë§ˆì§„ ê³„ì‚° (Zero Division ë°©ì§€)
                if signal.price <= 0 or leverage <= 0:
                    raise ValueError(f"ì˜ëª»ëœ ê°€ê²© ë˜ëŠ” ë ˆë²„ë¦¬ì§€: price={signal.price}, leverage={leverage}")
                
                quantity = position_size_usd / signal.price
                margin_required = position_size_usd / leverage
                
                # 5. ë¦¬ìŠ¤í¬ ê°€ê²© ê³„ì‚°
                liquidation_price = self._calculate_liquidation_price(signal, leverage)
                stop_loss_price = self._calculate_stop_loss_price(signal)
                take_profit_price = self._calculate_take_profit_price(signal)
                
                # 6. í¬ì§€ì…˜ ê°ì²´ ìƒì„±
                position = Position(
                    position_id=f"POS_{uuid.uuid4().hex[:8].upper()}",
                    signal_id=signal.signal_id,
                    symbol=signal.symbol,
                    side=signal.action.upper(),
                    entry_price=signal.price,
                    quantity=quantity,
                    leverage=leverage,
                    margin_mode=SystemConfig.TRADING["margin_mode"],
                    margin_required=margin_required,
                    liquidation_price=liquidation_price,
                    stop_loss_price=stop_loss_price,
                    take_profit_price=take_profit_price,
                    current_price=signal.price
                )
                
                # 7. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                await self._save_position_to_db(position)
                await self._save_signal_to_db(signal)
                
                # 8. ë©”ëª¨ë¦¬ì— í¬ì§€ì…˜ ì¶”ê°€
                self.active_positions[position.position_id] = position
                
                # 9. Lambda Architectureì— ë°ì´í„° ì „ì†¡
                if self.lambda_arch:
                    await self.lambda_arch.speed_layer.process_real_time_signal(signal)
                
                # 10. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                await self._start_position_monitoring(position)
                
                # 11. ì‹¤í–‰ ì‹œê°„ ê²€ì¦
                execution_time = (time.time() - execution_start) * 1000
                if execution_time > 500:
                    logging.warning(f"ê±°ë˜ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: {execution_time:.0f}ms")
                
                # 12. ì„±ê³µ ì•Œë¦¼
                await self.telegram.send_trade_notification(position, phoenix95_score, kelly_ratio)
                
                logging.info(f"ê±°ë˜ ì‹¤í–‰ ì™„ë£Œ: {position.position_id} ({execution_time:.0f}ms)")
                return position
                
            except Exception as e:
                logging.error(f"ê±°ë˜ ì‹¤í–‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                if attempt == self.max_retries:
                    await self.telegram.send_message(f"ğŸš¨ ê±°ë˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
                    return None
                await asyncio.sleep(self.retry_delay)
        
        return None
    
    # [ê¸°ì¡´ ë©”ì„œë“œë“¤ì€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ...]
    
    async def _comprehensive_risk_check(self, signal: TradingSignal) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬ìŠ¤í¬ ì²´í¬ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)"""
        risk_factors = []
        
        # 1. ìµœëŒ€ í¬ì§€ì…˜ ìˆ˜ ì²´í¬
        if len(self.active_positions) >= SystemConfig.TRADING["max_positions"]:
            return {"approved": False, "reason": "ìµœëŒ€ í¬ì§€ì…˜ ìˆ˜ ì´ˆê³¼"}
        
        # 2. ì¼ì¼ ì†ì‹¤ í•œë„ ì²´í¬
        daily_pnl = await self._get_daily_pnl()
        if daily_pnl <= -SystemConfig.TRADING["max_daily_loss"]:
            return {"approved": False, "reason": "ì¼ì¼ ì†ì‹¤ í•œë„ ì´ˆê³¼"}
        
        # 3. ì‹¬ë³¼ ì¤‘ë³µ ì²´í¬  
        symbol_positions = [p for p in self.active_positions.values() if p.symbol == signal.symbol]
        if len(symbol_positions) >= 1:  # ì‹¬ë³¼ë‹¹ 1ê°œë§Œ
            return {"approved": False, "reason": f"{signal.symbol} í¬ì§€ì…˜ ì´ë¯¸ ì¡´ì¬"}
        
        # 4. ì‹œì¥ ì‹œê°„ ì²´í¬
        now = datetime.utcnow()
        if now.weekday() >= 5:  # ì£¼ë§
            return {"approved": False, "reason": "ì£¼ë§ ê±°ë˜ ê¸ˆì§€"}
        
        # 5. ì‹ ë¢°ë„ ì¬í™•ì¸ (None ì²´í¬ í¬í•¨)
        if signal.phoenix95_score is not None and signal.phoenix95_score < SystemConfig.TRADING["confidence_threshold"]:
            risk_factors.append("ë‚®ì€ ì‹ ë¢°ë„")
        
        # 6. ë³€ë™ì„± ì²´í¬ (ê°„ë‹¨í•œ êµ¬í˜„)
        volatility_risk = await self._check_volatility_risk(signal.symbol)
        if volatility_risk > 0.8:
            risk_factors.append("ë†’ì€ ë³€ë™ì„±")
        
        risk_score = len(risk_factors) / 6  # ì •ê·œí™”
        
        return {
            "approved": risk_score < 0.5,
            "reason": "ë¦¬ìŠ¤í¬ ì²´í¬ í†µê³¼" if risk_score < 0.5 else f"ìœ„í—˜ ìš”ì†Œ: {', '.join(risk_factors)}",
            "risk_score": risk_score,
            "risk_factors": risk_factors
        }
    
    async def _calculate_optimal_position_size(self, kelly_ratio: float, signal: TradingSignal) -> float:
        """ìµœì  í¬ì§€ì…˜ í¬ê¸° ê³„ì‚° (ì•ˆì „í•œ ê³„ì‚°)"""
        try:
            # ê³„ì¢Œ ì”ê³  (ì‹¤ì œë¡œëŠ” ê±°ë˜ì†Œ APIì—ì„œ ì¡°íšŒ)
            account_balance = float(os.getenv("ACCOUNT_BALANCE", "50000"))
            
            # Kelly ê¸°ë°˜ ê¸°ë³¸ í¬ì§€ì…˜ í¬ê¸°
            kelly_position = account_balance * kelly_ratio
            
            # ì„¤ì •ëœ ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸° ì œí•œ
            max_position_size = account_balance * SystemConfig.TRADING["position_size_pct"]
            
            # ìµœì¢… í¬ì§€ì…˜ í¬ê¸° (ë³´ìˆ˜ì  ì ‘ê·¼)
            position_size = min(kelly_position, max_position_size)
            
            # ìµœì†Œ/ìµœëŒ€ ì œí•œ
            min_position = 100.0  # ìµœì†Œ $100
            max_position = 10000.0  # ìµœëŒ€ $10,000
            
            return max(min_position, min(position_size, max_position))
        except (ValueError, TypeError) as e:
            logging.error(f"í¬ì§€ì…˜ í¬ê¸° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 100.0  # ê¸°ë³¸ê°’
    
    async def _calculate_optimal_leverage(self, phoenix95_score: float) -> int:
        """ìµœì  ë ˆë²„ë¦¬ì§€ ê³„ì‚°"""
        max_leverage = SystemConfig.TRADING["max_leverage"]
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ë ˆë²„ë¦¬ì§€ ì¡°ì •
        if phoenix95_score >= 0.95:
            return max_leverage
        elif phoenix95_score >= 0.9:
            return min(8, max_leverage)
        elif phoenix95_score >= 0.85:
            return min(5, max_leverage)
        else:
            return min(3, max_leverage)
    
    def _calculate_liquidation_price(self, signal: TradingSignal, leverage: int) -> float:
        """ì²­ì‚°ê°€ ê³„ì‚° (ì•ˆì „í•œ ê³„ì‚°)"""
        try:
            maintenance_margin_rate = 0.004  # 0.4%
            
            if signal.action.lower() == "buy":
                return signal.price * (1 - (1/leverage) + maintenance_margin_rate)
            else:
                return signal.price * (1 + (1/leverage) - maintenance_margin_rate)
        except (ZeroDivisionError, TypeError) as e:
            logging.error(f"ì²­ì‚°ê°€ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return signal.price * 0.9  # ì•ˆì „í•œ ê¸°ë³¸ê°’
    
    def _calculate_stop_loss_price(self, signal: TradingSignal) -> float:
        """ì†ì ˆê°€ ê³„ì‚°"""
        stop_pct = SystemConfig.TRADING["stop_loss_pct"]
        
        if signal.action.lower() == "buy":
            return signal.price * (1 - stop_pct)
        else:
            return signal.price * (1 + stop_pct)
    
    def _calculate_take_profit_price(self, signal: TradingSignal) -> float:
        """ìµì ˆê°€ ê³„ì‚°"""
        profit_pct = SystemConfig.TRADING["take_profit_pct"]
        
        if signal.action.lower() == "buy":
            return signal.price * (1 + profit_pct)
        else:
            return signal.price * (1 - profit_pct)
    
    async def _start_position_monitoring(self, position: Position):
        """ì‹¤ì‹œê°„ í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        task = asyncio.create_task(self._monitor_position_realtime(position))
        self.monitoring_tasks[position.position_id] = task
        
        # Redisì— í¬ì§€ì…˜ ì •ë³´ ì €ì¥
        try:
            await self.redis.hset(
                f"position:{position.position_id}",
                mapping={
                    "symbol": position.symbol,
                    "side": position.side,
                    "entry_price": str(position.entry_price),
                    "quantity": str(position.quantity),
                    "leverage": str(position.leverage),
                    "status": position.status,
                    "created_at": position.created_at.isoformat()
                }
            )
        except Exception as e:
            logging.error(f"Redis í¬ì§€ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _monitor_position_realtime(self, position: Position):
        """ì‹¤ì‹œê°„ í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ (ë¬´í•œë£¨í”„ ë°©ì§€ ë° ì¬ì‹œë„ ì œí•œ)"""
        logging.info(f"í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {position.position_id}")
        
        retry_count = 0
        max_consecutive_errors = 10  # ì—°ì† ì˜¤ë¥˜ ì œí•œ
        
        while position.status == "OPEN" and retry_count < max_consecutive_errors:
            try:
                # í˜„ì¬ê°€ ì¡°íšŒ
                current_price = await self._get_current_price(position.symbol)
                if current_price <= 0:
                    raise ValueError(f"ì˜ëª»ëœ ê°€ê²©: {current_price}")
                
                position.current_price = current_price
                
                # P&L ê³„ì‚°
                pnl, pnl_pct = position.calculate_pnl(current_price)
                position.unrealized_pnl = pnl
                position.pnl_percentage = pnl_pct
                
                # ì²­ì‚° ìœ„í—˜ë„ ê³„ì‚°
                risk = position.calculate_liquidation_risk(current_price)
                position.liquidation_risk = risk
                
                # ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                exit_reason = await self._check_exit_conditions(position, current_price)
                if exit_reason:
                    await self._close_position(position, current_price, exit_reason)
                    break
                
                # Redis ì—…ë°ì´íŠ¸ (ì•ˆì „í•œ ì—…ë°ì´íŠ¸)
                try:
                    await self.redis.hset(
                        f"position:{position.position_id}",
                        mapping={
                            "current_price": str(current_price),
                            "unrealized_pnl": str(pnl),
                            "pnl_percentage": str(pnl_pct),
                            "liquidation_risk": str(risk),
                            "updated_at": datetime.utcnow().isoformat()
                        }
                    )
                except Exception as redis_error:
                    logging.warning(f"Redis ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {redis_error}")
                
                # ìœ„í—˜ ì•Œë¦¼ ì²´í¬
                if risk > 0.8:
                    await self.telegram.send_liquidation_warning(position)
                
                # ì„±ê³µì ì¸ ë°˜ë³µ í›„ retry_count ë¦¬ì…‹
                retry_count = 0
                await asyncio.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                retry_count += 1
                logging.error(f"í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜ {position.position_id} (ì¬ì‹œë„ {retry_count}): {e}")
                
                if retry_count >= max_consecutive_errors:
                    logging.critical(f"í¬ì§€ì…˜ {position.position_id} ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨ (ìµœëŒ€ ì˜¤ë¥˜ íšŸìˆ˜ ì´ˆê³¼)")
                    break
                
                await asyncio.sleep(min(5 * retry_count, 30))  # ì§€ìˆ˜ì  ë°±ì˜¤í”„ (ìµœëŒ€ 30ì´ˆ)
        
        # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì •ë¦¬
        if position.position_id in self.monitoring_tasks:
            del self.monitoring_tasks[position.position_id]
        
        if retry_count >= max_consecutive_errors:
            await self.telegram.send_system_notification(
                f"í¬ì§€ì…˜ {position.position_id} ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨", "CRITICAL"
            )
    
    async def _get_current_price(self, symbol: str) -> float:
        """í˜„ì¬ê°€ ì¡°íšŒ (ì‹œë®¬ë ˆì´ì…˜ - í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤ì œ API ì‚¬ìš© í•„ìš”)"""
        # âš ï¸ ê²½ê³ : ì´ê²ƒì€ ì‹œë®¬ë ˆì´ì…˜ì…ë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Binance API ì‚¬ìš©í•˜ì„¸ìš”!
        logging.warning(f"âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {symbol} ê°€ê²© ì¡°íšŒ")
        
        base_prices = {
            "BTCUSDT": 45000, "ETHUSDT": 3000, "ADAUSDT": 0.5,
            "SOLUSDT": 100, "AVAXUSDT": 40, "DOTUSDT": 8
        }
        
        base_price = base_prices.get(symbol, 45000)
        
        # ê°„ë‹¨í•œ ê°€ê²© ë³€ë™ ì‹œë®¬ë ˆì´ì…˜
        import random
        change = random.uniform(-0.01, 0.01)  # Â±1% ë³€ë™
        price = base_price * (1 + change)
        
        # ê°€ê²© ìœ íš¨ì„± ê²€ì¦
        if price <= 0:
            logging.error(f"ì˜ëª»ëœ ê°€ê²© ìƒì„±: {price}")
            return base_price
        
        return price
    
    async def _check_exit_conditions(self, position: Position, current_price: float) -> Optional[str]:
        """í¬ì§€ì…˜ ì¢…ë£Œ ì¡°ê±´ ì²´í¬ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)"""
        try:
            # ì†ì ˆê°€ ì²´í¬
            if position.side == "BUY" and current_price <= position.stop_loss_price:
                return "STOP_LOSS"
            if position.side == "SELL" and current_price >= position.stop_loss_price:
                return "STOP_LOSS"
            
            # ìµì ˆê°€ ì²´í¬  
            if position.side == "BUY" and current_price >= position.take_profit_price:
                return "TAKE_PROFIT"
            if position.side == "SELL" and current_price <= position.take_profit_price:
                return "TAKE_PROFIT"
            
            # ê¸´ê¸‰ ì²­ì‚° ì²´í¬ (ì²­ì‚°ê°€ 5% ì´ë‚´ ì ‘ê·¼)
            if position.liquidation_risk > 0.95:
                return "EMERGENCY_LIQUIDATION"
            
            return None
        except Exception as e:
            logging.error(f"ì¢…ë£Œ ì¡°ê±´ ì²´í¬ ì˜¤ë¥˜: {e}")
            return None
    
    async def _close_position(self, position: Position, exit_price: float, exit_reason: str):
        """í¬ì§€ì…˜ ì²­ì‚° (ì•ˆì „í•œ ì²­ì‚°)"""
        try:
            position.status = "CLOSED"
            position.exit_price = exit_price
            position.exit_time = datetime.utcnow()
            position.exit_reason = exit_reason
            
            # ìµœì¢… P&L ê³„ì‚°
            final_pnl, final_pnl_pct = position.calculate_pnl(exit_price)
            position.realized_pnl = final_pnl
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            await self._update_position_in_db(position)
            await self._save_trade_history(position)
            
            # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            if position.position_id in self.active_positions:
                del self.active_positions[position.position_id]
            
            # Redis ì •ë¦¬
            try:
                await self.redis.delete(f"position:{position.position_id}")
            except Exception as redis_error:
                logging.warning(f"Redis ì •ë¦¬ ì‹¤íŒ¨: {redis_error}")
            
            # ì²­ì‚° ì•Œë¦¼
            await self.telegram.send_close_notification(position, exit_reason)
            
            logging.info(f"í¬ì§€ì…˜ ì²­ì‚° ì™„ë£Œ: {position.position_id} | P&L: ${final_pnl:.2f} | ì‚¬ìœ : {exit_reason}")
        except Exception as e:
            logging.error(f"í¬ì§€ì…˜ ì²­ì‚° ì‹¤íŒ¨: {e}")
    
    async def _save_position_to_db(self, position: Position):
        """í¬ì§€ì…˜ DB ì €ì¥ (ì—°ê²° íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„)"""
        for attempt in range(3):  # 3íšŒ ì¬ì‹œë„
            try:
                async with asyncio.wait_for(self.postgres.acquire(), timeout=10) as conn:
                    await conn.execute("""
                        INSERT INTO positions (
                            position_id, signal_id, symbol, side, entry_price, quantity,
                            leverage, margin_mode, margin_required, liquidation_price,
                            stop_loss_price, take_profit_price, status, created_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                    """, position.position_id, position.signal_id, position.symbol, position.side,
                    position.entry_price, position.quantity, position.leverage, position.margin_mode,
                    position.margin_required, position.liquidation_price, position.stop_loss_price,
                    position.take_profit_price, position.status, position.created_at)
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
            except asyncio.TimeoutError:
                logging.warning(f"í¬ì§€ì…˜ DB ì €ì¥ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})")
            except Exception as e:
                logging.error(f"í¬ì§€ì…˜ DB ì €ì¥ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            
            if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
    
    async def _save_signal_to_db(self, signal: TradingSignal):
        """ì‹ í˜¸ DB ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(3):
            try:
                async with asyncio.wait_for(self.postgres.acquire(), timeout=10) as conn:
                    await conn.execute("""
                        INSERT INTO signals (
                            signal_id, symbol, action, price, confidence, phoenix95_score,
                            kelly_ratio, recommendation, timestamp, processed
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    """, signal.signal_id, signal.symbol, signal.action, signal.price,
                    signal.confidence, signal.phoenix95_score, signal.kelly_ratio,
                    signal.recommendation, signal.timestamp, signal.processed)
                return
            except asyncio.TimeoutError:
                logging.warning(f"ì‹ í˜¸ DB ì €ì¥ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})")
            except Exception as e:
                logging.error(f"ì‹ í˜¸ DB ì €ì¥ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            
            if attempt < 2:
                await asyncio.sleep(2 ** attempt)
    
    async def _update_position_in_db(self, position: Position):
        """í¬ì§€ì…˜ DB ì—…ë°ì´íŠ¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(3):
            try:
                async with asyncio.wait_for(self.postgres.acquire(), timeout=10) as conn:
                    await conn.execute("""
                        UPDATE positions SET
                            current_price = $1, unrealized_pnl = $2, pnl_percentage = $3,
                            liquidation_risk = $4, status = $5, exit_price = $6,
                            exit_time = $7, exit_reason = $8, realized_pnl = $9, updated_at = $10
                        WHERE position_id = $11
                    """, position.current_price, position.unrealized_pnl, position.pnl_percentage,
                    position.liquidation_risk, position.status, position.exit_price,
                    position.exit_time, position.exit_reason, position.realized_pnl,
                    position.updated_at, position.position_id)
                return
            except Exception as e:
                logging.error(f"í¬ì§€ì…˜ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
    
    async def _save_trade_history(self, position: Position):
        """ê±°ë˜ ì´ë ¥ ì €ì¥ (ì•ˆì „í•œ ì €ì¥)"""
        try:
            duration_minutes = 0
            if position.exit_time and position.created_at:
                duration_minutes = int((position.exit_time - position.created_at).total_seconds() / 60)
            
            async with asyncio.wait_for(self.postgres.acquire(), timeout=10) as conn:
                await conn.execute("""
                    INSERT INTO trade_history (
                        position_id, symbol, side, entry_price, exit_price, quantity,
                        leverage, pnl, pnl_percentage, duration_minutes, entry_time,
                        exit_time, exit_reason
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                """, position.position_id, position.symbol, position.side, position.entry_price,
                position.exit_price, position.quantity, position.leverage, position.realized_pnl,
                position.pnl_percentage, duration_minutes, position.created_at,
                position.exit_time, position.exit_reason)
        except Exception as e:
            logging.error(f"ê±°ë˜ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _get_daily_pnl(self) -> float:
        """ì¼ì¼ P&L ì¡°íšŒ (ì•ˆì „í•œ ì¡°íšŒ)"""
        try:
            async with asyncio.wait_for(self.postgres.acquire(), timeout=5) as conn:
                result = await conn.fetchval("""
                    SELECT COALESCE(SUM(pnl), 0) FROM trade_history
                    WHERE DATE(entry_time) = CURRENT_DATE
                """)
                return float(result) if result is not None else 0.0
        except Exception as e:
            logging.error(f"ì¼ì¼ P&L ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0.0
    
    async def _check_volatility_risk(self, symbol: str) -> float:
        """ë³€ë™ì„± ìœ„í—˜ë„ ì²´í¬ (0-1)"""
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” 24h ë³€ë™ì„± ê³„ì‚°)
        volatility_map = {
            "BTCUSDT": 0.3, "ETHUSDT": 0.4, "ADAUSDT": 0.6,
            "SOLUSDT": 0.7, "AVAXUSDT": 0.6
        }
        return volatility_map.get(symbol, 0.5)
    
    def get_portfolio_summary(self) -> Dict[str, Union[int, float, List[str]]]:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)"""
        if not self.active_positions:
            return {
                "active_positions": 0,
                "total_unrealized_pnl": 0.0,
                "total_margin_used": 0.0,
                "avg_leverage": 0.0,
                "avg_liquidation_risk": 0.0,
                "symbols": []
            }
        
        positions = list(self.active_positions.values())
        
        try:
            return {
                "active_positions": len(positions),
                "total_unrealized_pnl": sum(p.unrealized_pnl for p in positions),
                "total_margin_used": sum(p.margin_required for p in positions),
                "avg_leverage": float(np.mean([p.leverage for p in positions])),
                "avg_liquidation_risk": float(np.mean([p.liquidation_risk for p in positions])),
                "symbols": [p.symbol for p in positions]
            }
        except Exception as e:
            logging.error(f"í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                "active_positions": len(positions),
                "total_unrealized_pnl": 0.0,
                "total_margin_used": 0.0,
                "avg_leverage": 0.0,
                "avg_liquidation_risk": 0.0,
                "symbols": []
            }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ“± í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ (ê¸°ì¡´ê³¼ ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class UltimateTelegramNotifier:
    """Ultimate í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ - V3 ë©”ì‹œì§€ + V4 ê³ ë„í™” (ì•ˆì „ì„± ê°•í™”)"""
    
    def __init__(self):
        self.config = SystemConfig.TELEGRAM
        self.bot_token = self.config["bot_token"]
        self.chat_id = self.config["chat_id"]
        
        # í…”ë ˆê·¸ë¨ ì„¤ì • ê²€ì¦ (None ì²´í¬ ê°•í™”)
        if not self.bot_token:
            raise ValueError("TELEGRAM_BOT_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        if not self.chat_id:
            raise ValueError("TELEGRAM_CHAT_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        self.rate_limiter = asyncio.Semaphore(5)  # ì´ˆë‹¹ 5ê°œ ë©”ì‹œì§€ ì œí•œ
        self.last_alert_times: Dict[str, datetime] = {}  # ì•Œë¦¼ ì¿¨ë‹¤ìš´ ê´€ë¦¬
        
        logging.info(f"í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (Chat ID: {self.chat_id})")
        
    async def send_message(self, message: str, level: str = "INFO") -> bool:
        """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ (ì¿¨ë‹¤ìš´ ë° ì¬ì‹œë„ ë¡œì§)"""
        if not self.config["alerts"].get(level.lower(), True):
            return False
        
        # ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì²´í¬ (ì¤‘ë³µ ë°©ì§€)
        alert_key = f"{level}_{hash(message) % 10000}"
        now = datetime.utcnow()
        
        if alert_key in self.last_alert_times:
            time_diff = (now - self.last_alert_times[alert_key]).total_seconds()
            cooldown = SystemConfig.MONITORING["alert_cooldown"]
            if time_diff < cooldown:
                logging.debug(f"ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì¤‘: {alert_key}")
                return False
        
        async with self.rate_limiter:
            success = await self._send_message_internal(message, level)
            if success:
                self.last_alert_times[alert_key] = now
            return success
    
    async def _send_message_internal(self, message: str, level: str) -> bool:
        """ë‚´ë¶€ ë©”ì‹œì§€ ì „ì†¡ ë¡œì§ (ì¬ì‹œë„ í¬í•¨)"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
                
                # ë ˆë²¨ë³„ ì´ëª¨ì§€ ì¶”ê°€
                level_emojis = {
                    "INFO": "â„¹ï¸", "WARNING": "âš ï¸", "ERROR": "âŒ", 
                    "CRITICAL": "ğŸš¨", "SUCCESS": "âœ…"
                }
                emoji = level_emojis.get(level, "ğŸ“¢")
                
                # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (í…”ë ˆê·¸ë¨ 4096ì ì œí•œ)
                max_length = 4000
                if len(message) > max_length:
                    message = message[:max_length] + "..."
                
                formatted_message = f"{emoji} <b>[{level}]</b>\n{message}"
                
                data = {
                    "chat_id": self.chat_id,
                    "text": formatted_message,
                    "parse_mode": "HTML",
                    "disable_web_page_preview": True
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, data=data, timeout=10) as response:
                        if response.status == 200:
                            return True
                        elif response.status == 429:  # Rate limit
                            retry_after = int(response.headers.get("Retry-After", 1))
                            logging.warning(f"í…”ë ˆê·¸ë¨ ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸, {retry_after}ì´ˆ ëŒ€ê¸°")
                            await asyncio.sleep(retry_after)
                        else:
                            response_text = await response.text()
                            logging.warning(f"í…”ë ˆê·¸ë¨ ì‘ë‹µ ì˜¤ë¥˜: {response.status} - {response_text}")
                            
            except asyncio.TimeoutError:
                logging.warning(f"í…”ë ˆê·¸ë¨ ì „ì†¡ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})")
            except Exception as e:
                logging.error(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
        
        return False
    
    async def send_trade_notification(self, position: Position, phoenix95_score: float, kelly_ratio: float):
        """ê±°ë˜ ì‹¤í–‰ ì•Œë¦¼ - V3 ìŠ¤íƒ€ì¼ ë©”ì‹œì§€ (ì•ˆì „í•œ ê³„ì‚°)"""
        try:
            pnl_range_low = position.margin_required * (SystemConfig.TRADING["stop_loss_pct"] * position.leverage)
            pnl_range_high = position.margin_required * (SystemConfig.TRADING["take_profit_pct"] * position.leverage)
            
            message = f"""ğŸ¯ <b>Phoenix 95 Ultimate ê±°ë˜ ì‹¤í–‰</b>

ğŸ“Š <b>{position.symbol}</b> {position.side} {position.leverage}x ({position.margin_mode})
ğŸ’° ì§„ì…ê°€: <b>${position.entry_price:,.2f}</b>
ğŸ“ˆ ìµì ˆê°€: <b>${position.take_profit_price:,.2f}</b> (+{SystemConfig.TRADING['take_profit_pct']*100:.1f}%)
ğŸ“‰ ì†ì ˆê°€: <b>${position.stop_loss_price:,.2f}</b> (-{SystemConfig.TRADING['stop_loss_pct']*100:.1f}%)
ğŸš¨ ì²­ì‚°ê°€: <b>${position.liquidation_price:,.2f}</b>

ğŸ§  Phoenix95: <b>{phoenix95_score:.1%}</b>
âš–ï¸ Kelly ë¹„ìœ¨: <b>{kelly_ratio:.1%}</b>
ğŸ’µ ìˆ˜ëŸ‰: <b>{position.quantity:.6f}</b>
ğŸ’¸ ë§ˆì§„: <b>${position.margin_required:,.2f}</b>

ğŸ“Š ì˜ˆìƒ P&L: <b>-${pnl_range_low:.0f}</b> ~ <b>+${pnl_range_high:.0f}</b>
ğŸ†” í¬ì§€ì…˜: <code>{position.position_id}</code>

ğŸ• {datetime.utcnow().strftime('%H:%M:%S UTC')}"""
            
            await self.send_message(message, "SUCCESS")
        except Exception as e:
            logging.error(f"ê±°ë˜ ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def send_close_notification(self, position: Position, exit_reason: str):
        """í¬ì§€ì…˜ ì²­ì‚° ì•Œë¦¼ (ì•ˆì „í•œ ë©”ì‹œì§€ ìƒì„±)"""
        try:
            pnl_emoji = "ğŸ“ˆ" if position.realized_pnl and position.realized_pnl > 0 else "ğŸ“‰"
            reason_emoji = {
                "TAKE_PROFIT": "ğŸ¯", "STOP_LOSS": "ğŸ›¡ï¸", 
                "EMERGENCY_LIQUIDATION": "ğŸš¨", "MANUAL": "ğŸ‘¤"
            }
            
            duration = ""
            if position.exit_time and position.created_at:
                duration_mins = int((position.exit_time - position.created_at).total_seconds() / 60)
                duration = f"â±ï¸ ê±°ë˜ì‹œê°„: <b>{duration_mins}ë¶„</b>\n"
            
            realized_pnl = position.realized_pnl or 0.0
            pnl_percentage = position.pnl_percentage or 0.0
            
            message = f"""{pnl_emoji} <b>í¬ì§€ì…˜ ì²­ì‚°</b> {reason_emoji.get(exit_reason, "ğŸ“")}

ğŸ“Š <b>{position.symbol}</b> {position.side} {position.leverage}x
ğŸ’° ì§„ì…ê°€: <b>${position.entry_price:,.2f}</b>
ğŸ’¸ ì²­ì‚°ê°€: <b>${position.exit_price:,.2f}</b>
{duration}
ğŸ’µ P&L: <b>${realized_pnl:,.2f}</b> ({pnl_percentage:+.1f}%)
ğŸ“‹ ì‚¬ìœ : <b>{exit_reason.replace('_', ' ')}</b>

ğŸ†” í¬ì§€ì…˜: <code>{position.position_id}</code>
ğŸ• {datetime.utcnow().strftime('%H:%M:%S UTC')}"""
            
            level = "SUCCESS" if realized_pnl > 0 else "WARNING"
            await self.send_message(message, level)
        except Exception as e:
            logging.error(f"ì²­ì‚° ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def send_liquidation_warning(self, position: Position):
        """ì²­ì‚° ìœ„í—˜ ê²½ê³  (ì¿¨ë‹¤ìš´ ì ìš©)"""
        try:
            message = f"""ğŸ†˜ <b>ì²­ì‚° ìœ„í—˜ ê²½ê³ </b>

ğŸ“Š <b>{position.symbol}</b> {position.side} {position.leverage}x
ğŸ’° ì§„ì…ê°€: <b>${position.entry_price:,.2f}</b>
ğŸ’¸ í˜„ì¬ê°€: <b>${position.current_price:,.2f}</b>
ğŸš¨ ì²­ì‚°ê°€: <b>${position.liquidation_price:,.2f}</b>

âš ï¸ ìœ„í—˜ë„: <b>{position.liquidation_risk:.1%}</b>
ğŸ’” ë¯¸ì‹¤í˜„ P&L: <b>${position.unrealized_pnl:,.2f}</b> ({position.pnl_percentage:+.1f}%)

ğŸ†” í¬ì§€ì…˜: <code>{position.position_id}</code>
ğŸ• {datetime.utcnow().strftime('%H:%M:%S UTC')}

âš¡ ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!"""
            
            await self.send_message(message, "CRITICAL")
        except Exception as e:
            logging.error(f"ì²­ì‚° ê²½ê³  ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def send_system_notification(self, message: str, level: str = "INFO"):
        """ì‹œìŠ¤í…œ ì•Œë¦¼"""
        try:
            system_message = f"""ğŸ–¥ï¸ <b>Phoenix 95 Ultimate System</b>

{message}

ğŸ• {datetime.utcnow().strftime('%H:%M:%S UTC')}"""
            
            await self.send_message(system_message, level)
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def send_daily_summary(self, stats: Dict[str, Any]):
        """ì¼ì¼ ì„±ê³¼ ìš”ì•½ (ì•ˆì „í•œ í†µê³„ ê³„ì‚°)"""
        try:
            total_trades = stats.get('total_trades', 0)
            winning_trades = stats.get('winning_trades', 0)
            win_rate = (winning_trades / max(total_trades, 1)) * 100
            
            message = f"""ğŸ“Š <b>Phoenix 95 Ultimate ì¼ì¼ ì„±ê³¼</b>

ğŸ’° ì´ P&L: <b>${stats.get('total_pnl', 0):.2f}</b>
ğŸ“ˆ ì´ ê±°ë˜: <b>{total_trades}íšŒ</b>
ğŸ¯ ìŠ¹ë¥ : <b>{win_rate:.1f}%</b>
ğŸ† ìµœê³  ê±°ë˜: <b>${stats.get('best_trade', 0):.2f}</b>
ğŸ’” ìµœì•… ê±°ë˜: <b>${stats.get('worst_trade', 0):.2f}</b>
â±ï¸ í‰ê·  ê±°ë˜ì‹œê°„: <b>{stats.get('avg_duration', 0)}ë¶„</b>

ğŸ“Š í™œì„± í¬ì§€ì…˜: <b>{stats.get('active_positions', 0)}ê°œ</b>
ğŸ’¸ ì´ ë§ˆì§„: <b>${stats.get('total_margin', 0):.2f}</b>

ğŸ• {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}"""
            
            await self.send_message(message, "INFO")
        except Exception as e:
            logging.error(f"ì¼ì¼ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ”’ ë³´ì•ˆ & ì¸ì¦, ğŸ“Š ëª¨ë‹ˆí„°ë§ (ê¸°ì¡´ê³¼ ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SecurityManager:
    """ë³´ì•ˆ ê´€ë¦¬ì (ê²€ì¦ ê°•í™”)"""
    
    def __init__(self):
        self.config = SystemConfig.SECURITY
        
        # ì„¤ì • ê²€ì¦
        if not self.config["webhook_secret"]:
            raise ValueError("WEBHOOK_SECRETì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        if not self.config["api_key"]:
            raise ValueError("API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        if not self.config["jwt_secret"]:
            raise ValueError("JWT_SECRETì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
    def verify_webhook_signature(self, payload: str, signature: str) -> bool:
        """ì›¹í›… ì„œëª… ê²€ì¦ (ì•ˆì „í•œ ê²€ì¦)"""
        try:
            expected_signature = hmac.new(
                self.config["webhook_secret"].encode(),
                payload.encode(),
                hashlib.sha256
            ).hexdigest()
            
            return hmac.compare_digest(signature, expected_signature)
        except Exception as e:
            logging.error(f"ì„œëª… ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def verify_api_key(self, api_key: str) -> bool:
        """API í‚¤ ê²€ì¦ (None ì²´í¬ í¬í•¨)"""
        if not api_key or not self.config["api_key"]:
            return False
        return hmac.compare_digest(api_key, self.config["api_key"])
    
    def is_ip_allowed(self, ip: str) -> bool:
        """IP í—ˆìš© ëª©ë¡ í™•ì¸"""
        if not ip:
            return False
        allowed_ips = self.config["allowed_ips"]
        return ip in allowed_ips or "0.0.0.0" in allowed_ips


class SystemMonitor:
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ (ì¤‘ë³µ ì œê±°, ë©”íŠ¸ë¦­ í†µí•©)"""
    
    def __init__(self, redis_client, postgres_pool, telegram_notifier):
        self.redis = redis_client
        self.postgres = postgres_pool
        self.telegram = telegram_notifier
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        max_history = SystemConfig.MONITORING["max_metrics_history"]
        self.metrics_history = deque(maxlen=max_history)
        
        # í†µí•© ë©”íŠ¸ë¦­ ìºì‹œ (ì¤‘ë³µ ì œê±°)
        self._metrics_cache: Dict[str, Any] = {}
        self._cache_timestamp = datetime.utcnow()
        self._cache_ttl = 30  # 30ì´ˆ ìºì‹œ
        
    async def collect_system_metrics(self) -> Optional[PerformanceMetrics]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (í†µí•© ìˆ˜ì§‘, ì¤‘ë³µ ì œê±°)"""
        try:
            # ìºì‹œ í™•ì¸
            now = datetime.utcnow()
            if (now - self._cache_timestamp).total_seconds() < self._cache_ttl:
                if 'system_metrics' in self._metrics_cache:
                    return self._metrics_cache['system_metrics']
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
            cpu_usage, memory_usage = await self._get_system_resources()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜
            active_connections = await self._get_db_connections()
            
            # íŠ¸ë ˆì´ë”© ë©”íŠ¸ë¦­ (í†µí•©)
            trading_metrics = await self._get_unified_trading_metrics()
            
            metrics = PerformanceMetrics(
                timestamp=now,
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                response_time_ms=0.0,  # APIì—ì„œ ì¸¡ì •
                active_connections=active_connections,
                active_positions=trading_metrics.get('active_positions', 0),
                total_pnl=trading_metrics.get('total_pnl', 0.0),
                win_rate=trading_metrics.get('win_rate', 0.0),
                avg_trade_duration=trading_metrics.get('avg_duration', 0.0),
                phoenix95_avg_score=trading_metrics.get('avg_score', 0.0),
                max_drawdown=trading_metrics.get('max_drawdown', 0.0),
                var_95=trading_metrics.get('var_95', 0.0),
                sharpe_ratio=trading_metrics.get('sharpe_ratio', 0.0)
            )
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self._metrics_cache['system_metrics'] = metrics
            self._cache_timestamp = now
            
            self.metrics_history.append(metrics)
            return metrics
            
        except Exception as e:
            logging.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    async def _get_system_resources(self) -> Tuple[float, float]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ (ì•ˆì „í•œ ì¡°íšŒ)"""
        try:
            import psutil
            cpu_usage = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            return cpu_usage, memory_usage
        except ImportError:
            logging.warning("psutil ëª¨ë“ˆì´ ì—†ì–´ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return 0.0, 0.0
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0.0, 0.0
    
    async def _get_db_connections(self) -> int:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ ì¡°íšŒ"""
        try:
            async with asyncio.wait_for(self.postgres.acquire(), timeout=5) as conn:
                result = await conn.fetchval(
                    "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
                )
                return int(result) if result else 0
        except Exception as e:
            logging.error(f"DB ì—°ê²° ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0
    
    async def _get_unified_trading_metrics(self) -> Dict[str, Union[int, float]]:
        """í†µí•© íŠ¸ë ˆì´ë”© ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)"""
        try:
            # ìºì‹œ í™•ì¸
            if 'trading_metrics' in self._metrics_cache:
                cache_age = (datetime.utcnow() - self._cache_timestamp).total_seconds()
                if cache_age < self._cache_ttl:
                    return self._metrics_cache['trading_metrics']
            
            async with asyncio.wait_for(self.postgres.acquire(), timeout=10) as conn:
                # í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¡œ í†µí•© ì¡°íšŒ (ì¤‘ë³µ ì œê±°)
                today_stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_trades,
                        COUNT(*) FILTER (WHERE pnl > 0) as winning_trades,
                        COALESCE(SUM(pnl), 0) as total_pnl,
                        COALESCE(AVG(duration_minutes), 0) as avg_duration,
                        COALESCE(MAX(pnl), 0) as best_trade,
                        COALESCE(MIN(pnl), 0) as worst_trade
                    FROM trade_history 
                    WHERE DATE(entry_time) = CURRENT_DATE
                """)
                
                # í™œì„± í¬ì§€ì…˜ ìˆ˜
                active_positions = await conn.fetchval(
                    "SELECT COUNT(*) FROM positions WHERE status = 'OPEN'"
                ) or 0
                
                win_rate = 0.0
                if today_stats and today_stats['total_trades'] > 0:
                    win_rate = (today_stats['winning_trades'] / today_stats['total_trades']) * 100
                
                metrics = {
                    'active_positions': int(active_positions),
                    'total_pnl': float(today_stats['total_pnl']) if today_stats else 0.0,
                    'win_rate': win_rate,
                    'avg_duration': float(today_stats['avg_duration']) if today_stats else 0.0,
                    'avg_score': 0.85,  # Phoenix 95 í‰ê·  ì ìˆ˜
                    'max_drawdown': 0.0,
                    'var_95': 0.0,
                    'sharpe_ratio': 0.0,
                    'best_trade': float(today_stats['best_trade']) if today_stats else 0.0,
                    'worst_trade': float(today_stats['worst_trade']) if today_stats else 0.0,
                    'total_trades': int(today_stats['total_trades']) if today_stats else 0,
                    'winning_trades': int(today_stats['winning_trades']) if today_stats else 0
                }
                
                # ìºì‹œ ì €ì¥
                self._metrics_cache['trading_metrics'] = metrics
                return metrics
                
        except Exception as e:
            logging.error(f"íŠ¸ë ˆì´ë”© ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    async def check_alerts(self, metrics: PerformanceMetrics):
        """ì•Œë¦¼ ì²´í¬ (ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€)"""
        try:
            alerts = []
            thresholds = SystemConfig.MONITORING["performance_threshold"]
            
            # CPU ì‚¬ìš©ë¥  ì²´í¬
            if metrics.cpu_usage > thresholds["cpu_usage_pct"]:
                alerts.append(f"ë†’ì€ CPU ì‚¬ìš©ë¥ : {metrics.cpu_usage:.1f}%")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
            if metrics.memory_usage > thresholds["memory_usage_pct"]:
                alerts.append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {metrics.memory_usage:.1f}%")
            
            # ì‘ë‹µ ì‹œê°„ ì²´í¬
            if metrics.response_time_ms > thresholds["response_time_ms"]:
                alerts.append(f"ëŠë¦° ì‘ë‹µ ì‹œê°„: {metrics.response_time_ms:.0f}ms")
            
            # ì•Œë¦¼ ì „ì†¡ (ì¿¨ë‹¤ìš´ ì ìš©)
            for alert in alerts:
                await self.telegram.send_system_notification(alert, "WARNING")
        except Exception as e:
            logging.error(f"ì•Œë¦¼ ì²´í¬ ì‹¤íŒ¨: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸš€ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (Enhanced)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Phoenix95UltimateSystem:
    """Phoenix 95 Ultimate í†µí•© ì‹œìŠ¤í…œ - Enhanced with Lambda Architecture"""
    
    def __init__(self):
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸
        self.telegram = UltimateTelegramNotifier()
        self.security = SecurityManager()
        self.redis_client = None
        self.postgres_pool = None
        
        # Enhanced ì»´í¬ë„ŒíŠ¸ë“¤
        self.lambda_architecture = None
        self.stream_processor = None
        self.ml_pipeline_manager = None
        self.phoenix95_engine = None
        self.trader = None
        self.monitor = None
        
        # FastAPI ì•±
        self.app = FastAPI(
            title="Phoenix 95 Ultimate Trading System - Enhanced",
            description="V3 ê²€ì¦ + V4 ê³ ê¸‰ + Lambda ì•„í‚¤í…ì²˜ + ML Pipeline + ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬",
            version="Ultimate-Enhanced-1.0.0"
        )
        
        self._setup_middleware()
        self._setup_routes()
        
    def _setup_middleware(self):
        """ë¯¸ë“¤ì›¨ì–´ ì„¤ì •"""
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    
    def _setup_routes(self):
        """API ë¼ìš°íŠ¸ ì„¤ì • (Enhanced)"""
        
        @self.app.on_event("startup")
        async def startup_event():
            await self.initialize()
        
        @self.app.on_event("shutdown")
        async def shutdown_event():
            await self.cleanup()
        
        @self.app.post("/webhook/tradingview")
        async def receive_tradingview_signal(signal_data: dict, background_tasks: BackgroundTasks):
            """TradingView ì‹ í˜¸ ìˆ˜ì‹  (ê²€ì¦ ê°•í™”)"""
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ["symbol", "action", "price", "confidence"]
                for field in required_fields:
                    if field not in signal_data:
                        raise HTTPException(status_code=400, detail=f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                
                # ë°ì´í„° íƒ€ì… ê²€ì¦
                try:
                    price = float(signal_data["price"])
                    confidence = float(signal_data["confidence"])
                except (ValueError, TypeError):
                    raise HTTPException(status_code=400, detail="priceì™€ confidenceëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤")
                
                # ì‹ í˜¸ íŒŒì‹± ë° ê²€ì¦
                signal = TradingSignal(
                    signal_id=f"SIG_{uuid.uuid4().hex[:8].upper()}",
                    symbol=signal_data["symbol"].upper(),
                    action=signal_data["action"].lower(),
                    price=price,
                    confidence=confidence,
                    timestamp=datetime.utcnow()
                )
                
                if not signal.is_valid():
                    raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì‹ í˜¸ ë°ì´í„°")
                
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
                background_tasks.add_task(self._process_signal_complete, signal)
                
                return {
                    "status": "received",
                    "signal_id": signal.signal_id,
                    "timestamp": signal.timestamp.isoformat()
                }
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"ì‹ í˜¸ ìˆ˜ì‹  ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_check():
            """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ (Enhanced)"""
            try:
                health_status = {
                    "status": "healthy",
                    "timestamp": datetime.utcnow().isoformat(),
                    "version": "Ultimate-Enhanced-1.0.0",
                    "system": "Phoenix 95 Ultimate Trading System - Enhanced",
                    "components": {},
                    "portfolio": {},
                    "config": {},
                    "lambda_architecture": {},
                    "ml_pipeline": {},
                    "stream_processing": {}
                }
                
                # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì²´í¬
                db_healthy = False
                try:
                    async with asyncio.wait_for(self.postgres_pool.acquire(), timeout=5) as conn:
                        await conn.fetchval("SELECT 1")
                    db_healthy = True
                except Exception as e:
                    logging.warning(f"DB í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
                
                redis_healthy = False
                try:
                    await asyncio.wait_for(self.redis_client.ping(), timeout=5)
                    redis_healthy = True
                except Exception as e:
                    logging.warning(f"Redis í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
                
                # í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½
                try:
                    portfolio = self.trader.get_portfolio_summary() if self.trader else {}
                    health_status["portfolio"] = portfolio
                except Exception as e:
                    logging.warning(f"í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # Lambda Architecture ìƒíƒœ
                try:
                    if self.lambda_architecture:
                        # ê°„ë‹¨í•œ ìƒíƒœ ì²´í¬
                        health_status["lambda_architecture"] = {
                            "batch_layer": "active",
                            "speed_layer": "active", 
                            "serving_layer": "active"
                        }
                except Exception as e:
                    logging.warning(f"Lambda ì•„í‚¤í…ì²˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
                # ML Pipeline ìƒíƒœ
                try:
                    if self.ml_pipeline_manager:
                        health_status["ml_pipeline"] = self.ml_pipeline_manager.get_model_status()
                except Exception as e:
                    logging.warning(f"ML Pipeline ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
                # Stream Processing ìƒíƒœ
                try:
                    if self.stream_processor:
                        health_status["stream_processing"] = {
                            "consumers": len(self.stream_processor.consumers),
                            "active": bool(self.stream_processor.consumers)
                        }
                except Exception as e:
                    logging.warning(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
                # ì»´í¬ë„ŒíŠ¸ ìƒíƒœ
                health_status["components"] = {
                    "database": "healthy" if db_healthy else "error",
                    "redis": "healthy" if redis_healthy else "error",
                    "ai_engine": "healthy",
                    "trading_engine": "healthy" if self.trader else "error",
                    "telegram": "healthy",
                    "lambda_architecture": "healthy" if self.lambda_architecture else "inactive",
                    "ml_pipeline": "healthy" if self.ml_pipeline_manager else "inactive",
                    "stream_processor": "healthy" if self.stream_processor else "inactive"
                }
                
                # ì„¤ì • ì •ë³´
                health_status["config"] = {
                    "max_leverage": SystemConfig.TRADING["max_leverage"],
                    "max_positions": SystemConfig.TRADING["max_positions"],
                    "confidence_threshold": SystemConfig.TRADING["confidence_threshold"]
                }
                
                # ì „ì²´ ìƒíƒœ ê²°ì •
                if not db_healthy or not redis_healthy:
                    health_status["status"] = "degraded"
                
                return health_status
                
            except Exception as e:
                logging.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/positions")
        async def get_active_positions():
            """í™œì„± í¬ì§€ì…˜ ì¡°íšŒ (ì•ˆì „í•œ ì¡°íšŒ)"""
            try:
                if not self.trader:
                    return {"positions": [], "count": 0, "summary": {}}
                
                positions = []
                for position in self.trader.active_positions.values():
                    try:
                        positions.append({
                            "position_id": position.position_id,
                            "symbol": position.symbol,
                            "side": position.side,
                            "entry_price": position.entry_price,
                            "current_price": position.current_price,
                            "quantity": position.quantity,
                            "leverage": position.leverage,
                            "margin_required": position.margin_required,
                            "unrealized_pnl": position.unrealized_pnl,
                            "pnl_percentage": position.pnl_percentage,
                            "liquidation_risk": position.liquidation_risk,
                            "liquidation_price": position.liquidation_price,
                            "stop_loss_price": position.stop_loss_price,
                            "take_profit_price": position.take_profit_price,
                            "status": position.status,
                            "created_at": position.created_at.isoformat()
                        })
                    except Exception as e:
                        logging.warning(f"í¬ì§€ì…˜ {position.position_id} ì§ë ¬í™” ì‹¤íŒ¨: {e}")
                
                portfolio_summary = self.trader.get_portfolio_summary()
                
                return {
                    "positions": positions,
                    "count": len(positions),
                    "summary": portfolio_summary
                }
                
            except Exception as e:
                logging.error(f"í¬ì§€ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/performance")
        async def get_performance_stats():
            """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ (Enhanced - Lambda Architecture í¬í•¨)"""
            try:
                stats = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "ai_engine": {},
                    "system_metrics": {},
                    "trading_stats": {},
                    "lambda_architecture": {},
                    "ml_pipeline": {},
                    "stream_processing": {}
                }
                
                # AI ì—”ì§„ ì„±ëŠ¥
                if self.phoenix95_engine:
                    stats["ai_engine"] = self.phoenix95_engine.get_performance_stats()
                
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
                if self.monitor and self.monitor.metrics_history:
                    latest_metrics = self.monitor.metrics_history[-1]
                    stats["system_metrics"] = asdict(latest_metrics)
                
                # ê±°ë˜ í†µê³„
                if self.monitor:
                    stats["trading_stats"] = await self.monitor._get_unified_trading_metrics()
                
                # Lambda Architecture í†µê³„
                if self.lambda_architecture:
                    try:
                        # ë°°ì¹˜ ë ˆì´ì–´ í†µê³„
                        batch_analytics = await self.redis_client.hgetall("batch:analytics")
                        stats["lambda_architecture"]["batch_layer"] = batch_analytics
                        
                        # ìŠ¤í”¼ë“œ ë ˆì´ì–´ í†µê³„
                        speed_buffer_size = len(self.lambda_architecture.speed_layer.window_buffer)
                        stats["lambda_architecture"]["speed_layer"] = {
                            "buffer_size": speed_buffer_size,
                            "window_size": SystemConfig.LAMBDA_ARCHITECTURE["speed_layer"]["window_size"]
                        }
                        
                        # ì„œë¹™ ë ˆì´ì–´ í†µê³„
                        materialized_view = await self.redis_client.get("serving:materialized_view")
                        if materialized_view:
                            view_data = json.loads(materialized_view)
                            stats["lambda_architecture"]["serving_layer"] = {
                                "last_materialization": view_data.get("timestamp"),
                                "combined_stats": view_data.get("combined_stats", {})
                            }
                    except Exception as e:
                        logging.warning(f"Lambda Architecture í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ML Pipeline í†µê³„
                if self.ml_pipeline_manager:
                    stats["ml_pipeline"] = self.ml_pipeline_manager.get_model_status()
                
                # Stream Processing í†µê³„
                if self.stream_processor:
                    stats["stream_processing"] = {
                        "active_consumers": len(self.stream_processor.consumers),
                        "redis_streams": SystemConfig.STREAM_PROCESSING["redis_streams"]["streams"]
                    }
                
                return stats
                
            except Exception as e:
                logging.error(f"ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/lambda-architecture/status")
        async def get_lambda_architecture_status():
            """Lambda Architecture ìƒì„¸ ìƒíƒœ ì¡°íšŒ (NEW)"""
            try:
                if not self.lambda_architecture:
                    raise HTTPException(status_code=404, detail="Lambda Architectureê°€ ë¹„í™œì„±í™”ë¨")
                
                status = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "batch_layer": {},
                    "speed_layer": {},
                    "serving_layer": {}
                }
                
                # ë°°ì¹˜ ë ˆì´ì–´ ìƒíƒœ
                try:
                    batch_analytics = await self.redis_client.hgetall("batch:analytics")
                    status["batch_layer"] = {
                        "last_processing": batch_analytics.get("timestamp"),
                        "total_trades": batch_analytics.get("total_trades"),
                        "total_pnl": batch_analytics.get("total_pnl"),
                        "win_rate": batch_analytics.get("win_rate"),
                        "config": SystemConfig.LAMBDA_ARCHITECTURE["batch_layer"]
                    }
                except Exception as e:
                    status["batch_layer"]["error"] = str(e)
                
                # ìŠ¤í”¼ë“œ ë ˆì´ì–´ ìƒíƒœ
                try:
                    buffer_size = len(self.lambda_architecture.speed_layer.window_buffer)
                    recent_signals = list(self.lambda_architecture.speed_layer.window_buffer)[-10:]
                    
                    status["speed_layer"] = {
                        "buffer_size": buffer_size,
                        "max_buffer_size": SystemConfig.LAMBDA_ARCHITECTURE["speed_layer"]["buffer_size"],
                        "recent_signals_count": len(recent_signals),
                        "window_size_seconds": SystemConfig.LAMBDA_ARCHITECTURE["speed_layer"]["window_size"]
                    }
                except Exception as e:
                    status["speed_layer"]["error"] = str(e)
                
                # ì„œë¹™ ë ˆì´ì–´ ìƒíƒœ
                try:
                    materialized_view = await self.redis_client.get("serving:materialized_view")
                    if materialized_view:
                        view_data = json.loads(materialized_view)
                        status["serving_layer"] = {
                            "last_materialization": view_data.get("timestamp"),
                            "has_batch_data": bool(view_data.get("batch_metrics")),
                            "has_stream_data": bool(view_data.get("real_time_metrics")),
                            "combined_stats": view_data.get("combined_stats", {}),
                            "cache_ttl": SystemConfig.LAMBDA_ARCHITECTURE["serving_layer"]["ttl"]
                        }
                    else:
                        status["serving_layer"]["status"] = "No materialized view available"
                except Exception as e:
                    status["serving_layer"]["error"] = str(e)
                
                return status
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"Lambda Architecture ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/ml-pipeline/models")
        async def get_ml_models_status():
            """ML Pipeline ëª¨ë¸ ìƒíƒœ ì¡°íšŒ (NEW)"""
            try:
                if not self.ml_pipeline_manager:
                    raise HTTPException(status_code=404, detail="ML Pipelineì´ ë¹„í™œì„±í™”ë¨")
                
                model_status = self.ml_pipeline_manager.get_model_status()
                
                # ì¶”ê°€ í†µê³„
                performance_history = list(self.ml_pipeline_manager.model_performance_history)[-50:]
                if performance_history:
                    model_status["recent_performance"] = {
                        "sample_size": len(performance_history),
                        "avg_error": float(np.mean([p["error"] for p in performance_history])),
                        "latest_predictions": performance_history[-5:] if len(performance_history) >= 5 else performance_history
                    }
                
                return model_status
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"ML ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/ml-pipeline/retrain")
        async def trigger_model_retrain():
            """ëª¨ë¸ ì¬í›ˆë ¨ íŠ¸ë¦¬ê±° (NEW)"""
            try:
                if not self.ml_pipeline_manager:
                    raise HTTPException(status_code=404, detail="ML Pipelineì´ ë¹„í™œì„±í™”ë¨")
                
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¬í›ˆë ¨ ì‹¤í–‰
                asyncio.create_task(self.ml_pipeline_manager._retrain_models())
                
                return {
                    "status": "triggered",
                    "message": "ëª¨ë¸ ì¬í›ˆë ¨ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë¨",
                    "timestamp": datetime.utcnow().isoformat()
                }
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"ëª¨ë¸ ì¬í›ˆë ¨ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/stream-processing/status")
        async def get_stream_processing_status():
            """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ (NEW)"""
            try:
                if not self.stream_processor:
                    raise HTTPException(status_code=404, detail="Stream Processorê°€ ë¹„í™œì„±í™”ë¨")
                
                status = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "consumers": {},
                    "streams": {},
                    "config": SystemConfig.STREAM_PROCESSING
                }
                
                # ì»¨ìŠˆë¨¸ ìƒíƒœ
                for name, task in self.stream_processor.consumers.items():
                    status["consumers"][name] = {
                        "active": not task.done(),
                        "done": task.done(),
                        "cancelled": task.cancelled()
                    }
                
                # ìŠ¤íŠ¸ë¦¼ ìƒíƒœ (Redis Streams)
                redis_config = SystemConfig.STREAM_PROCESSING["redis_streams"]
                for stream_name, stream_key in redis_config["streams"].items():
                    try:
                        # ìŠ¤íŠ¸ë¦¼ ê¸¸ì´ ì¡°íšŒ
                        stream_length = await self.redis_client.xlen(stream_key)
                        # ìµœê·¼ ë©”ì‹œì§€ ì¡°íšŒ
                        recent_messages = await self.redis_client.xrevrange(stream_key, count=5)
                        
                        status["streams"][stream_name] = {
                            "stream_key": stream_key,
                            "length": stream_length,
                            "recent_messages_count": len(recent_messages),
                            "last_message_time": recent_messages[0][0] if recent_messages else None
                        }
                    except Exception as e:
                        status["streams"][stream_name] = {"error": str(e)}
                
                return status
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/admin/emergency_close")
        async def emergency_close_position(position_id: str):
            """ê¸´ê¸‰ í¬ì§€ì…˜ ì²­ì‚° (ì•ˆì „í•œ ì²­ì‚°)"""
            try:
                if not self.trader or position_id not in self.trader.active_positions:
                    raise HTTPException(status_code=404, detail="í¬ì§€ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                position = self.trader.active_positions[position_id]
                current_price = await self.trader._get_current_price(position.symbol)
                
                await self.trader._close_position(position, current_price, "MANUAL")
                
                return {
                    "status": "success",
                    "message": f"í¬ì§€ì…˜ {position_id} ê¸´ê¸‰ ì²­ì‚° ì™„ë£Œ",
                    "timestamp": datetime.utcnow().isoformat()
                }
                
            except HTTPException:
                raise
            except Exception as e:
                logging.error(f"ê¸´ê¸‰ ì²­ì‚° ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=500, detail=str(e))
    
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Enhanced with new components)"""
        try:
            logging.info("Phoenix 95 Ultimate Enhanced ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # Redis ì—°ê²° (ì¬ì‹œë„ ë¡œì§)
            for attempt in range(3):
                try:
                    self.redis_client = await aioredis.from_url(
                        SystemConfig.DATABASE["redis_url"],
                        encoding="utf-8",
                        decode_responses=True,
                        socket_timeout=10,
                        socket_connect_timeout=10
                    )
                    await self.redis_client.ping()
                    logging.info("Redis ì—°ê²° ì„±ê³µ")
                    break
                except Exception as e:
                    logging.warning(f"Redis ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                    if attempt == 2:
                        raise
                    await asyncio.sleep(5)
            
            # PostgreSQL ì—°ê²° (ì¬ì‹œë„ ë¡œì§)
            for attempt in range(3):
                try:
                    self.postgres_pool = await asyncpg.create_pool(
                        SystemConfig.DATABASE["postgres_url"],
                        min_size=5,
                        max_size=SystemConfig.DATABASE["connection_pool_size"],
                        command_timeout=SystemConfig.DATABASE["query_timeout"]
                    )
                    # ì—°ê²° í…ŒìŠ¤íŠ¸
                    async with self.postgres_pool.acquire() as conn:
                        await conn.fetchval("SELECT 1")
                    logging.info("PostgreSQL ì—°ê²° ì„±ê³µ")
                    break
                except Exception as e:
                    logging.warning(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                    if attempt == 2:
                        raise
                    await asyncio.sleep(5)
            
            # Enhanced ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            
            # 1. Lambda Data Architecture ì´ˆê¸°í™”
            self.lambda_architecture = LambdaDataArchitecture(
                self.redis_client, 
                self.postgres_pool
            )
            await self.lambda_architecture.start_lambda_architecture()
            logging.info("Lambda Data Architecture ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 2. Stream Processor ì´ˆê¸°í™”
            self.stream_processor = StreamProcessor(self.redis_client)
            await self.stream_processor.start_stream_processing()
            logging.info("Stream Processor ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 3. ML Pipeline Manager ì´ˆê¸°í™”
            self.ml_pipeline_manager = MLPipelineManager(self.postgres_pool)
            await self.ml_pipeline_manager.start_ml_pipeline()
            logging.info("ML Pipeline Manager ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 4. Phoenix95 AI Engine ì´ˆê¸°í™” (Enhanced with ML Pipeline)
            self.phoenix95_engine = Phoenix95AIEngine(self.ml_pipeline_manager)
            logging.info("Phoenix95 AI Engine (Enhanced) ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 5. Leverage Trader ì´ˆê¸°í™” (Enhanced with Lambda Architecture)
            self.trader = UltimateLeverageTrader(
                self.redis_client, 
                self.postgres_pool, 
                self.telegram,
                self.lambda_architecture
            )
            logging.info("Ultimate Leverage Trader (Enhanced) ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 6. System Monitor ì´ˆê¸°í™”
            self.monitor = SystemMonitor(
                self.redis_client,
                self.postgres_pool,
                self.telegram
            )
            logging.info("System Monitor ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
            asyncio.create_task(self._monitoring_loop())
            
            # ì‹œì‘ ì•Œë¦¼ (Enhanced)
            await self.telegram.send_system_notification(
                """ğŸ¯ <b>Phoenix 95 Ultimate Enhanced ì‹œìŠ¤í…œ ì‹œì‘</b>

âœ… V3 ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜ í™œì„±í™”
âœ… V4 ê³ ê¸‰ ê¸°ëŠ¥ í™œì„±í™”  
âœ… í—¤ì§€í€ë“œê¸‰ ë¦¬ìŠ¤í¬ ê´€ë¦¬ í™œì„±í™”

ğŸ†• <b>NEW FEATURES:</b>
ğŸŒŠ Lambda Data Architecture (ë°°ì¹˜/ìŠ¤í”¼ë“œ/ì„œë¹™ ë ˆì´ì–´)
ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
ğŸ¤– ML Pipeline ìë™í™” (ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬)
ğŸ“Š í†µí•© ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„

ğŸ’° ìµœëŒ€ í¬ì§€ì…˜: {SystemConfig.TRADING["max_positions"]}ê°œ
ğŸ›¡ï¸ ì¼ì¼ ì†ì‹¤ í•œë„: ${SystemConfig.TRADING["max_daily_loss"]}
âš¡ ë¶„ì„ ì†ë„: 2ì´ˆ ì´ë‚´ ë³´ì¥ (ML ë¶€ìŠ¤íŠ¸ í¬í•¨)
ğŸ”’ í™˜ê²½ë³€ìˆ˜ ê²€ì¦ ê°•í™”
ğŸš« ë¬´í•œë£¨í”„ ë° ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€

ğŸš€ Enhanced ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!""", 
                "SUCCESS"
            )
            
            logging.info("Phoenix 95 Ultimate Enhanced ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            try:
                await self.telegram.send_system_notification(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "ERROR")
            except:
                pass  # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨ ì‹œì—ë„ ì‹œìŠ¤í…œì€ ì¢…ë£Œë˜ì–´ì•¼ í•¨
            raise
    
    async def cleanup(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬ (Enhanced cleanup)"""
        try:
            logging.info("Enhanced ì‹œìŠ¤í…œ ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë“  ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì·¨ì†Œ
            if self.trader:
                for task in list(self.trader.monitoring_tasks.values()):
                    if not task.done():
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            pass
            
            # Stream Processor ì •ë¦¬
            if self.stream_processor:
                for name, task in self.stream_processor.consumers.items():
                    if not task.done():
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            pass
                logging.info("Stream Processor ì •ë¦¬ ì™„ë£Œ")
            
            # ì—°ê²° ì •ë¦¬
            if self.redis_client:
                try:
                    await self.redis_client.close()
                except Exception as e:
                    logging.warning(f"Redis ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            if self.postgres_pool:
                try:
                    await self.postgres_pool.close()
                except Exception as e:
                    logging.warning(f"PostgreSQL ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            try:
                await self.telegram.send_system_notification("Enhanced ì‹œìŠ¤í…œ ì •ìƒ ì¢…ë£Œ", "INFO")
            except Exception as e:
                logging.warning(f"ì¢…ë£Œ ì•Œë¦¼ ì‹¤íŒ¨: {e}")
            
            logging.info("Enhanced ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _process_signal_complete(self, signal: TradingSignal):
        """ì™„ì „í•œ ì‹ í˜¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Enhanced with Lambda Architecture)"""
        try:
            logging.info(f"Enhanced ì‹ í˜¸ ì²˜ë¦¬ ì‹œì‘: {signal.signal_id}")
            
            # 1. Phoenix 95 AI ë¶„ì„ (Enhanced with ML Pipeline)
            try:
                phoenix95_score, kelly_ratio, recommendation = await asyncio.wait_for(
                    self.phoenix95_engine.analyze_signal_complete(signal),
                    timeout=SystemConfig.PHOENIX95["analysis_timeout"]
                )
            except asyncio.TimeoutError:
                logging.warning(f"AI ë¶„ì„ íƒ€ì„ì•„ì›ƒ: {signal.signal_id}")
                phoenix95_score, kelly_ratio, recommendation = 0.0, 0.0, "REJECT"
            
            signal.phoenix95_score = phoenix95_score
            signal.kelly_ratio = kelly_ratio
            signal.recommendation = recommendation
            signal.processed = True
            
            logging.info(f"Enhanced AI ë¶„ì„ ì™„ë£Œ: Phoenix95={phoenix95_score:.3f}, Kelly={kelly_ratio:.3f}, Rec={recommendation}")
            
            # 2. Lambda Architectureì— ì‹ í˜¸ ë°ì´í„° ì „ì†¡
            if self.lambda_architecture:
                try:
                    await self.lambda_architecture.speed_layer.process_real_time_signal(signal)
                except Exception as e:
                    logging.warning(f"Lambda Architecture ì‹ í˜¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 3. ì¶”ì²œì´ ê±°ë˜ ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
            if recommendation in ["STRONG_BUY", "BUY", "WEAK_BUY"]:
                # 4. ê±°ë˜ ì‹¤í–‰ (Enhanced)
                position = await self.trader.execute_trade_complete(signal, phoenix95_score, kelly_ratio)
                
                if position:
                    logging.info(f"Enhanced ê±°ë˜ ì‹¤í–‰ ì„±ê³µ: {position.position_id}")
                    
                    # ML Pipelineì— ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡ (í–¥í›„ ì„±ëŠ¥ ì¶”ì ìš©)
                    if self.ml_pipeline_manager:
                        try:
                            # ì‹¤ì œ ê²°ê³¼ëŠ” í¬ì§€ì…˜ ì²­ì‚° ì‹œ ê¸°ë¡ë¨
                            await self.ml_pipeline_manager.record_prediction_result(
                                signal.signal_id, phoenix95_score, None
                            )
                        except Exception as e:
                            logging.warning(f"ML ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
                else:
                    logging.warning(f"Enhanced ê±°ë˜ ì‹¤í–‰ ì‹¤íŒ¨: {signal.signal_id}")
                    await self.telegram.send_system_notification(
                        f"ê±°ë˜ ì‹¤í–‰ ì‹¤íŒ¨\nì‹ í˜¸: {signal.symbol} {signal.action}", 
                        "WARNING"
                    )
            else:
                logging.info(f"ê±°ë˜ ê±°ë¶€: {recommendation}")
                await self.telegram.send_system_notification(
                    f"""ğŸ“Š Enhanced ì‹ í˜¸ ë¶„ì„ ì™„ë£Œ (ê±°ë˜ ê±°ë¶€)

ğŸ” {signal.symbol} {signal.action.upper()}
ğŸ’° ê°€ê²©: ${signal.price:,.2f}
ğŸ§  Phoenix95: {phoenix95_score:.1%}
âš–ï¸ Kelly: {kelly_ratio:.1%}
âŒ ì¶”ì²œ: {recommendation}

ğŸ’¡ ê±°ë˜ ì¡°ê±´ ë¯¸ì¶©ì¡± (Enhanced ë¶„ì„ ê²°ê³¼)""", 
                    "INFO"
                )
            
        except Exception as e:
            logging.error(f"Enhanced ì‹ í˜¸ ì²˜ë¦¬ ì‹¤íŒ¨ {signal.signal_id}: {e}")
            signal.processed = False
            signal.error_message = str(e)
            
            try:
                await self.telegram.send_system_notification(
                    f"Enhanced ì‹ í˜¸ ì²˜ë¦¬ ì˜¤ë¥˜\n{signal.symbol}: {str(e)}", 
                    "ERROR"
                )
            except Exception as telegram_error:
                logging.error(f"í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨: {telegram_error}")
    
    async def _monitoring_loop(self):
        """Enhanced ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        consecutive_errors = 0
        max_consecutive_errors = 5
        
        while consecutive_errors < max_consecutive_errors:
            try:
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = await self.monitor.collect_system_metrics()
                
                if metrics:
                    # ì•Œë¦¼ ì²´í¬
                    await self.monitor.check_alerts(metrics)
                    
                    # Redisì— ìµœì‹  ë©”íŠ¸ë¦­ ì €ì¥
                    try:
                        await self.redis_client.hset("system:metrics", mapping={
                            "cpu_usage": str(metrics.cpu_usage),
                            "memory_usage": str(metrics.memory_usage),
                            "active_positions": str(metrics.active_positions),
                            "total_pnl": str(metrics.total_pnl),
                            "timestamp": metrics.timestamp.isoformat()
                        })
                    except Exception as redis_error:
                        logging.warning(f"Redis ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {redis_error}")
                
                # Enhanced ì»´í¬ë„ŒíŠ¸ í—¬ìŠ¤ì²´í¬
                await self._enhanced_health_check()
                
                # ì„±ê³µì ì¸ ë°˜ë³µ í›„ ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹
                consecutive_errors = 0
                
                # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰
                await asyncio.sleep(SystemConfig.MONITORING["metrics_interval"])
                
            except Exception as e:
                consecutive_errors += 1
                logging.error(f"Enhanced ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜ ({consecutive_errors}/{max_consecutive_errors}): {e}")
                
                if consecutive_errors >= max_consecutive_errors:
                    logging.critical("Enhanced ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì¤‘ë‹¨ (ìµœëŒ€ ì˜¤ë¥˜ íšŸìˆ˜ ì´ˆê³¼)")
                    try:
                        await self.telegram.send_system_notification(
                            "Enhanced ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¤‘ë‹¨", "CRITICAL"
                        )
                    except:
                        pass
                    break
                
                # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                await asyncio.sleep(min(60 * consecutive_errors, 300))  # ìµœëŒ€ 5ë¶„
    
    async def _enhanced_health_check(self):
        """Enhanced ì»´í¬ë„ŒíŠ¸ í—¬ìŠ¤ì²´í¬"""
        try:
            # Lambda Architecture í—¬ìŠ¤ì²´í¬
            if self.lambda_architecture:
                # ê°„ë‹¨í•œ ìƒíƒœ ì²´í¬ (ì‹¤ì œë¡œëŠ” ë” ìƒì„¸í•œ ì²´í¬ í•„ìš”)
                buffer_size = len(self.lambda_architecture.speed_layer.window_buffer)
                if buffer_size > SystemConfig.LAMBDA_ARCHITECTURE["speed_layer"]["buffer_size"] * 0.9:
                    logging.warning(f"Speed Layer ë²„í¼ ê±°ì˜ ê°€ë“ì°¸: {buffer_size}")
            
            # ML Pipeline í—¬ìŠ¤ì²´í¬
            if self.ml_pipeline_manager:
                model_count = len(self.ml_pipeline_manager.model_cache)
                if model_count == 0:
                    logging.warning("ML Pipeline ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            # Stream Processor í—¬ìŠ¤ì²´í¬
            if self.stream_processor:
                active_consumers = sum(1 for task in self.stream_processor.consumers.values() if not task.done())
                total_consumers = len(self.stream_processor.consumers)
                if active_consumers < total_consumers:
                    logging.warning(f"ì¼ë¶€ ìŠ¤íŠ¸ë¦¼ ì»¨ìŠˆë¨¸ ë¹„í™œì„±í™”: {active_consumers}/{total_consumers}")
            
        except Exception as e:
            logging.error(f"Enhanced í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸš€ ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Enhanced)"""
    try:
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('phoenix95_ultimate_enhanced.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        # Enhanced ì‹œìŠ¤í…œ ì‹œì‘ ë©”ì‹œì§€
        print("=" * 80)
        print("ğŸ¯ Phoenix 95 Ultimate Trading System - ENHANCED VERSION")
        print("ğŸ’° V3 ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜ + V4 ê³ ê¸‰ ê¸°ëŠ¥ + í—¤ì§€í€ë“œê¸‰ ì•ˆì „ì„±")
        print("âš¡ Wall Streetê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ")
        print()
        print("ğŸ†• NEW ENHANCED FEATURES:")
        print("ğŸŒŠ Lambda Data Architecture (ë°°ì¹˜/ìŠ¤í”¼ë“œ/ì„œë¹™ ë ˆì´ì–´)")
        print("ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸") 
        print("ğŸ¤– ML Pipeline ìë™í™” (ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬)")
        print("ğŸ“Š í†µí•© ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„")
        print("ğŸ”§ ë³´ì•ˆ ê°•í™”, ë¡œì§ ìˆ˜ì •, ì¤‘ë³µ ì œê±°, ë¬´í•œë£¨í”„ ë°©ì§€")
        print("=" * 80)
        
        # Phoenix 95 Enhanced ì‹œìŠ¤í…œ ìƒì„±
        phoenix95_system = Phoenix95UltimateSystem()
        
        # FastAPI ì„œë²„ ì‹¤í–‰
        config = uvicorn.Config(
            phoenix95_system.app,
            host="0.0.0.0",
            port=int(os.getenv("PORT", "8080")),
            log_level="info",
            access_log=True
        )
        
        server = uvicorn.Server(config)
        await server.serve()
        
    except EnvironmentError as e:
        print(f"\nâŒ í™˜ê²½ì„¤ì • ì˜¤ë¥˜:\n{e}")
        print("\nğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:")
        print("TELEGRAM_BOT_TOKEN=your_bot_token")
        print("TELEGRAM_CHAT_ID=your_chat_id")
        print("WEBHOOK_SECRET=your_secret")
        print("API_KEY=your_api_key")
        print("JWT_SECRET=your_jwt_secret")
        print("# Enhanced ê¸°ëŠ¥ìš© ì˜µì…˜:")
        print("KAFKA_SERVERS=localhost:9092")
        print("MLFLOW_TRACKING_URI=http://localhost:5000")
        print("ACCOUNT_BALANCE=50000")
        sys.exit(1)
    except ValueError as e:
        print(f"\nâŒ ì„¤ì •ê°’ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Phoenix 95 Ultimate Enhanced ì‹œìŠ¤í…œ ì¢…ë£Œ")
    except Exception as e:
        print(f"\nâŒ Enhanced ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              ğŸ“‹ Enhanced ë²„ì „ ì™„ë£Œ ë³´ê³ ì„œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
ğŸ”§ Enhanced ë²„ì „ ì£¼ìš” ì¶”ê°€ ì‚¬í•­:

1. ğŸŒŠ Lambda ë°ì´í„° ì•„í‚¤í…ì²˜:
   - BatchDataProcessor: ëŒ€ìš©ëŸ‰ ê³¼ê±° ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬
   - StreamDataProcessor: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬
   - ViewMaterializer: ë°°ì¹˜+ìŠ¤íŠ¸ë¦¼ ë°ì´í„° í†µí•© ë·° êµ¬ì²´í™”
   - 3ê³„ì¸µ ë¶„ë¦¬ë¡œ í™•ì¥ì„±ê³¼ ë‚´ê²°í•¨ì„± ë³´ì¥

2. ğŸš€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
   - Redis Streams ê¸°ë°˜ ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì²˜ë¦¬
   - ìë™ ì»¨ìŠˆë¨¸ ê·¸ë£¹ ê´€ë¦¬ ë° ë©”ì‹œì§€ ACK
   - ì‹œì¥ ë°ì´í„°, ì‹ í˜¸, í¬ì§€ì…˜ ë³„ ìŠ¤íŠ¸ë¦¼ ë¶„ë¦¬
   - ë°±í”„ë ˆì…” ë° ì˜¤ë¥˜ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

3. ğŸ¤– ML Pipeline ìë™í™”:
   - ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ë¡œë”©, ì˜ˆì¸¡, ì¬í›ˆë ¨)
   - ì•™ìƒë¸” ì˜ˆì¸¡ ë° ê°€ì¤‘ í‰ê·  ê²°í•©
   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì¬í›ˆë ¨ íŠ¸ë¦¬ê±°
   - ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì  ë° ì„±ëŠ¥ í‰ê°€

4. ğŸ“Š Enhanced API ì—”ë“œí¬ì¸íŠ¸:
   - /lambda-architecture/status: Lambda ì•„í‚¤í…ì²˜ ìƒì„¸ ìƒíƒœ
   - /ml-pipeline/models: ML ëª¨ë¸ ìƒíƒœ ë° ì„±ëŠ¥
   - /ml-pipeline/retrain: ìˆ˜ë™ ì¬í›ˆë ¨ íŠ¸ë¦¬ê±°
   - /stream-processing/status: ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ìƒíƒœ

5. ğŸ”§ ê°œì„ ëœ ëª¨ë‹ˆí„°ë§:
   - Lambda ì•„í‚¤í…ì²˜ ì»´í¬ë„ŒíŠ¸ í—¬ìŠ¤ì²´í¬
   - ML ëª¨ë¸ ì„±ëŠ¥ ì‹¤ì‹œê°„ ì¶”ì 
   - ìŠ¤íŠ¸ë¦¼ ì»¨ìŠˆë¨¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§
   - í†µí•© ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ

6. âš¡ ì„±ëŠ¥ ìµœì í™”:
   - ML ë¶€ìŠ¤íŠ¸ë¥¼ í†µí•œ ì‹ í˜¸ ë¶„ì„ ì •í™•ë„ í–¥ìƒ
   - ìºì‹œ ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ ì¬ì‚¬ìš©
   - ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¡œ ì§€ì—° ì‹œê°„ ìµœì†Œí™”
   - ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ë¶„ì„

ì›ë³¸ ëŒ€ë¹„ ëˆ„ë½ë¥ : 0% (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ìƒˆ ê¸°ëŠ¥ ì¶”ê°€)

ğŸ¯ ì´ì œ Phoenix 95ëŠ” ì§„ì •í•œ Enterpriseê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œì…ë‹ˆë‹¤!
   - ì‹¤ì‹œê°„ ë¹…ë°ì´í„° ì²˜ë¦¬ (Lambda Architecture)
   - ìë™í™”ëœ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸
   - í™•ì¥ ê°€ëŠ¥í•œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
   - í”„ë¡œë•ì…˜ ë ˆë”” ëª¨ë‹ˆí„°ë§
"""