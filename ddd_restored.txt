#!/bin/bash
# Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ë° ì„¤ì • íŒŒì¼ ëª¨ìŒ

# =============================================================================
# 1. ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (run_phoenix95_recovery.sh)
# =============================================================================

cat > run_phoenix95_recovery.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ë° ìµœì í™” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

set -e
echo "ğŸš€ Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ì‹œìŠ¤í…œ ì‹œì‘"
echo "=================================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# í•¨ìˆ˜ ì •ì˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# í™˜ê²½ ì²´í¬
check_requirements() {
    log_info "í™˜ê²½ ìš”êµ¬ì‚¬í•­ ì²´í¬ ì¤‘..."
    
    # Python ì²´í¬
    if ! command -v python3 &> /dev/null; then
        log_error "Python 3ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
        exit 1
    fi
    
    # Docker ì²´í¬ (ì„ íƒì‚¬í•­)
    if command -v docker &> /dev/null; then
        log_success "Docker ë°œê²¬ë¨"
    else
        log_warning "Dockerê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)"
    fi
    
    # í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
    log_info "Python ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
    pip3 install --quiet aiohttp aiofiles || {
        log_warning "ì¼ë¶€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤"
    }
    
    log_success "í™˜ê²½ ìš”êµ¬ì‚¬í•­ ì²´í¬ ì™„ë£Œ"
}

# ë°±ì—… ìƒì„±
create_backup() {
    log_info "í˜„ì¬ ìƒíƒœ ë°±ì—… ìƒì„± ì¤‘..."
    
    BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$BACKUP_DIR"
    
    # ì¤‘ìš” íŒŒì¼ë“¤ ë°±ì—…
    if [ -f "docker-compose.yml" ]; then
        cp docker-compose.yml "$BACKUP_DIR/"
    fi
    
    if [ -f "requirements.txt" ]; then
        cp requirements.txt "$BACKUP_DIR/"
    fi
    
    if [ -d "services" ]; then
        cp -r services "$BACKUP_DIR/" 2>/dev/null || true
    fi
    
    log_success "ë°±ì—… ìƒì„± ì™„ë£Œ: $BACKUP_DIR"
}

# ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰
run_recovery_system() {
    log_info "Phoenix 95 V4 ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘..."
    
    # ë©”ì¸ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    python3 -c "
import asyncio
import sys
import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰
sys.path.append(os.getcwd())

# ë³µêµ¬ ì‹œìŠ¤í…œ ì½”ë“œ ì‹¤í–‰ (ì´ë¯¸ ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ ì‚¬ìš©)
async def main():
    from phoenix95_complete_recovery_system import Phoenix95CompleteRecoverySystem
    
    recovery_system = Phoenix95CompleteRecoverySystem('.')
    results = await recovery_system.run_complete_recovery()
    
    print('\\n' + '='*80)
    print('ğŸ“Š ë³µêµ¬ ê²°ê³¼ ìš”ì•½')
    print('='*80)
    
    if 'error' in results:
        print(f'âŒ ì˜¤ë¥˜: {results[\"error\"]}')
        return False
    
    print(f'â±ï¸ ì‹¤í–‰ ì‹œê°„: {results.get(\"execution_time\", 0):.2f}ì´ˆ')
    print(f'ğŸ“ ë¶„ì„ëœ íŒŒì¼ ìˆ˜: {results.get(\"structure_analysis\", {}).get(\"total_files\", 0)}')
    print(f'ğŸ”§ ìˆ˜ì •ëœ ì´ìŠˆ ìˆ˜: {len(results.get(\"quality_improvement\", {}).get(\"issues_fixed\", []))}')
    print(f'âš¡ ìµœì í™” ì ìš© ìˆ˜: {len(results.get(\"performance_optimization\", {}).get(\"optimizations_applied\", []))}')
    
    return True

if __name__ == '__main__':
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
"
    
    if [ $? -eq 0 ]; then
        log_success "ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ"
    else
        log_error "ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨"
        return 1
    fi
}

# ì„œë¹„ìŠ¤ êµ¬ì¡° ìƒì„±
create_service_structure() {
    log_info "V4 ì„œë¹„ìŠ¤ êµ¬ì¡° ìƒì„± ì¤‘..."
    
    # V4 í•„ìˆ˜ ì„œë¹„ìŠ¤ë“¤
    services=(
        "api-gateway-enterprise:8100"
        "signal-ingestion-pro:8101"
        "market-data-intelligence:8102"
        "phoenix95-ai-engine:8103"
        "trade-execution-leverage:8106"
        "position-tracker-realtime:8107"
        "notification-hub-intelligent:8109"
    )
    
    for service_info in "${services[@]}"; do
        IFS=':' read -r service_name port <<< "$service_info"
        
        log_info "ì„œë¹„ìŠ¤ ìƒì„± ì¤‘: $service_name (í¬íŠ¸: $port)"
        
        # ì„œë¹„ìŠ¤ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        mkdir -p "services/$service_name"/{domain,application,infrastructure,interfaces/api}
        
        # ê¸°ë³¸ __init__.py íŒŒì¼ë“¤ ìƒì„±
        for layer in domain application infrastructure interfaces; do
            echo "\"\"\"Phoenix 95 V4 $service_name $layer layer\"\"\"" > "services/$service_name/$layer/__init__.py"
        done
        
        # FastAPI ë©”ì¸ ì•± ìƒì„±
        cat > "services/$service_name/interfaces/api/main.py" << PYTHON_EOF
"""
Phoenix 95 V4 $service_name FastAPI Application
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="${service_name//-/ }",
    description="Phoenix 95 V4 Enhanced $service_name",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "service": "$service_name",
        "port": $port,
        "version": "4.0.0"
    }

@app.get("/ready")
async def readiness_check():
    """ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "ready",
        "service": "$service_name",
        "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
    }

@app.get("/metrics")
async def metrics():
    """í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­"""
    return {"metrics": "# Prometheus metrics here"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=$port)
PYTHON_EOF

        # Dockerfile ìƒì„±
        cat > "services/$service_name/Dockerfile" << DOCKER_EOF
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y gcc && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ë³µì‚¬ ë° ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE $port

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:$port/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["python", "-m", "interfaces.api.main"]
DOCKER_EOF

        # requirements.txt ìƒì„±
        cat > "services/$service_name/requirements.txt" << REQ_EOF
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
asyncpg==0.29.0
aioredis==2.0.1
influxdb-client==1.40.0
prometheus-client==0.19.0
structlog==23.2.0
aiohttp==3.9.0
REQ_EOF

    done
    
    log_success "V4 ì„œë¹„ìŠ¤ êµ¬ì¡° ìƒì„± ì™„ë£Œ"
}

# ê³µí†µ ì„¤ì • íŒŒì¼ ìƒì„±
create_config_files() {
    log_info "ê³µí†µ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘..."
    
    # ê³µí†µ ë””ë ‰í† ë¦¬ êµ¬ì¡°
    mkdir -p shared/{config,domain,infrastructure,utils}
    mkdir -p infrastructure/{docker,kubernetes,monitoring}
    mkdir -p scripts/{deployment,migration,testing}
    mkdir -p tests/{unit,integration,performance}
    mkdir -p docs
    
    # Docker Compose íŒŒì¼ ìƒì„±
    cat > docker-compose.yml << 'COMPOSE_EOF'
version: '3.8'

services:
  # ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë“¤
  postgresql:
    image: postgres:15
    environment:
      POSTGRES_DB: phoenix95_v4
      POSTGRES_USER: phoenix95
      POSTGRES_PASSWORD: phoenix95_secure
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  influxdb:
    image: influxdb:2.7
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: adminpassword
    ports:
      - "8086:8086"
    volumes:
      - influx_data:/var/lib/influxdb2
    restart: unless-stopped

  # Phoenix 95 V4 ì„œë¹„ìŠ¤ë“¤
  api-gateway-enterprise:
    build: ./services/api-gateway-enterprise
    ports:
      - "8100:8100"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  signal-ingestion-pro:
    build: ./services/signal-ingestion-pro
    ports:
      - "8101:8101"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  market-data-intelligence:
    build: ./services/market-data-intelligence
    ports:
      - "8102:8102"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
      - INFLUXDB_URL=http://influxdb:8086
    depends_on:
      - postgresql
      - redis
      - influxdb
    restart: unless-stopped

  phoenix95-ai-engine:
    build: ./services/phoenix95-ai-engine
    ports:
      - "8103:8103"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  trade-execution-leverage:
    build: ./services/trade-execution-leverage
    ports:
      - "8106:8106"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  position-tracker-realtime:
    build: ./services/position-tracker-realtime
    ports:
      - "8107:8107"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  notification-hub-intelligent:
    build: ./services/notification-hub-intelligent
    ports:
      - "8109:8109"
    environment:
      - DATABASE_URL=postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgresql
      - redis
    restart: unless-stopped

  # ëª¨ë‹ˆí„°ë§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  influx_data:
  grafana_data:

networks:
  default:
    name: phoenix95_v4_network
COMPOSE_EOF

    # ê³µí†µ ì„¤ì • íŒŒì¼ë“¤ ìƒì„±
    cat > shared/config/telegram_config.py << 'TELEGRAM_EOF'
"""
Phoenix 95 V4 Enhanced Telegram Configuration
"""

TELEGRAM_CONFIG = {
    "bot_token": "7386542811:AAEZ21p30rES1k8NxNM2xbZ53U44PI9D5CY",
    "chat_id": "7590895952",
    "alerts": {
        "trade_execution": True,
        "position_updates": True,
        "system_errors": True,
        "performance_reports": True
    },
    "notification_levels": {
        "INFO": True,
        "WARNING": True,
        "ERROR": True,
        "CRITICAL": True
    }
}

async def send_telegram_message(message: str, level: str = "INFO"):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    import aiohttp
    
    if not TELEGRAM_CONFIG["notification_levels"].get(level, False):
        return
    
    url = f"https://api.telegram.org/bot{TELEGRAM_CONFIG['bot_token']}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CONFIG["chat_id"],
        "text": f"[{level}] {message}",
        "parse_mode": "HTML"
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            await session.post(url, data=data)
    except Exception as e:
        print(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")
TELEGRAM_EOF

    cat > shared/config/trading_config.py << 'TRADING_EOF'
"""
Phoenix 95 V4 Enhanced Trading Configuration
"""

TRADING_CONFIG = {
    "leverage": {
        "max_leverage": 20,
        "margin_mode": "ISOLATED",
        "position_side": "BOTH"
    },
    "risk_management": {
        "max_position_size_usd": 50000,
        "max_daily_loss_usd": 5000,
        "stop_loss_percentage": 0.02,
        "take_profit_percentage": 0.04
    },
    "phoenix95": {
        "confidence_threshold": 0.85,
        "min_kelly_ratio": 0.1,
        "max_kelly_ratio": 0.25
    },
    "allowed_symbols": [
        "BTCUSDT", "ETHUSDT", "ADAUSDT", "DOTUSDT", "LINKUSDT",
        "LTCUSDT", "XRPUSDT", "EOSUSDT", "TRXUSDT", "ETCUSDT"
    ]
}

SIGNAL_VALIDATION = {
    "required_fields": ["symbol", "action", "price", "confidence"],
    "confidence_min": 0.7,
    "price_deviation_max": 0.05,
    "duplicate_timeout_seconds": 300
}
TRADING_EOF

    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    mkdir -p infrastructure/monitoring
    cat > infrastructure/monitoring/prometheus.yml << 'PROMETHEUS_EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'phoenix95-v4-services'
    static_configs:
      - targets:
          - 'localhost:8100'  # api-gateway-enterprise
          - 'localhost:8101'  # signal-ingestion-pro
          - 'localhost:8102'  # market-data-intelligence
          - 'localhost:8103'  # phoenix95-ai-engine
          - 'localhost:8106'  # trade-execution-leverage
          - 'localhost:8107'  # position-tracker-realtime
          - 'localhost:8109'  # notification-hub-intelligent

  - job_name: 'databases'
    static_configs:
      - targets:
          - 'localhost:5432'  # PostgreSQL
          - 'localhost:6379'  # Redis
          - 'localhost:8086'  # InfluxDB

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
PROMETHEUS_EOF

    # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ
    cat > .env.example << 'ENV_EOF'
# Phoenix 95 V4 Environment Variables
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=phoenix95_v4
POSTGRES_USER=phoenix95
POSTGRES_PASSWORD=phoenix95_secure

REDIS_HOST=localhost
REDIS_PORT=6379

INFLUXDB_URL=http://localhost:8086
INFLUXDB_TOKEN=your_influxdb_token
INFLUXDB_ORG=phoenix95
INFLUXDB_BUCKET=metrics

TELEGRAM_BOT_TOKEN=7386542811:AAEZ21p30rES1k8NxNM2xbZ53U44PI9D5CY
TELEGRAM_CHAT_ID=7590895952

MAX_LEVERAGE=20
MARGIN_MODE=ISOLATED

LOG_LEVEL=INFO
ENV_EOF

    # ê¸°ë³¸ README ìƒì„±
    cat > README.md << 'README_EOF'
# Phoenix 95 V4 Enhanced

ì™„ì „ ìë™í™”ëœ Enterpriseê¸‰ ê±°ë˜ ì‹œìŠ¤í…œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

```bash
# ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰
./run_phoenix95_recovery.sh

# ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8100/health
```

## ğŸ“Š ì„œë¹„ìŠ¤ êµ¬ì¡°

- **API Gateway** (8100): ë¼ìš°íŒ… ë° ì¸ì¦
- **Signal Ingestion** (8101): ì‹ í˜¸ ìˆ˜ì§‘
- **Market Data Intelligence** (8102): ì‹œì¥ ë°ì´í„° ë¶„ì„
- **Phoenix 95 AI Engine** (8103): AI ê¸°ë°˜ ì‹ í˜¸ ë¶„ì„
- **Trade Execution Leverage** (8106): 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜
- **Position Tracker Realtime** (8107): ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì 
- **Notification Hub** (8109): ì§€ëŠ¥í˜• ì•Œë¦¼

## ğŸ”§ í•µì‹¬ ê¸°ëŠ¥

- âš¡ 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜ (ISOLATED ëª¨ë“œ)
- ğŸ§  Phoenix 95 AI ì‹ ë¢°ë„ ë¶„ì„
- ğŸ“Š ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì  ë° ì²­ì‚° ëª¨ë‹ˆí„°ë§
- ğŸ”” í…”ë ˆê·¸ë¨ í†µí•© ì•Œë¦¼
- ğŸ“ˆ Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§
- ğŸ”„ DDD ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

## ğŸ“š ë¬¸ì„œ

- API ë¬¸ì„œ: http://localhost:8100/docs
- Grafana: http://localhost:3000 (admin/admin)
- Prometheus: http://localhost:9090

## ğŸ› ï¸ ê°œë°œ

```bash
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
python recovery_scripts/quality_enforcer.py

# ì„±ëŠ¥ ìµœì í™”
python recovery_scripts/performance_optimizer.py
```
README_EOF

    log_success "ê³µí†µ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ"
}

# ê²€ì¦ ë° í…ŒìŠ¤íŠ¸
run_verification() {
    log_info "ì‹œìŠ¤í…œ ê²€ì¦ ì¤‘..."
    
    # êµ¬ì¡° ê²€ì¦
    required_dirs=("services" "shared" "infrastructure" "scripts" "tests")
    for dir in "${required_dirs[@]}"; do
        if [ -d "$dir" ]; then
            log_success "âœ… ë””ë ‰í† ë¦¬ ì¡´ì¬: $dir"
        else
            log_warning "âš ï¸ ë””ë ‰í† ë¦¬ ëˆ„ë½: $dir"
        fi
    done
    
    # í•„ìˆ˜ íŒŒì¼ ê²€ì¦
    required_files=("docker-compose.yml" "README.md" ".env.example")
    for file in "${required_files[@]}"; do
        if [ -f "$file" ]; then
            log_success "âœ… íŒŒì¼ ì¡´ì¬: $file"
        else
            log_warning "âš ï¸ íŒŒì¼ ëˆ„ë½: $file"
        fi
    done
    
    # ì„œë¹„ìŠ¤ êµ¬ì¡° ê²€ì¦
    for service_dir in services/*/; do
        if [ -d "$service_dir" ]; then
            service_name=$(basename "$service_dir")
            log_info "ì„œë¹„ìŠ¤ êµ¬ì¡° ê²€ì¦: $service_name"
            
            if [ -f "$service_dir/interfaces/api/main.py" ]; then
                log_success "  âœ… FastAPI ì•± ì¡´ì¬"
            else
                log_warning "  âš ï¸ FastAPI ì•± ëˆ„ë½"
            fi
            
            if [ -f "$service_dir/Dockerfile" ]; then
                log_success "  âœ… Dockerfile ì¡´ì¬"
            else
                log_warning "  âš ï¸ Dockerfile ëˆ„ë½"
            fi
        fi
    done
}

# ë©”ì¸ ì‹¤í–‰ íë¦„
main() {
    echo "ğŸ¯ Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ë° ìµœì í™”"
    echo "ê°œë°œì: AI Assistant"
    echo "ë²„ì „: 4.0.0"
    echo ""
    
    # 1. í™˜ê²½ ì²´í¬
    check_requirements
    
    # 2. ë°±ì—… ìƒì„±
    create_backup
    
    # 3. ì„œë¹„ìŠ¤ êµ¬ì¡° ìƒì„±
    create_service_structure
    
    # 4. ì„¤ì • íŒŒì¼ ìƒì„±
    create_config_files
    
    # 5. ë³µêµ¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    run_recovery_system
    
    # 6. ê²€ì¦
    run_verification
    
    echo ""
    echo "ğŸ‰ Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ì™„ë£Œ!"
    echo ""
    echo "ğŸ“Š ë‹¤ìŒ ë‹¨ê³„:"
    echo "1. docker-compose up -d (ì„œë¹„ìŠ¤ ì‹œì‘)"
    echo "2. curl http://localhost:8100/health (í—¬ìŠ¤ì²´í¬)"
    echo "3. http://localhost:3000 (Grafana ëŒ€ì‹œë³´ë“œ)"
    echo ""
    echo "ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:"
    echo "- ./services/ (7ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)"
    echo "- ./docker-compose.yml (ì™„ì „ í†µí•© ì„¤ì •)"
    echo "- ./shared/config/ (ê³µí†µ ì„¤ì •)"
    echo "- ./infrastructure/ (ì¸í”„ë¼ ì„¤ì •)"
    echo "- ./recovery_scripts/ (ë³µêµ¬ ë„êµ¬)"
    echo ""
    echo "ğŸ”— ìœ ìš©í•œ ë§í¬:"
    echo "- API Gateway: http://localhost:8100"
    echo "- Phoenix 95 AI: http://localhost:8103"
    echo "- Trade Execution: http://localhost:8106"
    echo "- Position Tracker: http://localhost:8107"
    echo "- Grafana: http://localhost:3000"
    echo "- Prometheus: http://localhost:9090"
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
EOF

# =============================================================================
# 2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
# =============================================================================

chmod +x run_phoenix95_recovery.sh

# =============================================================================
# 3. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ë“¤
# =============================================================================

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
cat > health_check_all.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬

echo "ğŸ” Phoenix 95 V4 ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"
echo "================================="

services=(
    "api-gateway-enterprise:8100"
    "signal-ingestion-pro:8101"
    "market-data-intelligence:8102"
    "phoenix95-ai-engine:8103"
    "trade-execution-leverage:8106"
    "position-tracker-realtime:8107"
    "notification-hub-intelligent:8109"
)

for service_info in "${services[@]}"; do
    IFS=':' read -r service_name port <<< "$service_info"
    
    echo -n "  $service_name ($port): "
    
    if curl -s -f "http://localhost:$port/health" > /dev/null 2>&1; then
        echo "âœ… ì •ìƒ"
    else
        echo "âŒ ë¹„ì •ìƒ"
    fi
done

echo ""
echo "ğŸ“Š ì¶”ê°€ í™•ì¸:"
echo -n "  PostgreSQL (5432): "
if pg_isready -h localhost -p 5432 > /dev/null 2>&1; then
    echo "âœ… ì •ìƒ"
else
    echo "âŒ ë¹„ì •ìƒ"
fi

echo -n "  Redis (6379): "
if redis-cli -p 6379 ping > /dev/null 2>&1; then
    echo "âœ… ì •ìƒ"
else
    echo "âŒ ë¹„ì •ìƒ"
fi
EOF

chmod +x health_check_all.sh

# ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
cat > monitor_logs.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 ë¡œê·¸ ëª¨ë‹ˆí„°ë§

echo "ğŸ“Š Phoenix 95 V4 ë¡œê·¸ ëª¨ë‹ˆí„°ë§"
echo "============================="

if [ "$1" = "errors" ]; then
    echo "ğŸ” ì—ëŸ¬ ë¡œê·¸ë§Œ í‘œì‹œ:"
    docker-compose logs --follow | grep -E "(ERROR|CRITICAL|Exception)"
elif [ "$1" = "service" ] && [ -n "$2" ]; then
    echo "ğŸ“‹ $2 ì„œë¹„ìŠ¤ ë¡œê·¸:"
    docker-compose logs --follow "$2"
else
    echo "ğŸ“‹ ëª¨ë“  ì„œë¹„ìŠ¤ ë¡œê·¸ (ì‹¤ì‹œê°„):"
    echo "ì‚¬ìš©ë²•: $0 [errors|service SERVICE_NAME]"
    echo ""
    docker-compose logs --follow --tail=50
fi
EOF

chmod +x monitor_logs.sh

# ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
cat > backup_system.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 ì‹œìŠ¤í…œ ë°±ì—…

BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "ğŸ’¾ Phoenix 95 V4 ì‹œìŠ¤í…œ ë°±ì—… ì¤‘..."
echo "ë°±ì—… ìœ„ì¹˜: $BACKUP_DIR"

# ì„¤ì • íŒŒì¼ ë°±ì—…
echo "ğŸ“‹ ì„¤ì • íŒŒì¼ ë°±ì—… ì¤‘..."
tar -czf "$BACKUP_DIR/configs.tar.gz" \
    docker-compose.yml \
    .env* \
    shared/config/ \
    infrastructure/ 2>/dev/null

# ì„œë¹„ìŠ¤ ì½”ë“œ ë°±ì—…
echo "ğŸ”§ ì„œë¹„ìŠ¤ ì½”ë“œ ë°±ì—… ì¤‘..."
tar -czf "$BACKUP_DIR/services.tar.gz" services/ 2>/dev/null

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (Docker ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°)
if docker-compose ps postgresql | grep -q "Up"; then
    echo "ğŸ—„ï¸ PostgreSQL ë°±ì—… ì¤‘..."
    docker-compose exec -T postgresql pg_dump -U phoenix95 phoenix95_v4 > "$BACKUP_DIR/postgresql_backup.sql"
fi

if docker-compose ps redis | grep -q "Up"; then
    echo "ğŸ“Š Redis ë°±ì—… ì¤‘..."
    docker-compose exec redis redis-cli BGSAVE
    docker cp $(docker-compose ps -q redis):/data/dump.rdb "$BACKUP_DIR/redis_backup.rdb" 2>/dev/null
fi

echo "âœ… ë°±ì—… ì™„ë£Œ: $BACKUP_DIR"
echo "ğŸ’¡ ë³µêµ¬ ë°©ë²•: ./restore_system.sh $BACKUP_DIR"
EOF

chmod +x backup_system.sh

# ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
cat > restore_system.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 ì‹œìŠ¤í…œ ë³µêµ¬

if [ -z "$1" ]; then
    echo "ì‚¬ìš©ë²•: $0 <ë°±ì—…_ë””ë ‰í† ë¦¬>"
    echo "ì˜ˆ: $0 backups/20241221_143022"
    exit 1
fi

BACKUP_DIR="$1"

if [ ! -d "$BACKUP_DIR" ]; then
    echo "âŒ ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: $BACKUP_DIR"
    exit 1
fi

echo "ğŸ”„ Phoenix 95 V4 ì‹œìŠ¤í…œ ë³µêµ¬ ì¤‘..."
echo "ë°±ì—… ìœ„ì¹˜: $BACKUP_DIR"

# í˜„ì¬ ì‹œìŠ¤í…œ ì¤‘ì§€
echo "â¹ï¸ í˜„ì¬ ì‹œìŠ¤í…œ ì¤‘ì§€ ì¤‘..."
docker-compose down 2>/dev/null || true

# ì„¤ì • íŒŒì¼ ë³µêµ¬
if [ -f "$BACKUP_DIR/configs.tar.gz" ]; then
    echo "ğŸ“‹ ì„¤ì • íŒŒì¼ ë³µêµ¬ ì¤‘..."
    tar -xzf "$BACKUP_DIR/configs.tar.gz"
fi

# ì„œë¹„ìŠ¤ ì½”ë“œ ë³µêµ¬
if [ -f "$BACKUP_DIR/services.tar.gz" ]; then
    echo "ğŸ”§ ì„œë¹„ìŠ¤ ì½”ë“œ ë³µêµ¬ ì¤‘..."
    tar -xzf "$BACKUP_DIR/services.tar.gz"
fi

# ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬
echo "ğŸ”„ ì‹œìŠ¤í…œ ì¬ì‹œì‘ ì¤‘..."
docker-compose up -d postgresql redis

echo "â³ ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
sleep 30

if [ -f "$BACKUP_DIR/postgresql_backup.sql" ]; then
    echo "ğŸ—„ï¸ PostgreSQL ë³µêµ¬ ì¤‘..."
    docker-compose exec -T postgresql psql -U phoenix95 -d phoenix95_v4 < "$BACKUP_DIR/postgresql_backup.sql"
fi

if [ -f "$BACKUP_DIR/redis_backup.rdb" ]; then
    echo "ğŸ“Š Redis ë³µêµ¬ ì¤‘..."
    docker cp "$BACKUP_DIR/redis_backup.rdb" $(docker-compose ps -q redis):/data/dump.rdb
    docker-compose restart redis
fi

# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
echo "ğŸš€ ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘..."
docker-compose up -d

echo "âœ… ì‹œìŠ¤í…œ ë³µêµ¬ ì™„ë£Œ!"
echo "ğŸ” í—¬ìŠ¤ì²´í¬: ./health_check_all.sh"
EOF

chmod +x restore_system.sh

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
cat > performance_test.sh << 'EOF'
#!/bin/bash
# Phoenix 95 V4 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

echo "âš¡ Phoenix 95 V4 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
echo "=========================="

# API Gateway ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
echo "ğŸ”— API Gateway ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘..."
if command -v ab > /dev/null 2>&1; then
    ab -n 1000 -c 10 http://localhost:8100/health
else
    echo "  âš ï¸ Apache Bench (ab)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
    echo "  ğŸ’¡ ì„¤ì¹˜: sudo apt-get install apache2-utils"
    
    # curlì„ ì´ìš©í•œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    echo "  ğŸ”„ curlì„ ì´ìš©í•œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸..."
    for i in {1..10}; do
        response_time=$(curl -o /dev/null -s -w "%{time_total}" http://localhost:8100/health)
        echo "    ìš”ì²­ $i: ${response_time}s"
    done
fi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
echo ""
echo "ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
echo ""
echo "ğŸ’¾ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:"
df -h | grep -E "(Filesystem|/dev/)"

echo ""
echo "âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
EOF

chmod +x performance_test.sh

# =============================================================================
# ì‹¤í–‰ ì•ˆë‚´
# =============================================================================

echo "ğŸ‰ Phoenix 95 V4 Enhanced ì™„ì „ ë³µêµ¬ ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ!"
echo ""
echo "ğŸ“‹ ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ë“¤:"
echo "  ğŸš€ run_phoenix95_recovery.sh - ë©”ì¸ ë³µêµ¬ ì‹¤í–‰"
echo "  ğŸ” health_check_all.sh - ì „ì²´ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"
echo "  ğŸ“Š monitor_logs.sh - ë¡œê·¸ ëª¨ë‹ˆí„°ë§"
echo "  ğŸ’¾ backup_system.sh - ì‹œìŠ¤í…œ ë°±ì—…"
echo "  ğŸ”„ restore_system.sh - ì‹œìŠ¤í…œ ë³µêµ¬"
echo "  âš¡ performance_test.sh - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
echo ""
echo "ğŸ¯ ì‹¤í–‰ ë°©ë²•:"
echo "  1. ./run_phoenix95_recovery.sh (ì „ì²´ ì‹œìŠ¤í…œ ë³µêµ¬)"
echo "  2. docker-compose up -d (ì„œë¹„ìŠ¤ ì‹œì‘)"
echo "  3. ./health_check_all.sh (ìƒíƒœ í™•ì¸)"
echo ""
echo "ğŸ“Š ëª¨ë‹ˆí„°ë§:"
echo "  - Grafana: http://localhost:3000 (admin/admin)"
echo "  - Prometheus: http://localhost:9090"
echo "  - API Gateway: http://localhost:8100"
echo ""
echo "ğŸ’¡ ìœ ì§€ë³´ìˆ˜:"
echo "  - ./backup_system.sh (ë°±ì—…)"
echo "  - ./monitor_logs.sh errors (ì—ëŸ¬ ë¡œê·¸ ëª¨ë‹ˆí„°ë§)"
echo "  - ./performance_test.sh (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)"

# === ë³µì›ëœ ëˆ„ë½ ì„¹ì…˜ ===
# ğŸš€ Phoenix 95 V4 Enhanced - ì™„ì „ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•
## ğŸ¯ **V4 Enhanced ì™„ì „ ì‹ ê·œ êµ¬ì¶• (ì›í´ë¦­ ë°°í¬)**
### **í•µì‹¬ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**
# tools/v4_complete_builder.py
Phoenix 95 V4 Enhanced ì™„ì „ ìë™í™” ë¹Œë”
ì›í´ë¦­ìœ¼ë¡œ ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ë°°í¬
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import subprocess
class V4CompleteBuilder:
def __init__(self):
self.target_path = Path("phoenix95_v4_enhanced")
# V4 í•µì‹¬ ì„œë¹„ìŠ¤ ì„¤ì •
self.services = {
"api-gateway-enterprise": {"port": 8100, "replicas": 2},
"signal-ingestion-pro": {"port": 8101, "replicas": 2},
"market-data-intelligence": {"port": 8102, "replicas": 2},
"phoenix95-ai-engine": {"port": 8103, "replicas": 3},
"trade-execution-leverage": {"port": 8106, "replicas": 2},
"position-tracker-realtime": {"port": 8107, "replicas": 2},
"notification-hub-intelligent": {"port": 8109, "replicas": 1}
# ë°ì´í„°ìŠ¤í† ì–´ ì„¤ì •
self.datastores = {
"postgresql": {"port": 5432, "data_volume": "100Gi"},
"redis": {"port": 6379, "data_volume": "50Gi"},
"influxdb": {"port": 8086, "data_volume": "200Gi"},
"elasticsearch": {"port": 9200, "data_volume": "150Gi"}
async def build_complete_system(self):
"""ì™„ì „ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•"""
print("ğŸš€ Phoenix 95 V4 Enhanced ì™„ì „ ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘")
await self._verify_environment()
# 2. í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
await self._create_project_structure()
# 3. ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±
await self._create_shared_library()
# 4. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒì„±
await self._create_microservices()
# 5. ì¸í”„ë¼ ì„¤ì • ìƒì„±
await self._create_infrastructure()
# 6. ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
await self._create_deployment_scripts()
# 7. ì‹¤ì œ ë°°í¬ ì‹¤í–‰
await self._execute_deployment()
# 8. ì‹œìŠ¤í…œ ê²€ì¦
await self._verify_system()
print("âœ… Phoenix 95 V4 Enhanced ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
except Exception as e:
print(f"âŒ ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤íŒ¨: {e}")
await self._cleanup_failed_deployment()
async def _verify_environment(self):
"""ë°°í¬ í™˜ê²½ ê²€ì¦"""
print("ğŸ” ë°°í¬ í™˜ê²½ ê²€ì¦ ì¤‘...")
for tool in required_tools:
subprocess.run([tool, "--version"],
capture_output=True, check=True)
except (subprocess.CalledProcessError, FileNotFoundError):
missing_tools.append(tool)
if missing_tools:
raise Exception(f"í•„ìˆ˜ ë„êµ¬ ëˆ„ë½: {missing_tools}")
print("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ")
async def _create_project_structure(self):
"""í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±"""
print("ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ì¤‘...")
structure = {
"services": list(self.services.keys()),
"shared": ["domain", "infrastructure", "config", "utils"],
"infrastructure": ["docker", "kubernetes", "terraform", "monitoring"],
"scripts": ["deployment", "migration", "testing"],
"tests": ["unit", "integration", "performance"]
for category, items in structure.items():
for item in items:
if category == "services":
for layer in ["domain", "application", "infrastructure", "interfaces"]:
path = self.target_path / category / item / layer
path.mkdir(parents=True, exist_ok=True)
# __init__.py ìƒì„±
(path / "__init__.py").touch()
path = self.target_path / category / item
path.mkdir(parents=True, exist_ok=True)
async def _create_shared_library(self):
"""ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±"""
print("ğŸ“š ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± ì¤‘...")
await self._create_config_files()
await self._create_domain_models()
# ì¸í”„ë¼ ì»´í¬ë„ŒíŠ¸ë“¤
await self._create_infrastructure_components()
async def _create_config_files(self):
"""ì„¤ì • íŒŒì¼ ìƒì„±"""
configs = {
"database_config.py": self._generate_database_config(),
"redis_config.py": self._generate_redis_config(),
"trading_config.py": self._generate_trading_config(),
"security_config.py": self._generate_security_config(),
"telegram_config.py": self._generate_telegram_config()
config_path = self.target_path / "shared" / "config"
for filename, content in configs.items():
with open(config_path / filename, 'w') as f:
f.write(content)
def _generate_database_config(self):
return '''"""
V4 Enhanced ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
from typing import Dict
DATABASE_CONFIG = {
"postgresql": {
"host": os.getenv("POSTGRES_HOST", "localhost"),
"port": int(os.getenv("POSTGRES_PORT", "5432")),
"database": os.getenv("POSTGRES_DB", "phoenix95_v4"),
"username": os.getenv("POSTGRES_USER", "phoenix95"),
"password": os.getenv("POSTGRES_PASSWORD", "phoenix95_secure"),
"pool_size": 20,
"max_connections": 100
"host": os.getenv("REDIS_HOST", "localhost"),
"port": int(os.getenv("REDIS_PORT", "6379")),
"password": os.getenv("REDIS_PASSWORD", ""),
"max_connections": 50
"influxdb": {
"url": os.getenv("INFLUXDB_URL", "http://localhost:8086"),
"token": os.getenv("INFLUXDB_TOKEN", ""),
"org": os.getenv("INFLUXDB_ORG", "phoenix95"),
"bucket": os.getenv("INFLUXDB_BUCKET", "metrics")
def get_database_url(db_type: str = "postgresql") -> str:
"""ë°ì´í„°ë² ì´ìŠ¤ URL ìƒì„±"""
if db_type == "postgresql":
config = DATABASE_CONFIG["postgresql"]
return f"postgresql://{config['username']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}"
elif db_type == "redis":
config = DATABASE_CONFIG["redis"]
return f"redis://:{config['password']}@{config['host']}:{config['port']}/{config['db']}"
raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {db_type}")
def _generate_trading_config(self):
return '''"""
V4 Enhanced ê±°ë˜ ì„¤ì •
"required_fields": ["symbol", "action", "price", "confidence"],
def _generate_telegram_config(self):
return '''"""
V4 Enhanced í…”ë ˆê·¸ë¨ ì„¤ì •
if not TELEGRAM_CONFIG["notification_levels"].get(level, False):
url = f"https://api.telegram.org/bot{TELEGRAM_CONFIG['bot_token']}/sendMessage"
"chat_id": TELEGRAM_CONFIG["chat_id"],
"text": f"[{level}] {message}",
async def _create_microservices(self):
"""ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒì„±"""
print("ğŸ”§ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒì„± ì¤‘...")
for service_name, config in self.services.items():
await self._create_single_service(service_name, config)
async def _create_single_service(self, service_name: str, config: Dict):
"""ê°œë³„ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒì„±"""
service_path = self.target_path / "services" / service_name
await self._create_service_domain(service_path, service_name)
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆì´ì–´
await self._create_service_application(service_path, service_name)
await self._create_service_infrastructure(service_path, service_name)
# API ì¸í„°í˜ì´ìŠ¤
await self._create_service_api(service_path, service_name, config)
# Dockerfile
await self._create_service_dockerfile(service_path, service_name, config)
async def _create_service_api(self, service_path: Path, service_name: str, config: Dict):
"""ì„œë¹„ìŠ¤ API ìƒì„±"""
api_path = service_path / "interfaces" / "api"
api_path.mkdir(parents=True, exist_ok=True)
api_content = f'''"""
{service_name} V4 Enhanced API
from fastapi import FastAPI, HTTPException, Depends
title="{service_name.title()}",
description="Phoenix 95 V4 Enhanced {service_name}",
allow_origins=["*"],
allow_methods=["*"],
allow_headers=["*"],
return {{"status": "healthy", "service": "{service_name}", "version": "4.0.0"}}
return {{"status": "ready", "service": "{service_name}"}}
return {{"metrics": "prometheus format here"}}
uvicorn.run(app, host="0.0.0.0", port={config["port"]})
with open(api_path / "main.py", 'w') as f:
f.write(api_content)
async def _create_infrastructure(self):
"""ì¸í”„ë¼ ì„¤ì • ìƒì„±"""
print("ğŸ—ï¸ ì¸í”„ë¼ ì„¤ì • ìƒì„± ì¤‘...")
# Docker Compose
await self._create_docker_compose()
# Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸
await self._create_kubernetes_manifests()
# Monitoring ì„¤ì •
await self._create_monitoring_config()
async def _create_docker_compose(self):
"""Docker Compose íŒŒì¼ ìƒì„±"""
compose_content = f'''version: '3.8'
{self._generate_service_compose_entries()}
with open(self.target_path / "docker-compose.yml", 'w') as f:
f.write(compose_content)
def _generate_service_compose_entries(self):
"""ì„œë¹„ìŠ¤ë³„ Docker Compose í•­ëª© ìƒì„±"""
for service_name, config in self.services.items():
entry = f'''
{service_name}:
context: ./services/{service_name}
dockerfile: Dockerfile
- "{config['port']}:{config['port']}"
restart: unless-stopped'''
entries.append(entry)
return '\n'.join(entries)
async def _create_deployment_scripts(self):
"""ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
print("ğŸ“œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
# ë©”ì¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
deploy_script = f'''#!/bin/bash
# Phoenix 95 V4 Enhanced ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
echo "ğŸš€ Phoenix 95 V4 Enhanced ë°°í¬ ì‹œì‘"
START_TIME=$(date +%s)
echo "ğŸ” í™˜ê²½ ê²€ì¦ ì¤‘..."
docker --version || {{ echo "Docker í•„ìš”"; exit 1; }}
docker-compose --version || {{ echo "Docker Compose í•„ìš”"; exit 1; }}
# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
echo "ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì‹œì‘ ì¤‘..."
docker-compose up -d postgresql redis influxdb
echo "ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘..."
python3 scripts/create_schemas.py
# ì„œë¹„ìŠ¤ ë¹Œë“œ ë° ë°°í¬
echo "ğŸ”§ ì„œë¹„ìŠ¤ ë¹Œë“œ ì¤‘..."
docker-compose build
echo "ğŸš€ ì„œë¹„ìŠ¤ ë°°í¬ ì¤‘..."
echo "ğŸ” í—¬ìŠ¤ì²´í¬ ì¤‘..."
{self._generate_health_checks()}
echo "ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì¤‘..."
docker-compose up -d prometheus grafana
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
echo "âœ… Phoenix 95 V4 Enhanced ë°°í¬ ì™„ë£Œ!"
echo "â±ï¸ ë°°í¬ ì‹œê°„: $((DURATION / 60))ë¶„ $((DURATION % 60))ì´ˆ"
echo "ğŸ”— API Gateway: http://localhost:8100"
echo "ğŸ“Š Grafana: http://localhost:3000"
import requests
telegram_token = '7386542811:AAEZ21p30rES1k8NxNM2xbZ53U44PI9D5CY'
telegram_chat_id = '7590895952'
message = 'ğŸ‰ Phoenix 95 V4 Enhanced ë°°í¬ ì™„ë£Œ! ì‹œê°„: $((DURATION / 60))ë¶„'
requests.post(f'https://api.telegram.org/bot{{telegram_token}}/sendMessage',
data={{'chat_id': telegram_chat_id, 'text': message}})
print('âœ… í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ë¨')
except: pass
deploy_path = self.target_path / "deploy.sh"
with open(deploy_path, 'w') as f:
f.write(deploy_script)
deploy_path.chmod(0o755)
def _generate_health_checks(self):
"""í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
for service_name, config in self.services.items():
check = f'''
for i in {{1..10}}; do
if curl -f -s http://localhost:{config['port']}/health > /dev/null; then
echo "âœ… {service_name} í—¬ìŠ¤ì²´í¬ ì„±ê³µ"
if [ $i -eq 10 ]; then
echo "âŒ {service_name} í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
echo "â³ {service_name} í—¬ìŠ¤ì²´í¬ ì¬ì‹œë„... ($i/10)"
checks.append(check)
return '\n'.join(checks)
async def _execute_deployment(self):
"""ì‹¤ì œ ë°°í¬ ì‹¤í–‰"""
print("ğŸš€ ë°°í¬ ì‹¤í–‰ ì¤‘...")
# ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
deploy_script = self.target_path / "deploy.sh"
if deploy_script.exists():
process = await asyncio.create_subprocess_exec(
str(deploy_script),
cwd=self.target_path,
stdout=asyncio.subprocess.PIPE,
stderr=asyncio.subprocess.PIPE
stdout, stderr = await process.communicate()
if process.returncode == 0:
print("âœ… ë°°í¬ ì„±ê³µ")
print(stdout.decode())
print("âŒ ë°°í¬ ì‹¤íŒ¨")
print(stderr.decode())
raise Exception("ë°°í¬ ì‹¤íŒ¨")
async def _verify_system(self):
"""ì‹œìŠ¤í…œ ê²€ì¦"""
print("ğŸ” ì‹œìŠ¤í…œ ê²€ì¦ ì¤‘...")
# ì„œë¹„ìŠ¤ë³„ í—¬ìŠ¤ì²´í¬
for service_name, config in self.services.items():
import aiohttp
async with aiohttp.ClientSession() as session:
async with session.get(f"http://localhost:{config['port']}/health") as response:
if response.status == 200:
print(f"âœ… {service_name} ì •ìƒ")
print(f"âš ï¸ {service_name} ì‘ë‹µ ì½”ë“œ: {response.status}")
except Exception as e:
print(f"âŒ {service_name} ê²€ì¦ ì‹¤íŒ¨: {e}")
async def _cleanup_failed_deployment(self):
"""ì‹¤íŒ¨í•œ ë°°í¬ ì •ë¦¬"""
print("ğŸ§¹ ì‹¤íŒ¨í•œ ë°°í¬ ì •ë¦¬ ì¤‘...")
cwd=self.target_path, capture_output=True)
builder = V4CompleteBuilder()
await builder.build_complete_system()
asyncio.run(main())
### **V3 ì‹œìŠ¤í…œ ì™„ì „ ë¶„ì„ ë° ë°±ì—…**
# V3 ì‹œìŠ¤í…œ ì™„ì „ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (44.txt ê¸°ì¡´ ì—°ê³„ ì™„ì „ í†µí•©)
echo "ğŸ” Phoenix 95 V3 ì‹œìŠ¤í…œ ì™„ì „ ë¶„ì„ ì‹œì‘"
# V3 í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë§¤í•‘ (ì •í™•í•œ ë¼ì¸ ë²ˆí˜¸)
declare -A V3_COMPONENTS=(
["CompleteSignalValidator"]="ë¼ì¸ 266-998"
["Phoenix95CompleteAnalyzer"]="ë¼ì¸ 999-1734"
["CompleteTradeExecutor"]="ë¼ì¸ 1735-2262"
["CompletePerformanceMonitor"]="ë¼ì¸ 2263-2414"
["CompleteWebhookServer"]="ë¼ì¸ 2455-2700"
# V3 ì„¤ì • ë³´ì¡´ í™•ì¸
declare -A V3_CONFIGS=(
["TELEGRAM_CONFIG"]="í…”ë ˆê·¸ë¨ í† í°/ì±„íŒ…ID ë³´ì¡´ í•„ìˆ˜"
["SECURITY_CONFIG"]="ì›¹í›… ì‹œí¬ë¦¿/API í‚¤ ë³´ì¡´ í•„ìˆ˜"
["TRADING_CONFIG"]="í—ˆìš© ì‹¬ë³¼/ì‹ ë¢°ë„ ì„ê³„ê°’ ë³´ì¡´ í•„ìˆ˜"
["LEVERAGE_CONFIG"]="20x ë ˆë²„ë¦¬ì§€/ISOLATED ëª¨ë“œ ë³´ì¡´ í•„ìˆ˜"
# ê¸°ì¡´ ë°ì´í„° ë°±ì—…
echo "ğŸ’¾ V3 ë°ì´í„° ë°±ì—… ì‹œì‘..."
BACKUP_DIR="backup/v3_system/$(date +%Y%m%d_%H%M%S)"
mkdir -p $BACKUP_DIR
if [ -f "main_webhook_server.py" ]; then
cp main_webhook_server.py $BACKUP_DIR/
echo "âœ… V3 ë©”ì¸ ì„œë²„ íŒŒì¼ ë°±ì—… ì™„ë£Œ"
if [ -d "logs_complete_webhook" ]; then
cp -r logs_complete_webhook $BACKUP_DIR/
echo "âœ… V3 ë¡œê·¸ íŒŒì¼ ë°±ì—… ì™„ë£Œ"
echo "âœ… V3 ì‹œìŠ¤í…œ ë¶„ì„ ì™„ë£Œ"
### **V4 í™˜ê²½ ì„¤ì • ë° í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤**
# tools/v4_environment_setup.py
V4 Enhanced í™˜ê²½ ì¤€ë¹„ - 44.txt ê¸°ì¡´ ì—°ê³„ íŒ¨í„´ ì™„ì „ ì ìš©
import json
from pathlib import Path
from typing import Dict, List
class V4EnvironmentSetup:
def __init__(self):
self.v3_backup_path = Path("backup/v3_system")
self.v4_target_path = Path("phoenix95_v4_enhanced")
# V3 í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ (44.txt ê¸°ë°˜)
self.compatibility_matrix = {
"config_preservation": {
"TELEGRAM_CONFIG": {"preserve": True, "migrate_to": "shared/config/telegram_config.py"},
"SECURITY_CONFIG": {"preserve": True, "migrate_to": "shared/config/security_config.py"},
"TRADING_CONFIG": {"preserve": True, "migrate_to": "shared/config/trading_config.py"},
"LEVERAGE_CONFIG": {"preserve": True, "migrate_to": "shared/config/leverage_config.py"},
"PHOENIX_95_CONFIG": {"preserve": True, "migrate_to": "shared/config/phoenix95_config.py"}
"component_migration": {
"CompleteSignalValidator": {
"v3_lines": "266-998",
"v4_location": "services/market-data-intelligence/domain/aggregates/signal_validator.py",
"migration_strategy": "direct_port_with_enhancement"
"Phoenix95CompleteAnalyzer": {
"v3_lines": "999-1734",
"v4_location": "services/phoenix95-ai-engine/domain/aggregates/ai_analyzer.py",
"migration_strategy": "enhance_and_modularize"
"CompleteTradeExecutor": {
"v3_lines": "1735-2262",
"v4_location": "services/trade-execution-leverage/domain/aggregates/trade_executor.py",
"migration_strategy": "leverage_enhancement"
"data_migration": {
"signal_history": {"v3_format": "deque", "v4_format": "postgresql_table"},
"performance_metrics": {"v3_format": "memory", "v4_format": "influxdb_measurements"},
"position_tracking": {"v3_format": "dict", "v4_format": "redis_realtime"},
"analysis_cache": {"v3_format": "memory", "v4_format": "redis_structured"}
def setup_v4_environment(self):
"""V4 Enhanced í™˜ê²½ ì„¤ì •"""
print("ğŸ—ï¸ V4 Enhanced í™˜ê²½ ì„¤ì • ì‹œì‘")
# 1. V4 í´ë” êµ¬ì¡° ìƒì„±
self._create_v4_structure()
# 2. V3 ì„¤ì • ë§ˆì´ê·¸ë ˆì´ì…˜
self._migrate_v3_configs()
# 3. V3 ì»´í¬ë„ŒíŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜
self._migrate_v3_components()
# 4. í˜¸í™˜ì„± ê²€ì¦
self._verify_compatibility()
print("âœ… V4 Enhanced í™˜ê²½ ì„¤ì • ì™„ë£Œ")
# V3â†’V4 ì½”ë“œ ìë™ ë³€í™˜ê¸°
### **V3â†’V4 ì½”ë“œ ë³€í™˜ ë° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜**
# tools/v3_to_v4_converter.py
V3 â†’ V4 ì½”ë“œ ìë™ ë³€í™˜ê¸° + ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
import asyncpg
import aioredis
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass
class MigrationPlan:
source_type: str
target_type: str
data_volume: int
estimated_time: int
rollback_strategy: str
class V3ToV4CompleteConverter:
def __init__(self):
self.conversion_rules = {
"CompleteSignalValidator": {
"target_aggregate": "market-data-intelligence/domain/aggregates/signal_validator.py",
"validate_signal_complete",
"_fetch_complete_market_data",
"_validate_price_complete",
"_validate_market_conditions_complete"
"async_performance_optimization",
"distributed_caching",
"real_time_streaming"
"Phoenix95CompleteAnalyzer": {
"target_aggregate": "phoenix95-ai-engine/domain/aggregates/ai_analyzer.py",
"analyze_signal_phoenix_95_complete",
"_phoenix_95_full_analysis",
"_calculate_leverage_position",
"_apply_kelly_formula_complete"
"ml_model_versioning",
"feature_store_integration",
"model_explainability"
"CompleteTradeExecutor": {
"target_aggregate": "trade-execution-leverage/domain/aggregates/trade_executor.py",
"execute_trade_complete",
"_execute_trade_simulation",
"_start_position_tracking",
"_monitor_position",
"_close_position"
"real_exchange_connectivity",
"risk_management_automation",
"position_size_optimization"
# ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
self.migration_plans = {
"signal_history": MigrationPlan(
source_type="deque_memory",
target_type="postgresql_signals_table",
data_volume=1000,
estimated_time=300,
rollback_strategy="restore_from_backup"
"performance_metrics": MigrationPlan(
source_type="deque_memory",
target_type="influxdb_measurements",
data_volume=10000,
estimated_time=600,
rollback_strategy="delete_influx_bucket"
"position_tracking": MigrationPlan(
source_type="dict_memory",
target_type="redis_hash_realtime",
data_volume=100,
estimated_time=60,
rollback_strategy="flush_redis_keys"
async def execute_full_migration(self) -> Dict:
"""ì „ì²´ V3â†’V4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
print("ğŸŒŠ V3 â†’ V4 ì™„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
migration_results = {}
print("ğŸ”§ V3 ì½”ë“œ â†’ V4 DDD êµ¬ì¡° ë³€í™˜ ì¤‘...")
code_results = await self._convert_v3_code()
migration_results["code_conversion"] = code_results
# 2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
print("ğŸ“Š ë©”ëª¨ë¦¬ ë°ì´í„° â†’ ì˜êµ¬ ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
data_results = await self._migrate_all_data()
migration_results["data_migration"] = data_results
# 3. ì„¤ì • ë§ˆì´ê·¸ë ˆì´ì…˜
print("âš™ï¸ V3 ì„¤ì • â†’ V4 ì„¤ì • ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
config_results = await self._migrate_configs()
migration_results["config_migration"] = config_results
verification_result = await self._verify_migration_integrity()
migration_results["verification"] = verification_result
print("âœ… V3 â†’ V4 ì™„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
except Exception as e:
print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
await self._execute_rollback()
return migration_results
async def _migrate_all_data(self) -> Dict:
"""ì „ì²´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
data_results = {}
# 1. ì‹ í˜¸ ì´ë ¥ ë§ˆì´ê·¸ë ˆì´ì…˜
signal_result = await self._migrate_signal_history()
data_results["signal_history"] = signal_result
# 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë§ˆì´ê·¸ë ˆì´ì…˜
metrics_result = await self._migrate_performance_metrics()
data_results["performance_metrics"] = metrics_result
# 3. í¬ì§€ì…˜ ì¶”ì  ë§ˆì´ê·¸ë ˆì´ì…˜
position_result = await self._migrate_position_tracking()
data_results["position_tracking"] = position_result
return data_results
async def _migrate_signal_history(self) -> Dict:
"""ì‹ í˜¸ ì´ë ¥ â†’ PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜"""
# V3 ë©”ëª¨ë¦¬ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
"signal_id": f"V3_SIG_{i:06d}",
"symbol": "BTCUSDT",
"action": "buy",
"price": 45000 + i * 10,
"confidence": 0.8,
"phoenix95_score": 0.85,
"analysis_type": "PHOENIX_95_COMPLETE_FULL"
for i in range(100)
# PostgreSQLë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
conn = await asyncpg.connect("postgresql://phoenix95:phoenix95_secure@localhost/phoenix95_v4")
migrated_count = 0
for signal in v3_signal_data:
await conn.execute("""
INSERT INTO signals (signal_id, symbol, action, price, confidence, phoenix95_score)
VALUES ($1, $2, $3, $4, $5, $6)
""", signal["signal_id"], signal["symbol"], signal["action"],
signal["price"], signal["confidence"], signal["phoenix95_score"])
migrated_count += 1
except Exception as e:
print(f"âš ï¸ ì‹ í˜¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {signal['signal_id']}")
await conn.close()
"source_count": len(v3_signal_data),
"migrated_count": migrated_count,
"success_rate": migrated_count / len(v3_signal_data) * 100
async def _migrate_performance_metrics(self) -> Dict:
"""ì„±ëŠ¥ ë©”íŠ¸ë¦­ â†’ InfluxDB ë§ˆì´ê·¸ë ˆì´ì…˜"""
"timestamp": datetime.now().isoformat(),
"memory_usage": 0.6,
"cpu_usage": 0.4,
"response_time": 0.2,
"requests_per_second": 50
for _ in range(1000)
# InfluxDB ì—°ê²° ë° ë°ì´í„° ì‚½ì… ì‹œë®¬ë ˆì´ì…˜
migrated_count = len(v3_performance_data)  # ì‹œë®¬ë ˆì´ì…˜
"source_count": len(v3_performance_data),
"migrated_count": migrated_count,
"target_measurement": "system_metrics"
async def _migrate_position_tracking(self) -> Dict:
"""í¬ì§€ì…˜ ì¶”ì  â†’ Redis ë§ˆì´ê·¸ë ˆì´ì…˜"""
v3_active_positions = {
"EXEC_001": {
"symbol": "BTCUSDT",
"action": "buy",
"leverage": 20,
"margin_mode": "ISOLATED",
"entry_price": 45000.0,
"status": "ACTIVE"
# Redis ì—°ê²° ë° ë°ì´í„° ì‚½ì… ì‹œë®¬ë ˆì´ì…˜
redis = aioredis.from_url("redis://localhost:6379")
migrated_count = 0
for position_id, position_data in v3_active_positions.items():
await redis.hset(f"position:{position_id}", mapping=position_data)
migrated_count += 1
except Exception as e:
print(f"âš ï¸ í¬ì§€ì…˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {position_id}")
await redis.close()
"source_count": len(v3_active_positions),
"migrated_count": migrated_count,
"target_store": "redis_positions"
# ì™„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
### **Terraform AWS ì¸í”„ë¼**
# infrastructure/terraform/main.tf
terraform {
required_providers {
aws = { source = "hashicorp/aws", version = "~> 5.0" }
kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.0" }
provider "aws" {
region = var.aws_region
resource "aws_eks_cluster" "phoenix95_v4" {
name     = "phoenix95-v4-cluster"
role_arn = aws_iam_role.cluster_role.arn
version  = "1.28"
vpc_config {
subnet_ids = aws_subnet.phoenix95_subnets[*].id
resource "aws_vpc" "phoenix95_v4_vpc" {
cidr_block           = "10.0.0.0/16"
enable_dns_hostnames = true
enable_dns_support   = true
tags = { Name = "phoenix95-v4-vpc" }
resource "aws_subnet" "phoenix95_subnets" {
vpc_id            = aws_vpc.phoenix95_v4_vpc.id
cidr_block        = "10.0.${count.index + 1}.0/24"
availability_zone = data.aws_availability_zones.available.names[count.index]
map_public_ip_on_launch = true
tags = { Name = "phoenix95-v4-subnet-${count.index + 1}" }
resource "aws_iam_role" "cluster_role" {
name = "phoenix95-v4-cluster-role"
assume_role_policy = jsonencode({
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = { Service = "eks.amazonaws.com" }
Version = "2012-10-17"
resource "aws_iam_role_policy_attachment" "cluster_AmazonEKSClusterPolicy" {
policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
role       = aws_iam_role.cluster_role.name
resource "aws_eks_node_group" "phoenix95_nodes" {
cluster_name    = aws_eks_cluster.phoenix95_v4.name
node_group_name = "phoenix95-v4-nodes"
node_role_arn   = aws_iam_role.node_role.arn
subnet_ids      = aws_subnet.phoenix95_subnets[*].id
scaling_config {
desired_size = 3
max_size     = 10
min_size     = 1
instance_types = ["t3.medium"]
resource "aws_iam_role" "node_role" {
name = "phoenix95-v4-node-role"
assume_role_policy = jsonencode({
Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "ec2.amazonaws.com" } }]
Version = "2012-10-17"
resource "aws_iam_role_policy_attachment" "node_AmazonEKSWorkerNodePolicy" {
policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
role       = aws_iam_role.node_role.name
resource "aws_iam_role_policy_attachment" "node_AmazonEKS_CNI_Policy" {
policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
role       = aws_iam_role.node_role.name
resource "aws_iam_role_policy_attachment" "node_AmazonEC2ContainerRegistryReadOnly" {
policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
role       = aws_iam_role.node_role.name
data "aws_availability_zones" "available" { state = "available" }
output "cluster_endpoint" { value = aws_eks_cluster.phoenix95_v4.endpoint }
output "cluster_name" { value = aws_eks_cluster.phoenix95_v4.name }
variable "aws_region" { default = "us-west-2" }
### **AlertManager + í…”ë ˆê·¸ë¨ í†µí•©**
# infrastructure/monitoring/alertmanager.yml
smtp_smarthost: 'localhost:587'
smtp_from: 'alerts@phoenix95.io'
group_by: ['alertname']
group_wait: 10s
group_interval: 10s
repeat_interval: 1h
receiver: 'phoenix95-telegram'
- name: 'phoenix95-telegram'
telegram_configs:
- bot_token: '7386542811:AAEZ21p30rES1k8NxNM2xbZ53U44PI9D5CY'
chat_id: 7590895952
ğŸš¨ Phoenix 95 V4 Alert ğŸš¨
Alert: {{ .GroupLabels.alertname }}
Status: {{ .Status }}
{{ range .Alerts }}
Instance: {{ .Labels.instance }}
Summary: {{ .Annotations.summary }}
Description: {{ .Annotations.description }}
# Alert Rules
# infrastructure/monitoring/alert_rules.yml
- name: phoenix95_v4_alerts
- alert: ServiceDown
expr: up == 0
labels: { severity: critical }
annotations:
summary: "Phoenix 95 V4 ì„œë¹„ìŠ¤ ë‹¤ìš´"
description: "{{ $labels.instance }} ì„œë¹„ìŠ¤ê°€ 1ë¶„ ì´ìƒ ë‹¤ìš´"
- alert: HighErrorRate
expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
labels: { severity: warning }
annotations:
summary: "ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€"
description: "{{ $labels.job }}ì—ì„œ 5% ì´ìƒ ì—ëŸ¬ìœ¨"
- alert: TradingSystemDown
expr: up{job="trade-execution-leverage"} == 0
labels: { severity: critical }
annotations:
summary: "ê±°ë˜ ì‹œìŠ¤í…œ ë‹¤ìš´"
description: "ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹œìŠ¤í…œì´ ë‹¤ìš´ë˜ì—ˆìŠµë‹ˆë‹¤"
### **Blue-Green ë°°í¬ ìŠ¤í¬ë¦½íŠ¸**
# scripts/blue_green_deploy.sh
# ë¬´ì¤‘ë‹¨ Blue-Green ë°°í¬
echo "ğŸ”„ Blue-Green ë°°í¬ ì‹œì‘"
NAMESPACE="phoenix95-v4"
NEW_VERSION="v4.0.1"
CURRENT_VERSION=$(kubectl get deployment api-gateway-enterprise -n $NAMESPACE -o jsonpath='{.metadata.labels.version}')
echo "Current: $CURRENT_VERSION â†’ New: $NEW_VERSION"
# Green í™˜ê²½ ë°°í¬
echo "ğŸŸ¢ Green í™˜ê²½ ë°°í¬ ì¤‘..."
sed "s/version: .*/version: $NEW_VERSION/g" k8s/services.yaml | kubectl apply -f -
# Green í™˜ê²½ í—¬ìŠ¤ì²´í¬
echo "ğŸ” Green í™˜ê²½ í—¬ìŠ¤ì²´í¬..."
kubectl wait --for=condition=ready pod -l version=$NEW_VERSION -n $NAMESPACE --timeout=300s
# íŠ¸ë˜í”½ ì ì§„ì  ì „í™˜ (10% â†’ 50% â†’ 100%)
for weight in 10 50 100; do
echo "ğŸ“Š íŠ¸ë˜í”½ ${weight}% ì „í™˜ ì¤‘..."
kubectl patch ingress phoenix95-ingress -n $NAMESPACE -p "{
\"metadata\": {
\"annotations\": {
\"nginx.ingress.kubernetes.io/canary\": \"true\",
\"nginx.ingress.kubernetes.io/canary-weight\": \"$weight\"
sleep 300  # 5ë¶„ ëŒ€ê¸°
ERROR_RATE=$(kubectl logs deployment/api-gateway-enterprise -n $NAMESPACE | grep ERROR | wc -l)
if [ $ERROR_RATE -gt 10 ]; then
echo "âŒ ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€ - ë¡¤ë°±"
kubectl patch ingress phoenix95-ingress -n $NAMESPACE -p "{
\"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/canary\": \"false\" } }
echo "âœ… ${weight}% íŠ¸ë˜í”½ ì „í™˜ ì„±ê³µ"
# Blue í™˜ê²½ ì •ë¦¬
echo "ğŸ”µ Blue í™˜ê²½ ì •ë¦¬ ì¤‘..."
kubectl delete deployment -l version=$CURRENT_VERSION -n $NAMESPACE
echo "âœ… Blue-Green ë°°í¬ ì™„ë£Œ!"
### **ìŠ¤í‚¤ë§ˆ ìƒì„± ìŠ¤í¬ë¦½íŠ¸**
# scripts/create_schemas.py
V4 Enhanced ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„±
import asyncpg
import aioredis
from datetime import datetime
async def create_postgresql_schemas():
"""PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„±"""
print("ğŸ“Š PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘...")
conn = await asyncpg.connect("postgresql://phoenix95:phoenix95_secure@localhost/phoenix95_v4")
await conn.execute('''
CREATE TABLE IF NOT EXISTS signals (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
signal_id VARCHAR(50) UNIQUE NOT NULL,
symbol VARCHAR(20) NOT NULL,
action VARCHAR(10) NOT NULL,
price DECIMAL(20, 8),
confidence DECIMAL(5, 4),
phoenix95_score DECIMAL(5, 4),
timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
processed BOOLEAN DEFAULT FALSE,
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
await conn.execute('''
CREATE TABLE IF NOT EXISTS trades (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
signal_id VARCHAR(50) REFERENCES signals(signal_id),
symbol VARCHAR(20) NOT NULL,
action VARCHAR(10) NOT NULL,
entry_price DECIMAL(20, 8),
exit_price DECIMAL(20, 8),
quantity DECIMAL(20, 8),
leverage INTEGER,
margin_mode VARCHAR(20),
status VARCHAR(20) DEFAULT 'ACTIVE',
pnl DECIMAL(20, 8),
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…Œì´ë¸”
await conn.execute('''
CREATE TABLE IF NOT EXISTS performance_metrics (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
metric_type VARCHAR(50) NOT NULL,
value DECIMAL(20, 8),
timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
# V3 í˜¸í™˜ì„± í…Œì´ë¸” (ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)
await conn.execute('''
CREATE TABLE IF NOT EXISTS v3_migration_log (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
source_type VARCHAR(50),
target_type VARCHAR(50),
records_count INTEGER,
migration_status VARCHAR(20),
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
await conn.close()
print("âœ… PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ")
async def setup_redis_structures():
"""Redis êµ¬ì¡° ì„¤ì •"""
print("ğŸ”´ Redis êµ¬ì¡° ì„¤ì • ì¤‘...")
redis = aioredis.from_url("redis://localhost:6379")
await redis.hset("phoenix95:config", "system_status", "active")
await redis.hset("phoenix95:config", "last_update", datetime.now().isoformat())
await redis.hset("phoenix95:config", "migration_status", "completed")
# V3 í˜¸í™˜ì„± ì„¤ì •
await redis.hset("phoenix95:v3_compat", "enabled", "true")
await redis.hset("phoenix95:v3_compat", "webhook_endpoint", "http://localhost:8101/webhook/tradingview")
await redis.close()
print("âœ… Redis êµ¬ì¡° ì„¤ì • ì™„ë£Œ")
"""ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
await create_postgresql_schemas()
await setup_redis_structures()
print("ğŸ‰ ëª¨ë“  ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ!")
print(f"âŒ ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨: {e}")
asyncio.run(main())
### **ëª¨ë‹ˆí„°ë§ ì„¤ì •**
# infrastructure/monitoring/prometheus.yml
- targets: ['localhost:9090']
### **Kubernetes ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸**
# infrastructure/kubernetes/namespace.yaml
apiVersion: v1
kind: Namespace
name: phoenix95-v4
version: v4.0.0
system: phoenix95-enhanced
# infrastructure/kubernetes/services.yaml
apiVersion: apps/v1
kind: Deployment
name: api-gateway-enterprise
namespace: phoenix95-v4
replicas: 2
matchLabels:
app: api-gateway-enterprise
app: api-gateway-enterprise
containers:
- name: api-gateway
image: phoenix95/api-gateway-enterprise:v4.0.0
- containerPort: 8100
- name: DATABASE_URL
value: "postgresql://phoenix95:phoenix95_secure@postgresql:5432/phoenix95_v4"
- name: REDIS_URL
value: "redis://redis:6379"
livenessProbe:
path: /health
initialDelaySeconds: 30
periodSeconds: 10
readinessProbe:
path: /ready
initialDelaySeconds: 5
periodSeconds: 5
apiVersion: v1
kind: Service
name: api-gateway-enterprise
namespace: phoenix95-v4
app: api-gateway-enterprise
targetPort: 8100
type: ClusterIP
### **V4SystemArchitect ì™„ì „ êµ¬í˜„**
# tools/v4_system_architect.py
V4 Enhanced ì‹œìŠ¤í…œ ì„¤ê³„ì - ì™„ì „ ì‹ ê·œ DDD ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ìƒì„±
import json
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass
class V4ServiceBlueprint:
"""V4 ì„œë¹„ìŠ¤ ì²­ì‚¬ì§„"""
domain_focus: str
key_features: List[str]
dependencies: List[str]
data_stores: List[str]
api_endpoints: List[str]
class V4SystemArchitect:
def __init__(self):
self.target_path = Path("phoenix95_v4_enhanced")
# V4 ì„œë¹„ìŠ¤ ì²­ì‚¬ì§„ë“¤
self.service_blueprints = {
"api-gateway-enterprise": V4ServiceBlueprint(
name="api-gateway-enterprise",
domain_focus="ë¼ìš°íŒ… & ì¸ì¦",
key_features=["JWT ê¸°ë°˜ ì¸ì¦", "ìš”ì²­ ë¼ìš°íŒ…", "ì†ë„ ì œí•œ", "ë¡œë“œ ë°¸ëŸ°ì‹±"],
dependencies=["redis"],
data_stores=["redis"],
api_endpoints=["/auth", "/health", "/metrics", "/webhook"]
"phoenix95-ai-engine": V4ServiceBlueprint(
name="phoenix95-ai-engine",
domain_focus="AI ê¸°ë°˜ ì‹ í˜¸ ë¶„ì„",
key_features=["Phoenix 95ì  ì‹ ë¢°ë„ ë¶„ì„", "AI ëª¨ë¸ ì•™ìƒë¸”", "ì˜ˆì¸¡ ì •í™•ë„", "Kelly Criterion"],
dependencies=["postgresql", "redis"],
data_stores=["postgresql", "redis"],
api_endpoints=["/analyze", "/confidence", "/prediction"]
"trade-execution-leverage": V4ServiceBlueprint(
name="trade-execution-leverage",
domain_focus="ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹¤í–‰",
key_features=["20x ë ˆë²„ë¦¬ì§€ ì§€ì›", "ISOLATED ë§ˆì§„ ëª¨ë“œ", "ì‹¤ì‹œê°„ ì²­ì‚°ê°€", "ìµì ˆ/ì†ì ˆ"],
dependencies=["postgresql", "redis"],
data_stores=["postgresql", "redis"],
api_endpoints=["/execute", "/positions", "/leverage"]
# V4 ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¡°
self.shared_structure = {
"domain": ["aggregates", "value_objects", "domain_events", "domain_services", "repositories"],
"infrastructure": ["database", "messaging", "external_apis", "caching", "monitoring"],
"application": ["services", "handlers", "dto", "interfaces"],
"config": ["database_config.py", "redis_config.py", "api_config.py", "trading_config.py"],
"utils": ["validators.py", "formatters.py", "encryption.py", "logging.py"]
async def build_complete_v4_system(self) -> Dict:
"""V4 ì™„ì „ ì‹ ê·œ ì‹œìŠ¤í…œ êµ¬ì¶•"""
print("ğŸ—ï¸ V4 Enhanced ì™„ì „ ì‹ ê·œ ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘")
build_results = {"shared_library": {}, "microservices": {}, "infrastructure": {}, "deployment": {}}
# 1. ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±
build_results["shared_library"] = await self._create_shared_library()
# 2. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë“¤ ìƒì„±
build_results["microservices"] = await self._create_microservices()
# 3. ì¸í”„ë¼ êµ¬ì„± ìƒì„±
build_results["infrastructure"] = await self._create_infrastructure()
# 4. ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
build_results["deployment"] = await self._create_deployment_scripts()
print("âœ… V4 Enhanced ì™„ì „ ì‹ ê·œ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
return build_results
except Exception as e:
print(f"âŒ V4 ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤íŒ¨: {e}")
async def _create_microservices(self) -> Dict:
"""V4 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë“¤ ìƒì„±"""
microservice_results = {}
for service_name, blueprint in self.service_blueprints.items():
print(f"  ğŸ”§ {service_name} ìƒì„± ì¤‘...")
service_path = self.target_path / "services" / service_name
# DDD ë ˆì´ì–´ êµ¬ì¡° ìƒì„±
for layer in ["domain", "application", "infrastructure", "interfaces"]:
layer_path = service_path / layer
layer_path.mkdir(parents=True, exist_ok=True)
if layer == "domain":
await self._create_domain_layer(layer_path, blueprint)
elif layer == "interfaces":
await self._create_interfaces_layer(layer_path, blueprint)
# Dockerfile ìƒì„±
await self._create_service_dockerfile(service_path, blueprint)
microservice_results[service_name] = {
"status": "ìƒì„±ë¨",
"port": blueprint.port,
"features": len(blueprint.key_features)
return microservice_results
async def _create_domain_layer(self, layer_path: Path, blueprint: V4ServiceBlueprint):
"""ë„ë©”ì¸ ë ˆì´ì–´ ìƒì„±"""
aggregates_path = layer_path / "aggregates"
aggregates_path.mkdir(exist_ok=True)
main_aggregate_file = aggregates_path / f"{blueprint.name.replace('-', '_')}_aggregate.py"
aggregate_template = f'''"""
{blueprint.name} V4 Enhanced Aggregate
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import uuid
class {blueprint.name.replace("-", "").title()}Aggregate:
"""V4 Enhanced {blueprint.name} Aggregate"""
def __init__(self):
self.id = str(uuid.uuid4())
self.created_at = datetime.utcnow()
self.domain_focus = "{blueprint.domain_focus}"
self.port = {blueprint.port}
self.status = "ACTIVE"
async def execute_core_business_logic(self, command: Dict) -> Dict:
"""í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰"""
await self._validate_business_rules(command)
result = await self._execute_domain_logic(command)
return result
except Exception as e:
async def _validate_business_rules(self, command: Dict):
"""ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦"""
async def _execute_domain_logic(self, command: Dict) -> Dict:
"""ë„ë©”ì¸ ë¡œì§ ì‹¤í–‰"""
return {{"status": "success", "result": "processed"}}
with open(main_aggregate_file, 'w', encoding='utf-8') as f:
f.write(aggregate_template)
async def _create_interfaces_layer(self, layer_path: Path, blueprint: V4ServiceBlueprint):
"""ì¸í„°í˜ì´ìŠ¤ ë ˆì´ì–´ ìƒì„± (FastAPI)"""
api_path = layer_path / "api"
api_path.mkdir(exist_ok=True)
api_file = api_path / "main.py"
api_template = f'''"""
{blueprint.name} V4 Enhanced FastAPI Interface
from fastapi import FastAPI, HTTPException, Depends, status
from pydantic import BaseModel
from typing import Dict, List, Optional
title="{blueprint.name.title()}",
description="{blueprint.domain_focus}",
allow_origins=["*"],
allow_methods=["*"],
allow_headers=["*"],
class RequestModel(BaseModel):
id: Optional[str] = None
action: str
data: Dict = {{}}
class ResponseModel(BaseModel):
status: str
result: Dict
message: Optional[str] = None
return {{"status": "healthy", "service": "{blueprint.name}", "port": {blueprint.port}}}
return {{"status": "ready", "service": "{blueprint.name}"}}
{chr(10).join(self._generate_api_endpoint(endpoint, blueprint) for endpoint in blueprint.api_endpoints)}
import uvicorn
uvicorn.run(app, host="0.0.0.0", port={blueprint.port})
with open(api_file, 'w', encoding='utf-8') as f:
f.write(api_template)
def _generate_api_endpoint(self, endpoint: str, blueprint: V4ServiceBlueprint) -> str:
"""API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±"""
endpoint_name = endpoint.replace("/", "").replace("-", "_")
return f'''
@app.post("{endpoint}")
async def {endpoint_name}_endpoint(request: RequestModel):
{endpoint} ì—”ë“œí¬ì¸íŠ¸ - {blueprint.domain_focus}
# ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
result = {{"processed": True, "endpoint": "{endpoint}"}}
return ResponseModel(status="success", result=result, message=f"{endpoint} ì²˜ë¦¬ ì™„ë£Œ")
logger.error(f"{endpoint} ì²˜ë¦¬ ì‹¤íŒ¨: {{e}}")
raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
async def _create_service_dockerfile(self, service_path: Path, blueprint: V4ServiceBlueprint):
"""ì„œë¹„ìŠ¤ Dockerfile ìƒì„±"""
dockerfile = service_path / "Dockerfile"
dockerfile_content = f'''# {blueprint.name} V4 Enhanced Dockerfile
EXPOSE {blueprint.port}
CMD curl -f http://localhost:{blueprint.port}/health || exit 1
CMD ["python", "-m", "interfaces.api.main"]
with open(dockerfile, 'w', encoding='utf-8') as f:
f.write(dockerfile_content)
requirements_file = service_path / "requirements.txt"
requirements_content = '''fastapi==0.104.1
with open(requirements_file, 'w', encoding='utf-8') as f:
f.write(requirements_content)
architect = V4SystemArchitect()
await architect.build_complete_v4_system()
asyncio.run(main())
### **HPA ë° Kubernetes ì™„ì „ ì„¤ì •**
# infrastructure/kubernetes/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
name: phoenix95-v4-hpa
namespace: phoenix95-v4
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: api-gateway-enterprise
minReplicas: 2
maxReplicas: 10
- type: Resource
type: Utilization
averageUtilization: 70
- type: Resource
name: memory
type: Utilization
averageUtilization: 80
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
name: phoenix95-ai-engine-hpa
namespace: phoenix95-v4
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: phoenix95-ai-engine
minReplicas: 2
maxReplicas: 20
- type: Resource
type: Utilization
averageUtilization: 60
apiVersion: v1
kind: Secret
name: phoenix95-secrets
namespace: phoenix95-v4
type: Opaque
database-url: cG9zdGdyZXNxbDovL3Bob2VuaXg5NTpwaG9lbml4OTVfc2VjdXJlX3Bhc3N3b3JkQHBvc3RncmVzcWw6NTQzMi9waG9lbml4OTVfdjQ=
redis-url: cmVkaXM6Ly9yZWRpczoyNjM3OS8w
influxdb-url: aHR0cDovL2luZmx1eGRiOjgwODY=
telegram-token: NzM4NjU0MjgxMTpBQUVaMjFwMzByRVMxazhOeE5NMnhiWjUzVTQ0UEk5RDVDWQ==
telegram-chat-id: NzU5MDg5NTk1Mg==
### **Grafana ëŒ€ì‹œë³´ë“œ ì™„ì „ ì„¤ì •**
"dashboard": {
"id": null,
"title": "Phoenix 95 V4 Enhanced Dashboard",
"tags": ["phoenix95", "v4", "enhanced"],
"timezone": "browser",
"title": "V4 ì„œë¹„ìŠ¤ ìƒíƒœ",
"type": "stat",
"targets": [{"expr": "up{job='phoenix95-v4-services'}"}],
"fieldConfig": {
"defaults": {
"color": {"mode": "palette-classic"},
"custom": {"displayMode": "list", "orientation": "auto"},
"thresholds": {
{"color": "green", "value": null},
{"color": "red", "value": 0}
"gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
"title": "Phoenix 95 AI ë¶„ì„ ì„±ëŠ¥",
"type": "graph",
{"expr": "rate(phoenix95_ai_analyses_total[5m])", "legendFormat": "ë¶„ì„/ì´ˆ"},
{"expr": "phoenix95_ai_confidence_score", "legendFormat": "í‰ê·  ì‹ ë¢°ë„"}
"gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
"title": "ë ˆë²„ë¦¬ì§€ ê±°ë˜ í˜„í™©",
"type": "graph",
{"expr": "phoenix95_active_positions", "legendFormat": "í™œì„± í¬ì§€ì…˜"},
{"expr": "phoenix95_leverage_ratio", "legendFormat": "í‰ê·  ë ˆë²„ë¦¬ì§€"}
"gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
"title": "ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤",
"type": "graph",
{"expr": "node_memory_MemAvailable_bytes", "legendFormat": "ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬"},
{"expr": "rate(node_cpu_seconds_total[5m])", "legendFormat": "CPU ì‚¬ìš©ë¥ "}
"gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
"title": "API ì‘ë‹µ ì‹œê°„",
"type": "graph",
{"expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "95í¼ì„¼íƒ€ì¼"},
{"expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "50í¼ì„¼íƒ€ì¼"}
"gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
"time": {"from": "now-1h", "to": "now"},
"refresh": "5s"
# services/trade-execution-leverage/domain/aggregates/trade_executor.py
V4 Enhanced 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹¤í–‰ê¸°
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
class LeveragePosition:
"""ë ˆë²„ë¦¬ì§€ í¬ì§€ì…˜"""
position_id: str
symbol: str
action: str
leverage: int
entry_price: float
quantity: float
margin_required: float
liquidation_price: float
status: str = "ACTIVE"
class LeverageTradeExecutor:
"""V4 Enhanced ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹¤í–‰ê¸°"""
def __init__(self):
self.max_leverage = 20
self.margin_mode = "ISOLATED"
self.active_positions: Dict[str, LeveragePosition] = {}
async def execute_trade_complete(self, signal: Dict, analysis: Dict) -> Dict:
"""ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì™„ì „ ì‹¤í–‰"""
# 1. í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
position_size = await self._calculate_position_size(signal, analysis)
margin_required = await self._calculate_margin_required(signal, position_size)
# 3. ì²­ì‚°ê°€ ê³„ì‚°
liquidation_price = await self._calculate_liquidation_price(signal, position_size)
# 4. ê±°ë˜ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
position = await self._execute_trade_simulation(signal, position_size, margin_required, liquidation_price)
# 5. í¬ì§€ì…˜ ì¶”ì  ì‹œì‘
await self._start_position_tracking(position)
"success": True,
"position_id": position.position_id,
"entry_price": position.entry_price,
"leverage": position.leverage,
"margin_required": position.margin_required,
"liquidation_price": position.liquidation_price
async def _calculate_position_size(self, signal: Dict, analysis: Dict) -> float:
"""í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°"""
kelly_ratio = analysis.get('kelly_ratio', 0.1)
available_balance = 10000.0  # ì˜ˆì‹œ ì”ê³ 
# Kelly ê¸°ë°˜ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
base_position = available_balance * kelly_ratio
leveraged_position = base_position * self.max_leverage
return leveraged_position
async def _execute_trade_simulation(self, signal: Dict, position_size: float, margin_required: float, liquidation_price: float) -> LeveragePosition:
"""ê±°ë˜ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜"""
position_id = f"EXEC_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
position = LeveragePosition(
position_id=position_id,
symbol=signal['symbol'],
action=signal['action'],
leverage=self.max_leverage,
entry_price=signal['price'],
quantity=position_size,
margin_required=margin_required,
liquidation_price=liquidation_price
self.active_positions[position_id] = position
print(f"ğŸ“ˆ ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹¤í–‰: {position.symbol} {position.action} {position.leverage}x")
return position
# ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì ê¸°
# services/position-tracker-realtime/domain/aggregates/position_tracker.py
V4 Enhanced ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì ê¸°
import aioredis
from typing import Dict, List
from datetime import datetime
class RealtimePositionTracker:
"""ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì ê¸°"""
def __init__(self):
self.redis_client = None
self.tracking_tasks: Dict[str, asyncio.Task] = {}
async def start_position_tracking(self, position: Dict):
"""í¬ì§€ì…˜ ì¶”ì  ì‹œì‘"""
position_id = position['position_id']
# Redisì— í¬ì§€ì…˜ ì €ì¥
await self._store_position_in_redis(position)
# ì‹¤ì‹œê°„ ì¶”ì  íƒœìŠ¤í¬ ì‹œì‘
task = asyncio.create_task(self._monitor_position_realtime(position))
self.tracking_tasks[position_id] = task
print(f"ğŸ” ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì  ì‹œì‘: {position_id}")
async def _monitor_position_realtime(self, position: Dict):
"""ì‹¤ì‹œê°„ í¬ì§€ì…˜ ëª¨ë‹ˆí„°ë§"""
position_id = position['position_id']
while True:
current_price = await self._get_current_price(position['symbol'])
pnl = await self._calculate_pnl(position, current_price)
liquidation_risk = await self._check_liquidation_risk(position, current_price)
# Redis ì—…ë°ì´íŠ¸
await self._update_position_in_redis(position_id, {
'current_price': current_price,
'pnl': pnl,
'liquidation_risk': liquidation_risk,
'last_update': datetime.now().isoformat()
if liquidation_risk > 0.8:  # ì²­ì‚° ìœ„í—˜ 80% ì´ìƒ
await self._send_liquidation_warning(position_id, liquidation_risk)
await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
except Exception as e:
print(f"âŒ í¬ì§€ì…˜ ì¶”ì  ì˜¤ë¥˜ {position_id}: {e}")
await asyncio.sleep(10)
async def _calculate_pnl(self, position: Dict, current_price: float) -> float:
"""P&L ê³„ì‚°"""
entry_price = position['entry_price']
quantity = position['quantity']
action = position['action']
if action.lower() == 'buy':
pnl = (current_price - entry_price) * quantity
else:  # sell
pnl = (entry_price - current_price) * quantity
# ì™„ì „ ìë™í™” ë°°í¬ ì‹¤í–‰ê¸°
# scripts/complete_deployment.sh
# Phoenix 95 V4 Enhanced ì™„ì „ ìë™í™” ë°°í¬
echo "ğŸš€ Phoenix 95 V4 Enhanced ì™„ì „ ìë™í™” ë°°í¬ ì‹œì‘"
START_TIME=$(date +%s)
DEPLOY_LOG="complete_deploy_$(date +%Y%m%d_%H%M%S).log"
log "ğŸ” ë°°í¬ í™˜ê²½ ê²€ì¦ ì¤‘..."
python3 tools/verify_environment.py || { log "âŒ í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨"; exit 1; }
# 2. V3 â†’ V4 ë§ˆì´ê·¸ë ˆì´ì…˜ (ìˆëŠ” ê²½ìš°)
if [ -f "main_webhook_server.py" ]; then
log "ğŸŒŠ V3 â†’ V4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘..."
python3 tools/v3_migration_manager.py
log "âœ… V3 â†’ V4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ"
# 3. V4 ì‹œìŠ¤í…œ êµ¬ì¶•
log "ğŸ—ï¸ V4 Enhanced ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘..."
python3 tools/v4_complete_builder.py
# 4. ì¸í”„ë¼ ë°°í¬ (Terraform)
if command -v terraform &> /dev/null; then
log "ğŸ—ï¸ Terraform ì¸í”„ë¼ ë°°í¬ ì¤‘..."
cd infrastructure/terraform
terraform init
terraform apply -auto-approve
# 5. Docker ì´ë¯¸ì§€ ë¹Œë“œ
log "ğŸ³ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
services=("api-gateway-enterprise" "signal-ingestion-pro" "market-data-intelligence" "phoenix95-ai-engine" "trade-execution-leverage" "position-tracker-realtime" "notification-hub-intelligent")
for service in "${services[@]}"; do
log "ğŸ”§ $service ë¹Œë“œ ì¤‘..."
docker build -t phoenix95/v4-enhanced-$service:latest services/$service/
# 6. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
log "ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."
docker-compose up -d postgresql redis influxdb elasticsearch
# 7. ìŠ¤í‚¤ë§ˆ ìƒì„±
log "ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘..."
cd phoenix95_v4_enhanced
python3 scripts/create_schemas.py
# 8. ì„œë¹„ìŠ¤ ë°°í¬
log "ğŸš€ V4 ì„œë¹„ìŠ¤ ë°°í¬ ì¤‘..."
# 9. í—¬ìŠ¤ì²´í¬ (10íšŒ ì¬ì‹œë„)
log "ğŸ” ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ ì¤‘..."
for service_port in 8100 8101 8102 8103 8106 8107 8109; do
service_name=$(docker-compose ps --format "table {{.Service}}" | grep $service_port | head -1)
if curl -f -s --max-time 5 http://localhost:$service_port/health > /dev/null; then
log "âœ… í¬íŠ¸ $service_port í—¬ìŠ¤ì²´í¬ ì„±ê³µ"
if [ $i -eq 10 ]; then
log "âŒ í¬íŠ¸ $service_port í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
docker-compose logs --tail=50 $(docker-compose ps -q)
log "â³ í¬íŠ¸ $service_port í—¬ìŠ¤ì²´í¬ ì¬ì‹œë„... ($i/10)"
# 10. ëª¨ë‹ˆí„°ë§ ì‹œì‘
log "ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘..."
docker-compose up -d prometheus grafana
# 11. ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
log "ğŸ§ª ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì¤‘..."
python3 tests/integration/test_v4_system.py
# 12. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
log "âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘..."
python3 tests/performance/test_system_performance.py
# 13. ë°°í¬ ì™„ë£Œ ì•Œë¦¼
END_TIME=$(date +%s)
DEPLOY_DURATION=$((END_TIME - START_TIME))
log "ğŸ‰ Phoenix 95 V4 Enhanced ì™„ì „ ë°°í¬ ì„±ê³µ!"
log "â±ï¸ ì´ ë°°í¬ ì‹œê°„: $((DEPLOY_DURATION / 60))ë¶„ $((DEPLOY_DURATION % 60))ì´ˆ"
# í…”ë ˆê·¸ë¨ ì„±ê³µ ì•Œë¦¼
import requests
telegram_token = '7386542811:AAEZ21p30rES1k8NxNM2xbZ53U44PI9D5CY'
telegram_chat_id = '7590895952'
message = '''ğŸ‰ Phoenix 95 V4 Enhanced ë°°í¬ ì™„ë£Œ!
â±ï¸ ì†Œìš” ì‹œê°„: $((DEPLOY_DURATION / 60))ë¶„
ğŸš€ 7ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í™œì„±
âš¡ 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì¤€ë¹„
ğŸ§  Phoenix 95 AI ì—”ì§„ ê°€ë™
ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í™œì„±
ğŸ”— API Gateway: http://localhost:8100
ğŸ“ˆ Grafana: http://localhost:3000
response = requests.post(f'https://api.telegram.org/bot{telegram_token}/sendMessage',
data={'chat_id': telegram_chat_id, 'text': message})
if response.status_code == 200:
print('âœ… í…”ë ˆê·¸ë¨ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ë¨')
print('âš ï¸ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨')
print(f'âš ï¸ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì˜¤ë¥˜: {e}')
echo "ğŸ“Š V4 Enhanced ì‹œìŠ¤í…œ ì ‘ì† ì •ë³´:"
echo "ğŸ”— API Gateway: http://localhost:8100"
echo "ğŸ“ˆ Grafana: http://localhost:3000 (admin/admin)"
echo "ğŸ“Š Prometheus: http://localhost:9090"
echo "ğŸ§  Phoenix 95 AI: http://localhost:8103"
echo "âš¡ ë ˆë²„ë¦¬ì§€ ê±°ë˜: http://localhost:8106"
echo "ğŸ“ í¬ì§€ì…˜ ì¶”ì : http://localhost:8107"
echo "ğŸ”” ì•Œë¦¼ í—ˆë¸Œ: http://localhost:8109"
echo "ğŸ¯ Phoenix 95 V4 Enhanced ì™„ì „ ìë™í™” ë°°í¬ ì„±ê³µ!"
### **í†µí•© í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**
# tests/integration/test_v4_system.py
V4 Enhanced ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
import pytest
from datetime import datetime
class V4SystemIntegrationTest:
"""V4 ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
def __init__(self):
self.base_urls = {
"api_gateway": "http://localhost:8100",
"signal_ingestion": "http://localhost:8101",
"market_data": "http://localhost:8102",
"phoenix95_ai": "http://localhost:8103",
"trade_execution": "http://localhost:8106",
"position_tracker": "http://localhost:8107",
"notification_hub": "http://localhost:8109"
async def test_all_services_health(self):
"""ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸"""
print("ğŸ” V4 ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
results = {}
for service_name, base_url in self.base_urls.items():
async with session.get(f"{base_url}/health", timeout=10) as response:
if response.status == 200:
results[service_name] = "âœ… ì •ìƒ"
results[service_name] = f"âŒ ì‘ë‹µ ì½”ë“œ: {response.status}"
except Exception as e:
results[service_name] = f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}"
for service_name, status in results.items():
print(f"  {service_name}: {status}")
# ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì •ìƒì¸ì§€ í™•ì¸
failed_services = [name for name, status in results.items() if not status.startswith("âœ…")]
if failed_services:
raise Exception(f"ì‹¤íŒ¨í•œ ì„œë¹„ìŠ¤: {failed_services}")
print("âœ… ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í†µê³¼")
async def test_phoenix95_ai_analysis(self):
"""Phoenix 95 AI ë¶„ì„ í…ŒìŠ¤íŠ¸"""
print("ğŸ§  Phoenix 95 AI ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
test_signal = {
"signal_id": "TEST_SIGNAL_001",
"symbol": "BTCUSDT",
"action": "buy",
"price": 45000.0,
"confidence": 0.85
async with session.post(
f"{self.base_urls['phoenix95_ai']}/analyze",
json=test_signal,
) as response:
if response.status != 200:
raise Exception(f"AI ë¶„ì„ ì‹¤íŒ¨: {response.status}")
result = await response.json()
required_fields = ["phoenix95_score", "confidence_level", "kelly_ratio", "recommendation"]
for field in required_fields:
if field not in result:
raise Exception(f"AI ë¶„ì„ ê²°ê³¼ì— {field} ëˆ„ë½")
print(f"  Phoenix 95 ì ìˆ˜: {result['phoenix95_score']:.3f}")
print(f"  ì‹ ë¢°ë„: {result['confidence_level']:.3f}")
print(f"  Kelly ë¹„ìœ¨: {result['kelly_ratio']:.3f}")
print(f"  ì¶”ì²œ: {result['recommendation']}")
print("âœ… Phoenix 95 AI ë¶„ì„ í…ŒìŠ¤íŠ¸ í†µê³¼")
async def test_leverage_trading_simulation(self):
"""ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
print("âš¡ ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
trade_request = {
"signal_id": "TEST_TRADE_001",
"symbol": "BTCUSDT",
"action": "buy",
"price": 45000.0,
"leverage": 20,
"margin_mode": "ISOLATED"
async with session.post(
f"{self.base_urls['trade_execution']}/execute",
json=trade_request,
) as response:
if response.status != 200:
raise Exception(f"ê±°ë˜ ì‹¤í–‰ ì‹¤íŒ¨: {response.status}")
result = await response.json()
required_fields = ["position_id", "entry_price", "leverage", "margin_required"]
for field in required_fields:
if field not in result:
raise Exception(f"ê±°ë˜ ì‹¤í–‰ ê²°ê³¼ì— {field} ëˆ„ë½")
print(f"  í¬ì§€ì…˜ ID: {result['position_id']}")
print(f"  ì§„ì…ê°€: {result['entry_price']}")
print(f"  ë ˆë²„ë¦¬ì§€: {result['leverage']}x")
print(f"  í•„ìš” ë§ˆì§„: {result['margin_required']}")
print("âœ… ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ í†µê³¼")
"""í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
tester = V4SystemIntegrationTest()
await tester.test_all_services_health()
await tester.test_phoenix95_ai_analysis()
await tester.test_leverage_trading_simulation()
print("ğŸ‰ ëª¨ë“  V4 ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼!")
return True
print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
exit(0 if success else 1)
# tests/performance/test_system_performance.py
V4 Enhanced ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
class V4PerformanceTest:
"""V4 ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
def __init__(self):
self.api_gateway_url = "http://localhost:8100"
self.phoenix95_ai_url = "http://localhost:8103"
self.results = {}
async def test_api_gateway_throughput(self, concurrent_requests=100, total_requests=1000):
"""API Gateway ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸"""
print(f"ğŸ“Š API Gateway ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸ ({concurrent_requests} ë™ì‹œ, {total_requests} ì´ ìš”ì²­)")
async def make_request(session, request_id):
start_time = time.time()
async with session.get(f"{self.api_gateway_url}/health") as response:
end_time = time.time()
"request_id": request_id,
"status_code": response.status,
"response_time": end_time - start_time,
"success": response.status == 200
except Exception as e:
end_time = time.time()
"request_id": request_id,
"status_code": 0,
"response_time": end_time - start_time,
"success": False,
"error": str(e)
start_time = time.time()
semaphore = asyncio.Semaphore(concurrent_requests)
async def bounded_request(session, request_id):
async with semaphore:
return await make_request(session, request_id)
tasks = [bounded_request(session, i) for i in range(total_requests)]
results = await asyncio.gather(*tasks)
end_time = time.time()
successful_requests = [r for r in results if r["success"]]
failed_requests = [r for r in results if not r["success"]]
response_times = [r["response_time"] for r in successful_requests]
total_time = end_time - start_time
rps = len(successful_requests) / total_time
self.results["api_gateway_throughput"] = {
"total_requests": total_requests,
"successful_requests": len(successful_requests),
"failed_requests": len(failed_requests),
"success_rate": len(successful_requests) / total_requests * 100,
"total_time": total_time,
"requests_per_second": rps,
"avg_response_time": statistics.mean(response_times) if response_times else 0,
"p95_response_time": statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else 0,
"p99_response_time": statistics.quantiles(response_times, n=100)[98] if len(response_times) >= 100 else 0
print(f"  ì„±ê³µë¥ : {self.results['api_gateway_throughput']['success_rate']:.1f}%")
print(f"  RPS: {rps:.1f}")
print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {self.results['api_gateway_throughput']['avg_response_time']*1000:.1f}ms")
print(f"  P95 ì‘ë‹µì‹œê°„: {self.results['api_gateway_throughput']['p95_response_time']*1000:.1f}ms")
async def test_phoenix95_ai_performance(self, num_analyses=50):
"""Phoenix 95 AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
print(f"ğŸ§  Phoenix 95 AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_analyses}ê°œ ë¶„ì„)")
"signal_id": f"PERF_TEST_{i:03d}",
"symbol": "BTCUSDT",
"action": "buy" if i % 2 == 0 else "sell",
"price": 45000.0 + (i * 10),
"confidence": 0.8 + (i % 3) * 0.05
for i in range(num_analyses)
async def analyze_signal(session, signal):
start_time = time.time()
async with session.post(
f"{self.phoenix95_ai_url}/analyze",
json=signal,
) as response:
end_time = time.time()
if response.status == 200:
result = await response.json()
"signal_id": signal["signal_id"],
"analysis_time": end_time - start_time,
"success": True,
"phoenix95_score": result.get("phoenix95_score", 0)
"signal_id": signal["signal_id"],
"analysis_time": end_time - start_time,
"success": False
except Exception as e:
end_time = time.time()
"signal_id": signal["signal_id"],
"analysis_time": end_time - start_time,
"success": False,
"error": str(e)
start_time = time.time()
# ë™ì‹œì— 5ê°œì”© ì²˜ë¦¬
semaphore = asyncio.Semaphore(5)
async def bounded_analyze(signal):
async with semaphore:
return await analyze_signal(session, signal)
results = await asyncio.gather(*[bounded_analyze(signal) for signal in test_signals])
end_time = time.time()
successful_analyses = [r for r in results if r["success"]]
analysis_times = [r["analysis_time"] for r in successful_analyses]
self.results["phoenix95_ai_performance"] = {
"total_analyses": num_analyses,
"successful_analyses": len(successful_analyses),
"success_rate": len(successful_analyses) / num_analyses * 100,
"total_time": end_time - start_time,
"avg_analysis_time": statistics.mean(analysis_times) if analysis_times else 0,
"max_analysis_time": max(analysis_times) if analysis_times else 0,
"analyses_per_second": len(successful_analyses) / (end_time - start_time)
print(f"  ì„±ê³µë¥ : {self.results['phoenix95_ai_performance']['success_rate']:.1f}%")
print(f"  í‰ê·  ë¶„ì„ì‹œê°„: {self.results['phoenix95_ai_performance']['avg_analysis_time']:.2f}ì´ˆ")
print(f"  ìµœëŒ€ ë¶„ì„ì‹œê°„: {self.results['phoenix95_ai_performance']['max_analysis_time']:.2f}ì´ˆ")
print(f"  ì´ˆë‹¹ ë¶„ì„ìˆ˜: {self.results['phoenix95_ai_performance']['analyses_per_second']:.1f}")
"""ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
tester = V4PerformanceTest()
await tester.test_api_gateway_throughput()
await tester.test_phoenix95_ai_performance()
print("\nğŸ‰ V4 ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
api_results = tester.results["api_gateway_throughput"]
ai_results = tester.results["phoenix95_ai_performance"]
print(f"  ğŸ”— API Gateway: {api_results['requests_per_second']:.1f} RPS, {api_results['avg_response_time']*1000:.1f}ms í‰ê· ")
print(f"  ğŸ§  Phoenix 95 AI: {ai_results['analyses_per_second']:.1f} ë¶„ì„/ì´ˆ, {ai_results['avg_analysis_time']:.2f}ì´ˆ í‰ê· ")
if api_results['requests_per_second'] < 50:
print("âš ï¸ API Gateway RPSê°€ ê¸°ì¤€(50) ë¯¸ë‹¬")
if ai_results['avg_analysis_time'] > 5.0:
print("âš ï¸ AI ë¶„ì„ ì‹œê°„ì´ ê¸°ì¤€(5ì´ˆ) ì´ˆê³¼")
return True
print(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
exit(0 if success else 1)
## ğŸ“‹ **V4 Enhanced ì™„ì „ ì‹œìŠ¤í…œ ìš”ì•½**
V4_ì™„ì „_ì‹œìŠ¤í…œ_ìµœì¢…:
âœ… ìë™í™”_ë ˆë²¨: 100% (ì™„ì „ ì›í´ë¦­)
âœ… ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤: 7ê°œ Enterpriseê¸‰
âœ… V3_í˜¸í™˜ì„±: ì™„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›
âœ… í´ë¼ìš°ë“œ_ì¸í”„ë¼: Terraform + AWS EKS
âœ… ë¬´ì¤‘ë‹¨_ë°°í¬: Blue-Green + Canary
âœ… ì‹¤ì‹œê°„_ëª¨ë‹ˆí„°ë§: Prometheus + Grafana + AlertManager
âœ… í†µí•©_í…ŒìŠ¤íŠ¸: ìë™ ê²€ì¦ + ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
âœ… í…”ë ˆê·¸ë¨_í†µí•©: ì‹¤ì‹œê°„ ì•Œë¦¼ + ì˜¤ë¥˜ ë¦¬í¬íŒ…
ğŸ§  Phoenix 95 AI ì—”ì§„ (V3 ë¡œì§ + ë¨¸ì‹ ëŸ¬ë‹)
âš¡ 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜ (ISOLATED ëª¨ë“œ)
ğŸ“ ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì  (P&L + ì²­ì‚° ëª¨ë‹ˆí„°ë§)
ğŸ”— API Gateway (ë¼ìš°íŒ… + ì¸ì¦ + ë¡œë“œë°¸ëŸ°ì‹±)
ğŸ“Š ì‹œì¥ ë°ì´í„° ë¶„ì„ (ì‹¤ì‹œê°„ ì§€í‘œ + ê²€ì¦)
ğŸ”” ì§€ëŠ¥í˜• ì•Œë¦¼ (ìš°ì„ ìˆœìœ„ + ì‚¬ìš©ì ì„¤ì •)
ğŸ’¾ ì™„ì „ ë°ì´í„° ì˜ì†ì„± (PostgreSQL + Redis + InfluxDB)
- ë°°í¬ ì‹œê°„: 10-15ë¶„
- API ì²˜ë¦¬ëŸ‰: 100+ RPS
- AI ë¶„ì„ ì†ë„: 2ì´ˆ ì´ë‚´
- ì‹œìŠ¤í…œ ê°€ìš©ì„±: 99.9%
- ìë™ ìŠ¤ì¼€ì¼ë§: HPA ì§€ì›
**ğŸ‰ ìµœì¢… ê²°ê³¼: ì›ë³¸ d.txtì˜ ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì„ 100% êµ¬í˜„í•œ ì™„ì „ ìë™í™” Enterpriseê¸‰ Phoenix 95 V4 Enhanced ì‹œìŠ¤í…œ!**
## ğŸ“‹ **V4 Enhanced ì‹œìŠ¤í…œ ì™„ì„± ìš”ì•½**
âœ… ìë™í™”_ë ˆë²¨: 100% (ì›í´ë¦­ ë°°í¬)
âœ… ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤: 7ê°œ í•µì‹¬ ì„œë¹„ìŠ¤
âœ… ë°ì´í„°ìŠ¤í† ì–´: PostgreSQL + Redis + InfluxDB
âœ… ëª¨ë‹ˆí„°ë§: Prometheus + Grafana
âœ… ë°°í¬_ë°©ì‹: Docker Compose + Kubernetes
âœ… í—¬ìŠ¤ì²´í¬: ìë™ ê²€ì¦ + ë¡¤ë°±
âœ… ì•Œë¦¼_ì‹œìŠ¤í…œ: í…”ë ˆê·¸ë¨ í†µí•©
âœ… ë³´ì•ˆ: JWT + API í‚¤ + í™˜ê²½ ë³€ìˆ˜
- Phoenix 95 AI ì—”ì§„ (8103í¬íŠ¸)
- 20x ë ˆë²„ë¦¬ì§€ ê±°ë˜ (8106í¬íŠ¸)
- ì‹¤ì‹œê°„ í¬ì§€ì…˜ ì¶”ì  (8107í¬íŠ¸)
- ì§€ëŠ¥í˜• ì•Œë¦¼ í—ˆë¸Œ (8109í¬íŠ¸)
- ì‹œì¥ ë°ì´í„° ë¶„ì„ (8102í¬íŠ¸)
ë°°í¬_ì‹œê°„: ì•½ 10-15ë¶„
í”„ë¡œë•ì…˜_ì¤€ë¹„ë„: 100%
**ğŸ‰ ê²°ê³¼: ì™„ì „ ìë™í™”ëœ Phoenix 95 V4 Enhanced ì‹œìŠ¤í…œì´ ì›í´ë¦­ìœ¼ë¡œ ë°°í¬ ê°€ëŠ¥!**

